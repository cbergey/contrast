# Experiment 1   

In Experiment 1, we ask whether people use descriptive contrast to identify the target of an ambiguous reference. Our experiment was inspired by by work from @sedivy_achieving_1999 showing that people interpret at least some prenominal adjective use as contrastive when the target referents are familiar objects. In their task, four objects appeared on a screen: a target (e.g., a tall cup), a contrastive pair (e.g., a short cup), a competitor that shares the target’s feature but not category (e.g., a tall pitcher), and an irrelevant distractor (e.g., a key). Participants then heard a referring expression: "Pick up the tall cup." Participants looked more quickly to the correct object when the utterance referred to an object with a same-category contrastive pair (tall cup vs. short cup) than when it referred to an object without a contrastive pair (e.g., when there was no short cup in the display). 

Their results suggest that listeners expect speakers to use prenominal description when they are distinguishing between potential referents of the same type, and listeners use this inference to rapidly allocate their attention to the target as an utterance progresses. This principle does not apply equally across adjective types, however: color adjectives seem to hold less contrastive weight [@sedivy_pragmatic_2003], perhaps because color adjectives are often used redundantly in English [@pechmann_incremental_1989]. These experiments demonstrate that listeners use contrast among familiar referents to guide their attention allocation, though not their explicit referent choice, which occurs after the noun disambiguates the object.

In a referential disambiguation task, we presented participants with arrays of novel fruit objects \ref{fig:colortrial}. On critical trials, participants saw a target object, a lure object that shared the target’s critical feature but not its shape, and a contrastive pair that shared the target’s shape but not its critical feature. Participants heard an utterance denoting the feature: "Find the [blue/big] toma." For the target object, use of the adjective was necessary to disambiguate from the same-shape distractor; for the lure, the adjective was relatively superfluous description. If participants use contrastive inference to choose novel referents, they should choose the target object more often than the lure. To examine whether contrast occurs across adjective types, we test participants in two conditions: color contrast and size contrast. Though we expect participants to shift toward choosing the item with a contrastive pair in both conditions, we do not expect them to treat color and size equally. Because color is often used redundantly in English while size is not, we expect size to hold more contrastive weight, encouraging a more consistent contrastive inference [@pechmann_incremental_1989]. 


```{r colortrial, fig.env = "figure", fig.pos = "H", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "On the left: an example of a contrastive trial in which the critical feature is size. Here, the participant would hear the instruction ``Find the small toma.'' On the right: an example of a contrastive trial in which the critical feature is color. Here, the participant would hear the instruction ``Find the red toma.'' In both cases, the target is the top object."}
img <- png::readPNG(here("writing/figs/sizecolorcontrast.png"))
grid::grid.raster(img)
```


## Method

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv")) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 
```

### Participants.

We recruited `r e1_total_subjs` participants through Amazon Mechanical Turk. Half of the participants were assigned to a condition in which the critical feature was color (stimuli contrasted on color), and the other half were assigned to a condition in which the critical feature was size. Each participant gave informed consent and was paid $0.30 in exchange for their participation.
  

### Stimuli.

Stimulus displays were arrays of three novel fruit objects. Fruits were chosen randomly at each trial from 25 fruit kinds. Ten of the 25 fruit drawings were adapted and redrawn from @kanwisher; we designed the remaining 15 fruit kinds. Each fruit kind had an instance in each of four colors (red, blue, green, or purple) and two sizes (big or small). Particular target colors were assigned randomly at each trial and particular target sizes were counterbalanced across display types.  There were two display types: unique target displays and contrastive displays. Unique target displays contained a target object that had a unique shape and was unique on the trial's critical feature (color or size), and two distractor objects that matched each other's (but not the target's) shape and critical feature. These unique target displays were included as a check that participants were making reasonable referent choices and to space out contrastive displays to prevent participants from dialing in on the contrastive object setup during the experiment. Contrastive displays contained a target, its contrastive pair (matched the target's shape but not its critical feature), and a lure (matched the target’s critical feature but not its shape). The positions of the target and distractor items were randomized within a triad configuration.


\textcolor{red}{[Why is there no lure on the right side?]}.
```{r e1-fig, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Proportion of times that participants chose the target and lure items as a function of condition and whether an adjective was provided. Points indicate group means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping."}
condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )
e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  mutate(item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(~condition, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("Utterance type") + 
  geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")

# e1_mean_data %>%
#   filter(searchtype == "contrast") %>%
#   ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
#   facet_wrap(~condition, labeller = as_labeller(condition_names)) + 
#   geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
#                   position = position_dodge(.25)) + 
#   scale_color_ptol() + 
#   ylab("Item chosen") + 
#   xlab("") + 
#   geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
#   theme(legend.position = "none", text = element_text(size=15))
```

### Design and Procedure.

Participants were told they would play a game in which they would search for strange alien fruits. Each participant saw eight trials. Half of the trials were unique target displays and half were contrastive displays. Crossed with display type, half of trials had audio instructions that described the critical feature of the target (e.g., "Find the [blue/big] toma"), and half of trials had audio instructions with no adjective description (e.g., "Find the toma"). A name was randomly chosen at each trial from a list of eight nonce names: blicket, wug, toma, gade, sprock, koba, zorp, and lomet. 

After completing the study, participants were asked to select which of a set of alien words they had heard previously during the study. Four were words they had heard, and four were novel lure words. Participants were dropped from further analysis if they did not respond to at least 6 of these 8 memory check questions correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level) or if they missed any of four color perception check trials (resulting $n =$ `r length(e1_keep_subjs)`).

## Results and Discussion

\textcolor{red}{[Talk about pre-registration]}.

```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)

glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-term) %>%
  rename(term = condition) %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_color_chance", "e1_size_chance"), c("color", "size"), 
      ~ make_text_vars(glmer_chance_comparison, .x, .y))


glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_target_size", "e1_target_adj", "e1_target_size_adj"), 
      c("conditionsize", "adjTRUE", "conditionsize:adjTRUE"), 
      ~ make_text_vars(glmer_unique, .x, .y))
```


We first confirmed that participants understood the task by analyzing performance on unique target trials, the filler trials in which there was a target unique on both shape and the relevant adjective. We asked whether participants chose the target more often than expected by chance ($33\%$) by fitting a mixed effects logistic regression with an intercept term, a random effect of subject, and an offset of $logit(1/3)$ to set chance probability to the correct level. The intercept term was reliably different from zero for both color ($\beta =$ `r e1_color_chance_estimate`, $t =$ `r e1_color_chance_statistic`, $p$ `r e1_color_chance_p.value`) and size ($\beta =$ `r e1_size_chance_estimate` , $t =$ `r e1_size_chance_statistic`, $p$ `r e1_size_chance_p.value`), indicating that participants consistently chose the unique object on the screen when given an instruction like "Find the (blue) toma." In addition, participants were more likely to select the target when an adjective was provided in the audio instruction in both conditions. We confirmed this effect statistically by fitting a mixed effects logistic regression predicting target selection from condition, adjective use, and their interaction with random effects of participants. Use of description in the audio increased target choice ($\beta =$ `r e1_target_adj_estimate`, $t =$ `r e1_target_adj_statistic`, $p$ `r e1_target_adj_p.value`), and adjective type (color vs. size) was not statistically related to target choice ($\beta =$ `r e1_target_size_estimate`, $t =$ `r e1_target_size_statistic`, $p =$ `r e1_target_size_p.value`). The two effects did not significantly interact ($\beta =$ `r e1_target_size_adj_estimate`, $t =$ `r e1_target_size_adj_statistic`, $p$ `r e1_target_size_adj_p.value`). Participants had a general tendency to choose the target in unique target trials, which was strengthened if the audio instruction contained the relevant adjective.


```{r e1-models-contrast}
# in contrast trials w/an adjective, do people choose the target over the lure?
# prereg'd
chance_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(chance_model, "e1_overall_contrast")


# in only *color* contrast trials w/an adjective, do people choose the target over the lure?
# not prereg'd
color_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "color",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(color_model, "e1_color_contrast")

size_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "size",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(size_model, "e1_size_contrast")


no_adj_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == FALSE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy()  %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(no_adj_model, "e1_no_adj")

# in contrast trials w/an adjective, 
# does the type of adjective matter in choosing target over lure?
# prereg'd
adj_type_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))



# in contrast trials, do adj type and presence of an adj interact 
# in determining target over lure choice?
# not prereg'd
adj_by_adjtype_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast",
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition * adj + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))


# throw everything in the model
# prereg'd
# full_model <- e1_data_no_gather %>%
#   filter(searchtype == "contrast" | searchtype == "uniquetarget") %>%
#   glmer(chosetarget ~ adj * condition * searchtype + (searchtype * adj | subid),
#         family = "binomial", data = .) %>%
#   tidy() %>%
#   filter(effect == "fixed")  %>%
#   mutate(p.value = printp(p.value))
```

Our key test was whether participants would choose the target object on contrastive trials--when they heard an adjective in the referential expression. To perform this test, we compared participants' rate of choosing the target to their rate of choosing the lure, which shares the relevant critical feature with the target, when they heard the adjective. Overall, participants chose the target with a contrasting pair more often than the unique lure ($\beta =$ `r e1_overall_contrast_estimate`, $t =$ `r e1_overall_contrast_statistic`, $p =$ `r e1_overall_contrast_p.value`). Considering color and size separately, participants chose the target more than the lure in the size condition ($\beta =$ `r e1_size_contrast_estimate`, $t =$ `r e1_size_contrast_statistic`, $p =$ `r e1_size_contrast_p.value`), but not in the color condition ($\beta =$ `r e1_color_contrast_estimate`, $t =$ `r e1_color_contrast_statistic`, $p =$ `r e1_color_contrast_p.value`). On contrastive trials in which a descriptor was not given, participants dispreferred the target, instead choosing the lure object, which matched the target on the descriptor but had a unique shape ($\beta =$ `r e1_no_adj_estimate`, $t =$ `r e1_no_adj_statistic`, $p =$ `r e1_no_adj_p.value`). Participants' choice of the target in the size condition was therefore not due to a prior preference for the target in contrastive displays, but relied on contrastive interpretation of the adjective. 

When faced with unfamiliar objects referred to by unfamiliar words, people can use pragmatic inference to resolve referential ambiguity and learn the meanings of these new words. In Experiment 1, we found that participants have a general tendency to choose objects that are unique in shape when reference is ambiguous. However, when they hear an utterance with description (e.g., "blue toma", "small toma"), they shift away from choosing unique objects and toward choosing objects that have a similar contrasting counterpart. Furthermore, use of size adjectives--but not color adjectives--prompts people to choose the target object with a contrasting counterpart more often than the unique lure object. We find that people are able to use contrastive inferences about size to successfully resolve which unfamiliar object an unfamiliar word refers to. 

## Model

To formalize the inference that participants were asked to make, we developed a model in the Rational Speech Act Framework  [RSA, @frank2012]. In this framework, pragmatic listeners ($L$) are modeled as drawing inferences about speakers' ($S$) communicative intentions in talking to a hypothetical literal listener ($L_{0}$). This literal listener makes no pragmatic inferences at all, evaluating the literal truth of statements (e.g., it is true that a red toma can be called "toma" and "red toma" but not "blue toma"), and chooses randomly among all referents consistent with a statement. In planning their referring expressions, speakers choose utterances that are successful at accomplishing two goals: (1) making the listener as likely as possible to select the correct object, and (2) minimizing their communicative cost (i.e., producing as few words as possible). Pragmatic listeners use Bayes' rule to invert the speaker's utility function, essentially inferring what the speaker's intention was likely to be given the utterance they produced. 

$$Literal: P_{Lit} = \delta\left(u,r\right)P\left(r\right)$$
$$Speaker: P_S\left(u \vert r\right) \propto \alpha \left(P_{Lit}\left(r \vert u\right) - C\right)$$
$$Listener: P_{Learn}\left(r \vert u\right) \propto P_s\left(u \vert r\right)P\left(r\right)$$

For this experiment, we build on a Rational Speech Act model developed by @frank2014 to jointly resolve reference and learn new words. The primary extension of RSA is that the pragmatic learner is a pragmatic listener who has has uncertainty about the meanings of words in their language, and thus cannot directly compute the speaker's utility as written. Instead, the speaker's utility is conditioned on the set of mappings, and the learners must also infer which set of mappings is correct:  
$$Learner: P_L\left(r \vert u\right) \propto P_s\left(u \vert r; m\right)P\left(r\right)P\left(m\right)$$

In these experiments, we assume that the prior probability to refer to each object $(P\left(r\right))$ is equal, and similarly that all mappings $(P\left(m\right))$ are equally likely, so they cancel out in computations. We further assume that the cost of producing any word is identical, and so the cost of an utterance is equal to its length. All that remains is to specify the possible mappings, and literal meanings, and alternative utterances possible on each trial of the experiment. We describe the size condition here, but the computation for the color condition is analogous. 

On the trial shown in the left panel of Figure \ref{fig:e1-fig} people see two objects that look something like a hair dryer and one that looks like a pear and they are asked to "Find the toma." Here, in the experiment design and the model, we take advantage of the fact that English speakers tend to assume that nouns generally correspond to differences in shape rather than other features [@landau1992]. Given this, the two possible mappings are $\{m_1: hair dryer-``toma'', pear-``?''\}$, and $\{m_2: hair dryer-``?'', pear-``toma''\}$ The literal semantics of each object allow them to be referred to by their shape label (e.g. "toma"), or by a descriptor that is true of them (e.g. "small"), but not names for other shapes or untrue descriptors.


Having heard "Find the toma," the model must now choose a referent. If the true mapping for "toma" is the hair dryer ($m_1$), this utterance is ambiguous to the literal listener, as there are two referents consistent with the literal meaning toma. Consequently, whichever of the two referents the speaker intends to point out to the learner, the speaker's utility will be relatively low. In contrast, if the true mapping for "toma" is the pear ($m_1$), then the utterance will be unambiguous to the literal listener, and thus the speaker's utterance will have higher utility. As a result, the model can infer that the more likely mapping is $m_2$ and choose the pear, simultaneously resolving reference and learning the meaning of "toma."

If instead the speaker produced "Find the small toma," the model will make a different inference. If the true mapping for "toma" is hair dryer ($m_2$), this utterance now uniquely identifies one referent for the literal listener and thus has high utility. It also uniquely identifies the target if "toma" means pear ($m_1$). However, if "toma" means pear, the speaker's utterance was inefficient because the single word utterance "toma" would have identified the target to the literal listener and incurred less cost. Thus, the model can infer that "toma" is more likely to mean hair dryer and choose the small hair dryer appropriately.

While these descriptions use deterministic language for clarity, the model's computation is probabilistic and thus reflects tendencies to choose those objects rather than fixed rules. Figure \ref{fig:e1-webppl-plot} shows model predictions alongside people's behavior for the size and color contrast conditions in Experiment 1. In line with the intuition above, the model predicts that hearing a bare noun (e.g. "toma") should lead people to infer that the intended referent is the unique object (lure), whereas hearing a modified noun (e.g. "small toma") should lead people to infer that the speaker's intended referent has a same-shaped counterpart without the described feature (i.e., is the target object). 

```{r e1-webppl, eval = FALSE}
two_world_utterances <- tibble(utterance = c("toma", "blue toma"),
                               utterance_num = as.character(1:2))

two_world_inference <- map_dfr(two_world_utterances %>% pull(utterance),
                               ~webppl(program_file =
                                         here("webppl/discrete_semantics.wppl"),
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 
  

e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj")) %>%
  mutate(prob_min = prob, prob_max = prob)
```

```{r estimate-e1-params, eval = FALSE}
e1_estimation_data <- e1_data_no_gather %>%
  filter(searchtype == "contrast") %>%
  mutate(choseother = !chosetarget & !choselure) %>%
  select(condition, adj, chosetarget, choselure, choseother) %>%
  mutate(utt = if_else(adj, "blue dax", "dax")) %>%
  mutate(world = case_when(chosetarget ~ "two dax",
                         choselure ~ "two toma",
                         choseother ~ "two dax",
                         TRUE ~ NA_character_),
         obj = case_when(choseother ~ "red dax",
                         chosetarget | choselure ~ "blue dax",
                         TRUE ~ NA_character_)) %>%
  filter(!is.na(world), !is.na(obj))

size_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "size")

color_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "color")

size_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = size_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

color_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e1_parameters <- size_parameter %>%
  bind_rows(color_parameter)


write_csv(e1_parameters, here("webppl/model_parameters/e1_parameters.csv"))
```

```{r load-e1-params}
e1_parameters <- read_csv(here("webppl/model_parameters/e1_parameters.csv"),
                          show_col_types = FALSE)
```

```{r summarise-e1-parameters}
e1_parameter_means <- e1_parameters %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e1_color_parameter <- e1_parameter_means %>% 
  filter(parameter == "color")

e1_size_parameter <- e1_parameter_means %>% 
  filter(parameter == "size")


# ggplot(e1_parameters, aes(x = value, fill = parameter)) +
#   geom_histogram(alpha = .5) + 
#   theme(legend.position = c(.8, .8))
```

```{r e1-webppl-empirical, eval = FALSE}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))


e1_webppl_input <- two_world_utterances %>%
  mutate(parameter = "color") %>%
  bind_rows(mutate(two_world_utterances, parameter = "size")) %>%
  left_join(e1_parameter_means, by = "parameter") %>%
  group_by(utterance_num, parameter) %>%
  nest()
  
two_world_inference <- e1_webppl_input  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/discrete_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

write_csv(two_world_inference, here("webppl/model_estimates/e1_estimates.csv"))
```

```{r load-e1-webppl-empirical}
two_world_inference <- read_csv(here("webppl/model_estimates/e1_estimates.csv"),
                                show_col_types = FALSE)
```

```{r e1-join-data}
e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj", 
                                        "condition" = "parameter")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)
```

```{r e1-webppl-plot, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Proportion of times that people (and our model) chose the target and lure items as a function of adjective type and whether an adjective was provided. Points indicate empirical means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping. Solid lines show model predictions."}
ggplot(e1_webppl_data, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "item chosen") +
  theme(legend.position = "right") +
  scale_color_ptol() +
  scale_fill_ptol()
```

Our empirical data suggest that people treat color and size adjectives differently, making a stronger contrastive inference with size than with color. One potential explanation for this difference is that people are aware of production asymmetries between color and size. As mentioned, speakers tend to over-describe color, providing more color adjectives than necessary to establish reference, while describing size more minimally [@pechmann_incremental_1989; @nadig_evidence_2002]. Listeners may be aware of this production asymmetry and discount the contrastive weight of color adjectives with respect to reference. 

In the Rational Speech Act model, this kind of difference is captured neatly by a difference in the listener's beliefs about the speaker's rationality (i.e. how sensitive the speaker is to differences in utility of different utterances). To determine the value of the rationality parameter in each condition, we used Empirical Bayesian inference to estimate the likely range of parameter values. These estimates varied substantially across conditions, with the rationality parameter in the color condition estimated to be `r e1_color_parameter$mean` with a 95% credible interval of [`r e1_color_parameter$ci_lower`, `r e1_color_parameter$ci_upper`], and the rationality parameter in the size condition estimated to be `r e1_size_parameter$mean` [`r e1_size_parameter$ci_lower`, `r e1_size_parameter$ci_upper`].

Figure \ref{fig:e1-webppl-plot} shows the model predictions along with the empirical data from Experiment 1. The model broadly captures the contrastive inference--when speakers produce an adjective noun combination like "red toma," the model selects the target object more often than the lure object. The extent to which the model makes this inference varies as predicted between the color and size adjective conditions in line with the different estimated rationality values. In both conditions, despite estimating the value of rationality that makes the observed data more likely, the model overpredicts the extent of the contrastive inference that people make. Intuitively, it appears that in over the strength of their contrastive inferences, people have an especially strong tendency to choose a unique object when they hear an unmodified noun (e.g. "toma"). In an attempt to capture this uniqueness tendency, the model overpredicts the extent of the contrastive inference. 

The model captures the difference between color and size in a difference in the rationality parameter, but leaves open the ultimate source of this difference in rationality. Why there is a production asymmetry in the first place? For now, we bracket this question and note that listeners in our task appropriately discount color's contrastive weight given production norms. 

An alternative way to capture this preference would be to locate it in a different part of the model. One possibility is that the semantics of color and size work differently. A recent model from @degen_when_2020 does predict a color--size asymmetry based on different semantic exactness. In this model, literal semantics are treated as continuous rather than discrete, so "blue" is neither 100% true nor 100% false of a particular object, but can instead be 90% true. They successfully model a number of color--size asymmetries by treating color as having stronger literal semantics (i.e. "blue toma" is a better description of a small blue toma than "small toma" is). However, this model predicts the opposite asymmetry of what we found. Because color has stronger semantics than size, listeners show a stronger contrast effect for color than size. We show this effect in appendix A. Thus, though a continuous semantics can explain our asymmetry, this explanation is unlikely given the continuous semantics that predicts other empirical color--size asymmetries does not predict our findings.

\textcolor{red}{[bridge to e2? i think we want the discussion of of both empirical results and model results to go here actually]}.