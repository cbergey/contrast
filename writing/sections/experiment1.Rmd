# Experiment 1   

In Experiment 1, we ask whether people use contrastive inferences to identify the target of an ambiguous referring expression. Our experiment was inspired by work from @sedivy_achieving_1999 showing that people can use contrastive inferences to guide their attention to referents as utterances progress. In their task, participants saw displays of four objects: a target (e.g., a tall cup), a contrastive pair (e.g., a short cup), a competitor that shares the target’s feature but not category (e.g., a tall pitcher), and an irrelevant distractor (e.g., a key). Participants then heard a referring expression: "Pick up the tall cup." Consider the tall cup (target): 'tall' helpfully distinguishes it from the short cup (contrastive pair). On the other hand, consider the tall pitcher (lure): 'tall' makes no helpful distinction from another pitcher and could even introduce ambiguity, as 'tall' applies to both the target and lure and delays the onset of the noun. Participants looked more quickly to the correct object when the utterance referred to an object with a same-category contrastive pair (tall cup vs. short cup) than when it referred to an object without a contrastive pair (e.g., when there was no short cup in the display), and did so before the onset of the noun.

Their results suggest that listeners expect speakers to use description when they are distinguishing between potential referents of the same type, and use this inference to rapidly allocate their attention to the target as an utterance progresses. This principle does not apply equally across adjective types, however: color adjectives seem to hold less contrastive weight [@sedivy_pragmatic_2003], perhaps because color adjectives are often used redundantly in English--that is, people describe objects' colors even when this description is not necessary to establish reference [@pechmann_incremental_1989]. @kreiss2020production demonstrate that listeners' familiar referent choices closely conform to speakers' production norms, such that over-specified modifiers hold less contrastive weight. If this generalizes to novel object choice, we should find that size adjectives prompt stronger contrastive inferences than color adjectives.

In a pre-registered reference resolution task, we presented participants with arrays of novel fruit objects. On critical trials, participants saw a target object, a lure object that shared the target's critical feature but not its shape, and a contrastive pair that shared the target's shape but not its critical feature (Fig. \ref{fig:colortrial}). Participants heard an utterance, sometimes mentioning the critical feature: "Find the [blue/big] toma." In all trials, utterances used the definite determiner "the," which conveys that there is a specific referent to be identified. For the target object, which had a same-shaped counterpart, use of the adjective was necessary to establish reference. For the lure, which was unique in shape, the adjective was relatively superfluous description. If participants use contrastive inference to choose novel referents, they should choose the target object more often than the lure. To examine whether contrastive inferences differ across adjective types, we tested participants with two feature types: color and size. Though we expected participants to shift toward choosing the item with a contrastive pair when they heard either color or size adjectives, we did not expect them to treat color and size equally. Because color is often used redundantly in English while size is not, we expected size to hold more contrastive weight, encouraging a more consistent contrastive inference [@pechmann_incremental_1989]. The pre-registration of our method, recruitment plan, exclusion criteria, and analyses can be found on the Open Science Framework here: https://osf.io/pqkfy .

```{r colortrial, fig.env = "figure", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "On the left: an example of a contrastive display trial in which the feature type is size. Here, the participant would hear the instruction, e.g., ``Find the toma'' or ``Find the small toma.'' The target is the small hairdryer-shaped object. On the right: an example of a contrastive display trial in which the feature type is color. Here, the participant would hear the instruction, e.g., ``Find the toma'' or ``Find the red toma.'' The target is the red star-shaped object. In each case, the lure shares the target's feature (small on the left, red on the right) but not its shape. The contrastive pair shares the target's shape but not its feature. Labels of the target and lure are provided for clarity and were not shown to participants.", cache = FALSE}
img <- png::readPNG(here("writing/figs/sizecolorcontrast.png"))
grid::grid.raster(img)
```

## Method

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv")) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 

average_time <- e1_data_no_gather %>%
  group_by(subid) %>%
  summarise(total_time = sum(rtsearch)/1000) %>% # convert milliseconds to seconds
  ungroup() %>%
  summarise(mean_time = mean(total_time))
```

### Participants.

We recruited a pre-registered sample of `r e1_total_subjs` participants through Amazon Mechanical Turk. Each participant gave informed consent and was paid $0.30 in exchange for their participation. Participants were told the task was estimated to take 3 minutes and on average they took 44 seconds to complete the task (not including reading the consent form).
  

### Stimuli.

Stimulus displays were arrays of three novel fruit objects. We chose alien fruits as stimuli because fruits are a superordinate category that can intuitively vary considerably in shape, color, and size. Fruits were selected randomly at each trial from 20 fruit kinds. Ten of the 20 fruit drawings were adapted and redrawn from @kanwisher; we designed the remaining 10 fruit kinds. Each fruit kind had an instance in each of four colors (red, blue, green, or purple) and two sizes (big or small). Particular target colors were assigned randomly at each trial and particular target sizes were counterbalanced across display types. The on-screen positions of the target and distractor items were randomized within a triad configuration.

There were two display types: contrastive displays and unique target displays. Contrastive displays contained a target, its contrastive pair (which matched the target's shape but not its feature), and a lure (which matched the target’s feature but not its shape; Fig. \ref{fig:colortrial}). Contrastive displays are the display type of interest, as the presence of a contrastive pair allows for a contrastive inference.

Unique target displays contained a target object that had a unique shape and a unique feature value (color or size), and two distractor objects that matched each other's (but not the target's) shape and feature. Unique target displays were included as filler trials, to space out contrastive displays and to prevent participants from dialing in on the intended contrastive inference during the experiment. Further details about these trials, and the analysis of participants' choices in them, can be found in the Supplemental Materials. All discussion of the results in the main text include only the contrastive displays.

In summary, we manipulated three factors: utterance type (*noun* or *adjective noun*), feature type (color or size), and display type (contrastive display or unique target display). Utterance type and display type were manipulated within subjects, as utterance type is the central manipulation of interest and variation in display type was included as filler to prevent participants from cluing into the intended inference. Feature type (color or size) was manipulated between subjects, to keep the number of unique novel stimuli to be generated manageable given the constraint that each participant could not be shown any stimulus shape on more than one trial.

```{r extra_plot}
condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color",
                    "noun" = "noun \n 'toma'",
                    "adjective noun" = "adjective noun \n 'blue toma'"
                    )
# e1_mean_data %>%
#   filter(searchtype == "contrast") %>%
#   mutate(item = factor(item, levels = c("target", "lure"))) %>%
#   ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
#   facet_grid(~condition, labeller = as_labeller(condition_names)) + 
#   geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
#                   position = position_dodge(.25)) + 
#   scale_color_ptol() + 
#   ylab("Item chosen") + 
#   xlab("Utterance type") + 
#   geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
#   theme(legend.position = "none")

# e1_mean_data %>%
#   filter(searchtype == "contrast") %>%
#   ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
#   facet_wrap(~condition, labeller = as_labeller(condition_names)) + 
#   geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
#                   position = position_dodge(.25)) + 
#   scale_color_ptol() + 
#   ylab("Item chosen") + 
#   xlab("") + 
#   geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
#   theme(legend.position = "none", text = element_text(size=15))
```

### Design and Procedure.

Participants were told they would play a game in which they would search for strange alien fruits. There were eight trials in total. Half of the trials were contrastive displays and half were unique target displays (filler trials). Utterance type was crossed with display type: half of trials had audio instructions with an adjective that described the critical feature of the target (e.g., "Find the blue toma" or "Find the big toma"; the *adjective noun* utterance type), and half of trials had audio instructions with no adjective (e.g., "Find the toma"; the *noun* utterance type). A label was randomly chosen at each trial from a list of eight novel words: blicket, wug, toma, gade, sprock, koba, zorp, and lomet. 

After completing the study, as a check of their attention to the task, participants were asked to select which of a set of alien words they had heard previously during the study. Four were words they had heard, and four were novel lure words. Participants were dropped from further analysis if they did not meet our pre-registered exclusion criteria of responding to at least 6 of these 8 check items correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level) and answering four simple color perception check trials correctly (resulting $n =$ `r length(e1_keep_subjs)`)^[Experiments 1 and 3 were run in 2020, during the COVID-19 pandemic, when high exclusion rates on Amazon Mechanical Turk were being reported by many experimenters. Though our pre-registered criteria led to many exclusions, the check given to participants tested memory for a few novel words heard in the experiment, which we do not believe was an overly stringent requirement.].

## Results

```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)

glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-term) %>%
  rename(term = condition) %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_color_chance", "e1_size_chance"), c("color", "size"), 
      ~ make_text_vars(glmer_chance_comparison, .x, .y))


glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_target_size", "e1_target_adj", "e1_target_size_adj"), 
      c("conditionsize", "adjTRUE", "conditionsize:adjTRUE"), 
      ~ make_text_vars(glmer_unique, .x, .y))
```


```{r e1-models-contrast}
# in contrast trials w/an adjective, do people choose the target over the lure?
# prereg'd
chance_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(chance_model, "e1_overall_contrast")


# in only *color* contrast trials w/an adjective, do people choose the target over the lure?
# not prereg'd
color_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "color",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(color_model, "e1_color_contrast")

size_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "size",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(size_model, "e1_size_contrast")


no_adj_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == FALSE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy()  %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(no_adj_model, "e1_no_adj")

# in contrast trials w/an adjective, 
# does the type of adjective matter in choosing target over lure?
# prereg'd
adj_type_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed", term == "conditionsize") %>%
  mutate(p.value = printp(p.value))

make_text_vars(adj_type_model, "e1_adj_type")

# in contrast trials, do adj type and presence of an adj interact 
# in determining target over lure choice?
# not prereg'd
adj_by_adjtype_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast",
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition * adj + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))


# throw everything in the model
# prereg'd
full_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast" | searchtype == "uniquetarget") %>%
  glmer(chosetarget ~ adj * condition * searchtype + (searchtype * adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")  %>%
  mutate(p.value = printp(p.value))
```

```{r e1-webppl, eval = FALSE}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))

two_world_inference <- map_dfr(two_world_utterances %>% pull(utterance),
                               ~webppl(program_file =
                                         here("webppl/discrete_semantics.wppl"),
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 
  

e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj")) %>%
  mutate(prob_min = prob, prob_max = prob)
```

```{r estimate-e1-params, eval = FALSE}
e1_estimation_data <- e1_data_no_gather %>%
  filter(searchtype == "contrast") %>%
  mutate(choseother = !chosetarget & !choselure) %>%
  select(condition, adj, chosetarget, choselure, choseother) %>%
  mutate(utt = if_else(adj, "blue dax", "dax")) %>%
  mutate(world = case_when(chosetarget ~ "two dax",
                         choselure ~ "two toma",
                         choseother ~ "two dax",
                         TRUE ~ NA_character_),
         obj = case_when(choseother ~ "red dax",
                         chosetarget | choselure ~ "blue dax",
                         TRUE ~ NA_character_)) %>%
  filter(!is.na(world), !is.na(obj))

size_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "size")

color_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "color")

size_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = size_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

color_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e1_parameters <- size_parameter %>%
  bind_rows(color_parameter)

#write_csv(size_estimation_data, here("webppl/data/e1_size_estimation_data.csv"))
#write_csv(color_estimation_data, here("webppl/data/e1_color_estimation_data.csv"))
#write_csv(e1_parameters, here("webppl/model_parameters/e1_parameters.csv"))
```


```{r load-e1-params}
e1_parameters <- read_csv(here("webppl/model_parameters/e1_parameters.csv"),
                          show_col_types = FALSE)
```

```{r summarise-e1-parameters}
e1_parameter_means <- e1_parameters %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e1_color_parameter <- e1_parameter_means %>% 
  filter(parameter == "color")

e1_size_parameter <- e1_parameter_means %>% 
  filter(parameter == "size")
```

```{r e1-webppl-empirical, eval = FALSE}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))


e1_webppl_input <- two_world_utterances %>%
  mutate(parameter = "color") %>%
  bind_rows(mutate(two_world_utterances, parameter = "size")) %>%
  left_join(e1_parameter_means, by = "parameter") %>%
  group_by(utterance_num, parameter) %>%
  nest()
  
two_world_inference <- e1_webppl_input  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/discrete_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

two_world_inference <- e1_webppl_input  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/discrete_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

#write_csv(two_world_inference, here("webppl/model_estimates/e1_estimates.csv"))
```

```{r load-e1-webppl-empirical}
two_world_inference <- read_csv(here("webppl/model_estimates/e1_estimates.csv"),
                                show_col_types = FALSE)
```

```{r e1-join-data}
e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj", 
                                        "condition" = "parameter")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)
```

```{r e1-webppl-plot, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Proportion of times that people (and our model) chose the target and lure items, depending on utterance type (noun vs. adjective noun) and feature type (color vs. size). Note that this is only among contrastive display trials (the type of display shown in Fig. 1). Points indicate empirical means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping. Solid horizontal lines indicate model predictions."}
ggplot(e1_webppl_data, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "proportion item choice") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = item, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")
```

Our key pre-registered analysis was whether participants would choose the target object when they heard an adjective in the referring expression. For example, when they saw the stimuli in the left panel of Figure \ref{fig:colortrial} and heard "Find the small toma," would they choose the target (small hairdryer) over the lure (small pear)? To perform this test, we compared participants' rate of choosing the target to their rate of choosing the lure, which shares the relevant feature with the target, on *adjective noun* trials. Overall, participants chose the target more often than the lure, indicating that they used contrastive inferences to resolve reference ($\beta =$ `r e1_overall_contrast_estimate`, $t =$ `r e1_overall_contrast_statistic`, $p =$ `r e1_overall_contrast_p.value`). That is, overall, they tended to choose the small hairdryer over the small pear in the left panel of Figure \ref{fig:colortrial}, and the red star over the red zucchini in the right panel. To test whether the strength of the contrastive inference differed between color and size, we pre-registered a version of this regression with a term for feature type, and found that people were more likely to choose the target over the lure in the size condition than the color condition ($\beta =$ `r e1_adj_type_estimate`, $t =$ `r e1_adj_type_statistic`, $p =$ `r e1_adj_type_p.value`). That is, referring again to the displays in \ref{fig:colortrial}, people were more likely to choose the target when hearing "Find the small toma" and picking among the items on the left than when hearing "Find the red toma" and choosing among items on the right. Overall, when people hear an utterance like "blue toma" or "big toma", they tend to choose the target over the lure, and this tendency is stronger with size adjectives than color adjectives (Fig. \ref{fig:e1-webppl-plot}).

Given this result, we tested whether people consistently chose the target over the lure in *adjective noun* trials in the color and size conditions separately, as a stricter check of whether the effect was present in both feature types (not pre-registered). Considering color and size separately, participants chose the target significantly more often than the lure in the size condition ($\beta =$ `r e1_size_contrast_estimate`, $t =$ `r e1_size_contrast_statistic`, $p =$ `r e1_size_contrast_p.value`), but not in the color condition ($\beta =$ `r e1_color_contrast_estimate`, $t =$ `r e1_color_contrast_statistic`, $p =$ `r e1_color_contrast_p.value`). 

When there was no adjective in the utterance (*noun* trials), participants dispreferred the target, instead choosing the lure object, which matched the target's feature but had a unique shape ($\beta =$ `r e1_no_adj_estimate`, $t =$ `r e1_no_adj_statistic`, $p =$ `r e1_no_adj_p.value`). That is, when people hear an utterance like "Find the toma," they tend to choose the lure (Fig. \ref{fig:e1-webppl-plot}): the pear in the left panel and the zucchini in the right panel of Figure \ref{fig:colortrial}. Participants' choice of the target in *adjective noun* trials in the size condition was therefore not due to a prior preference for the target, but relied on contrastive interpretation of the adjective. In the Supplemental Materials, we report an additional pre-registered analysis of all Experiment 1 data with maximal terms and random effects; those results are consistent with the more focused tests reported here.

## Discussion

When faced with unfamiliar objects referred to by unfamiliar words, people can use pragmatic inference to resolve referential ambiguity and learn the meanings of new words. In Experiment 1, we found that people have a general tendency to choose objects that are unique in shape when reference is ambiguous: when they see a display like those in Figure \ref{fig:colortrial} and hear "Find the toma," they tend to choose the lure. However, when they hear an utterance with an adjective (e.g., "Find the red toma", "Find the small toma"), they shift away from choosing the unique lure and toward choosing the target, which has a similar contrasting counterpart. Furthermore, use of size adjectives—but not color adjectives—prompts people to choose the target object significantly more often than the lure object. We found that people are able to use contrastive inferences about size to successfully resolve which unfamiliar object an unfamiliar word refers to. 

## Model

To formalize the inference that participants were asked to make, we developed a model in the Rational Speech Act Framework  [RSA, @frank2012].^[We implemented these models in the WebPPL programming language [@dippl], using analyses informed by @scontras_probabilistic_2018 and using the Rwebppl package [@braginskyrwebppl].] In this framework, pragmatic listeners ($L$) are modeled as drawing inferences about speakers' ($S$) communicative intentions in talking to a hypothetical literal listener ($L_{0}$). This literal listener makes no pragmatic inferences at all: it evaluates the literal truth of a statement (e.g., it is true that a red toma can be called "toma" and "red toma" but not "blue toma") and chooses randomly among all referents consistent with that statement. In planning their referring expressions, speakers choose utterances that are successful at accomplishing two goals: (1) making the listener as likely as possible to select the correct object, and (2) minimizing their communicative cost (i.e., producing as few words as possible). Note that though determiners are not given in the model's utterances, the assumption that the utterance refers to a specific referent is built into the model structure, consistent with the definite determiners used in the task. Pragmatic listeners use Bayes' rule to invert the speaker's utility function, essentially inferring what the speaker's intention was likely to be given the utterance they produced. 

$$Literal: P_{Lit} = \delta\left(u,r\right)P\left(r\right)$$
$$Speaker: P_S\left(u \vert r\right) \propto \alpha \left(P_{Lit}\left(r \vert u\right) - C\right)$$
$$Listener: P_{Learn}\left(r \vert u\right) \propto P_s\left(u \vert r\right)P\left(r\right)$$

For this experiment, we build on a Rational Speech Act model developed by @frank2014 to jointly resolve reference and learn new words. The primary modification of RSA is the use of a pragmatic *learner*: a pragmatic listener who has uncertainty about the meanings of words in their language, and thus cannot directly compute the speaker's utility as written. Instead, the speaker's utility is conditioned on the set of mappings between words and meanings, and the learner must also infer which set of mappings is correct:  
$$Learner: P_L\left(r \vert u\right) \propto P_s\left(u \vert r; m\right)P\left(r\right)P\left(m\right)$$

In these experiments, we assume that the prior probability to refer to each object $(P\left(r\right))$ is equal, and similarly that all mappings $(P\left(m\right))$ are equally likely, so they cancel out in computations. We further assume that the cost of producing any word is identical, and so the cost of an utterance is equal to its length. All that remains is to specify the possible mappings, and literal meanings, and alternative utterances possible on each trial of the experiment. We describe the size condition here, but the computation for the color condition is analogous. 

On the trial shown in the left panel of Figure \ref{fig:colortrial}, people see two objects that look something like a hair dryer and one that looks like a pear, and they are asked to "Find the toma." Here, in the experiment design and the model, we take advantage of the fact that English speakers tend to assume that nouns generally correspond to differences in shape rather than other features [@landau1992]. Given this, the two possible mappings are $\{m_1: hair dryer-``toma", pear-?\}$ and $\{m_2: hair dryer-?, pear-``toma"\}$. The literal semantics of each object allow them to be referred to by their shape label (e.g. "toma"), or by a adjective that is true of them (e.g. "small"), but not names for other shapes or untrue adjectives.

Having heard "Find the toma," the model must now choose a referent. If the true mapping for "toma" is the hair dryer ($m_1$), this utterance is ambiguous to the literal listener, as there are two referents consistent with the literal meaning toma. Consequently, whichever of the two referents the speaker intends to point out to the learner, the speaker's utility will be relatively low. Alternatively, if the true mapping for "toma" is the pear ($m_2$), then the utterance will be unambiguous to the literal listener, and thus the speaker's utterance will have higher utility. As a result, the model can infer that the more likely mapping is $m_2$ and choose the pear, simultaneously resolving reference and learning the meaning of "toma."

If instead the speaker produced "Find the small toma," the model will make a different inference. If the true mapping for "toma" is hair dryer ($m_2$), this utterance now uniquely identifies one referent for the literal listener and thus has high utility. It also uniquely identifies the target if "toma" means pear ($m_1$). However, if "toma" means pear, the speaker's utterance was inefficient because the single word utterance "toma" would have identified the target to the literal listener and incurred less cost. Thus, the model can infer that "toma" is more likely to mean hair dryer and choose the small hair dryer appropriately.

While these descriptions use deterministic language for clarity, the model's computation is probabilistic and thus reflects tendencies to choose those objects rather than fixed rules. Figure \ref{fig:e1-webppl-plot} shows model predictions alongside people's behavior in Experiment 1. In line with the intuition above, the model predicts that hearing just a noun (e.g. "toma"; *noun* utterance type) should lead people to infer that the intended referent is the unique object (lure), whereas hearing a modified noun (e.g. "small toma"; *adjective noun* utterance type) should lead people to infer that the speaker's intended referent has a same-shaped counterpart without the described feature (i.e., is the target object). 

Our empirical data suggest that people treat color and size adjectives differently, making a stronger contrastive inferences about size than color. In the Rational Speech Act model, this kind of difference can be captured by the rationality parameter $\alpha$, which adjusts how sensitive the speaker is to differences in utility of different utterances. We estimated the rationality parameter separately for color and size to account for this difference and better fit the data. Note that we are using this parameter to approximate people's behavior and do not ascribe a particular psychological interpretation to it; we describe below some alternative modeling choices that could derive color--size asymmetries with more principled interpretations about people's reasoning processes. 

To determine the value of the feature rationality parameter that best describes participants' behavior in each condition, we used Bayesian data analysis, estimating the posterior probability of the observed data under each possible value of $\alpha$ multiplied by the prior probability of each of those values. To estimate the parameter value in each condition, $\alpha$ was drawn from a Gamma distribution with shape and scale parameters set to 2 ($Gamma\left(2,2\right)$), and we sampled using Markov Chain Monte Carlo (MCMC) sampling. This prior encodes a weak preference for small values of $\alpha$, but the estimates below were not sensitive to other choices of hyper-parameters. Posterior mean estimates of the feature rationality parameter varied substantially across conditions. In the color condition, the feature rationality parameter was estimated to be `r e1_color_parameter$mean` with a 95% credible interval of [`r e1_color_parameter$ci_lower`, `r e1_color_parameter$ci_upper`]. In the size condition, the feature rationality parameter was estimated to be `r e1_size_parameter$mean` [`r e1_size_parameter$ci_lower`, `r e1_size_parameter$ci_upper`].

Figure \ref{fig:e1-webppl-plot} shows the model predictions along with the empirical data from Experiment 1. The model broadly captures the contrastive inference: when speakers produce an *adjective noun* utterance like "red toma," the model selects the target object more often than the lure object. The different estimated feature rationality values also allow the model to capture that people make stronger contrastive inferences about size than color. However, in both conditions, the model overpredicts the extent of the contrastive inference that people make. Intuitively, it appears that over and above the strength of their contrastive inferences, people have an especially strong tendency to choose a unique object when they hear an unmodified noun (i.e., to choose the lure in the *noun* condition). In an attempt to capture this uniqueness tendency, the model overpredicts the extent of the contrastive inference (choice of the target in the *adjective noun* condition). 

Why do people make stronger pragmatic inferences about size than color when determining reference? Our model implements this difference in a relatively agnostic way, and these data alone cannot arbitrate between particular explanations, but we spell out a few possibilities and modeling alternatives here.

One way to capture this asymmetry would be to locate it in a different part of the model: in the literal semantics of color and size. A recent model from @degen_when_2020 does predict a color--size asymmetry based on different semantic exactness. In this model, literal semantics are treated as continuous rather than discrete, so "blue" is neither 100% true nor 100% false of a particular object, but can instead be 90% true. They successfully model a number of color--size asymmetries in production data by treating color as having stronger literal semantics (e.g. "blue toma" is a better description of a small blue toma than "small toma" is). A continuous semantics model with stronger literal semantics for color than size can capture the key asymmetry between color and size in the *adjective noun* trials—that people make more consistent contrastive inferences about size than color—because speakers are expected to mention color more often. However, when fitting a continuous semantics model to all of our data (*noun* and *adjective noun* trials), we do not find the expected strength of semantic values (as demonstrated in the Supplemental Materials). This may be because of people's choice patterns in the *noun* trials as well as their generally noisy guessing. Overall, this approach is a promising avenue for accounting for color--size asymmetries, but may need additional adjustments to account for people's choice patterns in a highly ambiguous task like this one.
<!--
Another possibility is that people attend to the production probabilities of different adjective types and attenuate their inferences accordingly. As discussed, speakers mention color more often than size, and listeners may keep track of these probabilities and discount the weight of color description in identifying referents. Experiments with familiar objects show that people make stronger contrastive inferences with respect to size than color, and @kreiss2020production demonstrate that it is possible to explain differential inferences among color adjectives using production norms. In the Supplemental Materials, we provide a demonstration that people's stronger inferences from size adjectives than color adjectives can be captured by differing production expectations about size and color, and discuss this point further.

One potential explanation for this difference is that people are aware of production asymmetries between color and size. As mentioned, speakers tend to over-describe color, providing more color adjectives than necessary to establish reference, while describing size more minimally [@pechmann_incremental_1989; @nadig_evidence_2002]. Listeners may be aware of this production asymmetry and discount the contrastive weight of color adjectives with respect to reference.
-->

Another possibility is that people's different inferences about size and color adjectives are explained by the gradable semantics of size. Size adjectives are relative gradable adjectives: their meaning is judged relative to a comparison class (e.g., "He is a tall basketball player" may have a meaning akin to "He is tall for a basketball player") [@kennedy_vagueness_2007]. Because this comparison class is sensitive to context (it can even change within a sentence, e.g., "He is tall, but not tall for a basketball player"), there is active disagreement about whether this aspect of gradable adjective meaning is properly considered semantics or pragmatics, or encompasses both semantic and pragmatic processes [@xiang_pragmatic_2022; @tessler_informational_2020]. A gradable semantics account of our finding would posit that a comparison class is necessary to judge size but not color, which accounts for the asymmetry. This would accord with work in which the presence of a local comparison class facilitates reference resolution among familiar objects described with relative adjectives [@aparicio_processing_2016]. That is, in a trial such as the one on the left in Figure \ref{fig:colortrial}, a participant sees two hairdryer-shaped objects, one big and one small, and one small pear-shaped object. When they hear "Find the small toma," they choose the only object that is small and has a potential known comparison class: the small hairdryer, which has a larger hairdryer counterpart. On the other hand, color adjectives are not relative gradable adjectives, and so a comparison class is not necessary to interpret them: they have more absolute meaning. Thus, it is possible to explain the color--size asymmetry by the necessity of a comparison class for judging size, and this may be attributed to semantics, pragmatics, or a combination of both. 
<!--
One way to implement relative gradable adjective meaning in the model would be to retool the pragmatic learner part of the model: a learner might need a comparison point to tell whether a novel object is small or big, but not red or purple, and thus avoid choosing a unique (shaped) object when size is specified but be willing to choose a unique object when color is specified. It is also possible to implement relative gradable semantics in the literal listener (the part of the model that usually corresponds to literal semantics), though it would depart from prior work in that the literal listener typically only evaluates whether a description is true of an object in isolation and does not actively learn during the task. In sum, it is possible to implement gradable adjective meaning either as an aspect of literal semantics or pragmatics.
-->

Overall, we found that people can use contrastive inferences from description to map an unknown word to an unknown object. This inference is captured by an extension of the Rational Speech Act model using a pragmatic learner, who is simultaneously making inferences over possible referents and possible lexicons. This model can also capture people's tendency to make stronger contrastive inferences from color description than size description through differences in the rationality parameter, though the origin of these differences cannot be pinned down with this experiment alone. Our experiment and model results suggest that people can resolve a request like "Give me the small dax" by reasoning that the speaker must have been making a useful distinction by mentioning size, and therefore looking for multiple similar objects that differ in size and choosing the smaller one. Immediately available objects are not the only ones worth making a distinction from, however. Next, we turn to another salient set of objects a speaker might want to set a referent apart from: the referent's category.