---
title             : "Using contrastive inferences to learn about new words and categories"
shorttitle        : "Learning from contrastive inference"

author: 
  - name          : "Claire Bergey"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S. University Avenue, Chicago, IL 60637"
    email         : "cbergey@uchicago.edu"
  - name          : "Dan Yurovsky"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "The University of Chicago"
  - id            : "2"
    institution   : "Carnegie Mellon University"

 

abstract: |
  In the face of unfamiliar language or objects, description is one cue people can use to learn about both. Beyond narrowing potential referents to those that match a descriptor, listeners could infer that a described object is one that contrasts with other relevant objects of the same type (e.g., ``The tall cup'' contrasts with another, shorter cup). This contrast may be in relation to other present objects in the environment or to the referent’s category. In three experiments, we investigate whether listeners use descriptive contrast to resolve reference and make inferences about novel referents' categories. People use size adjectives contrastively to guide referent choice, though they do not do so using color adjectives (Experiment 1). People also use description to infer that a novel object is atypical of its category (Experiment 2). However, these two inferences do not trade off substantially: people infer a described referent is atypical even when the descriptor was necessary to establish reference. We model these experiments in the Rational Speech Act (RSA) framework and find it predicts both of these inferences. Overall, people are able to use descriptive contrast to resolve reference and make inferences about a novel object’s category, allowing them to learn more about new things than literal meaning alone allows."

authornote: |
  All data and code for these analyses are available at https://osf.io/3f8hy/?view_only=9a196db0444c4867bc899cc70a7a1e9c.


keywords          : "parent-child interaction; language development; communication"
wordcount         : "1385"
references        : "42"

bibliography      : ["contrast.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"    
output            : papaja::apa6_pdf
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = '!tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3)
set.seed(42)
options(digits=3, dplyr.summarise.inform = FALSE)
```

```{r libraries, cache = FALSE}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(lme4)
library(broom)
library(broom.mixed)
library(here)
library(english)
library(ggthemes)
library(papaja)
library(gridExtra)
library(glue)
library(directlabels)
library(tidyboot)
library(lmerTest)
library(knitr)
library(rwebppl)
library(ggridges)
logit <- function(x) {log(x/(1-x))}
```

```{r set-theme, cache = FALSE}
theme_set(theme_few(base_size = 10) + theme(legend.position = "none"))
```

```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```

An utterance can say much more about the world than its literal interpretation might suggest. For instance, the utterance "We should hire a female professor" may convey much about the speaker's goals, the makeup of a department, or even the biases of a field that is not literally stated. These pragmatic inferences are pervasive in everyday conversation: by reasoning about what someone says in relation to the context and what they might have said otherwise, we can glean more of their intended meaning. They may be especially powerful, however, if we can use them in less familiar contexts as well: to resolve ambiguity and learn about the unfamiliar. Can people use pragmatic inferences to learn about new words and categories? 

One potential pragmatic tool for resolving communicative uncertainty is contrastive inference. Contrastive inferences are those inferences that derive from the principle that description should discriminate. This principle falls out of the more general Gricean maxim that speakers should say as much as they need to say and no more [@grice1975logic]. To the extent that communicators strive to be minimal and informative, description should discriminate between the referent and some relevant contrasting set. This contrastive inference is fairly obvious from some types of description, such as some postnominal modifiers: "The door with the lock" clearly implies a contrasting door without one [@sedivy_invoking_2002; @sedivy_pragmatic_2003-2; @nietal]. The degree of contrast implied by more common descriptive forms, such as prenominal adjectives in English, is less clear. Speakers do not always use prenominal adjectives minimally, often describing more than is needed to establish reference [@pechmann_incremental_1989; @mangold_informativeness_1988; @engelhardt_speakers_2006]. How, then, do listeners interpret these descriptions? 

Sedivy and colleagues carried out a visual world task demonstrating that people interpret at least some prenominal adjective use as contrastive [@sedivy_achieving_1999]. In their task, four objects appeared on a screen: a target (e.g., a tall cup), a contrastive pair (e.g., a short cup), a competitor that shares the target’s feature but not category (e.g., a tall pitcher), and an irrelevant distractor. Participants then heard a referential expression: "Pick up the tall cup." Participants looked more quickly to the correct object when the utterance referred to an object with a same-category contrastive pair (tall cup vs. short cup) than when it referred to an object without a contrastive pair (e.g., the tall pitcher). Their results suggest that listeners expect speakers to use prenominal description when they are distinguishing between potential referents of the same type, and listeners use this inference to rapidly allocate their attention to the target as an utterance progresses. This kind of inference can be derived from a rational speaker framework in which listeners reason that speakers using an utterance with a description, rather than one without, chose to do so to make a useful contribution to listener understanding [@frank2012]. This effect was demonstrated for size and material adjectives; the results for color adjectives were mixed [@sedivy_achieving_1999; @sedivy_pragmatic_2003-2]. **more discussion of color/size and typicality effects** More recently, this contrastive processing effect was replicated with 5-year-old participants using size adjectives [@huangsnedeker2008]. These experiments demonstrate that listeners interpret at least some prenominal adjectives contrastively, and use this contrastive inference to guide their attention allocation. These results leave open, however, whether listeners use prenominal adjective contrast to resolve referential ambiguity and explicitly guide their referent choice.

Beyond contrasting a referent with other objects in the environment, description may draw a contrast between a referent and its category. In production studies, participants tend to describe atypical features more than they describe typical ones [@mitchell_2013; @westerbeek_2015; @rubio-fernandez_how_2016]. For instance, they almost always include a color descriptor when referring to a blue banana, but not when referring to a yellow one. This, too, can be derived from a rational model of speaker behavior [@degen_when_2020] **is this the cite we want?** . How do listeners interpret such adjective use? Suppose someone hears a referring expression to an unfamiliar object: "Look at that red sprocket." In order to determine whether 'red' was used in contrast to other objects in the environment or to the referent's category, a rational listener must integrate contextual information. If there are many sprockets of different colors around, 'red' was likely used to pick out an individual sprocket. If not, it may have been used to mark the abnormality of this sprocket—perhaps it is rare for sprockets to be red. In this way, it is possible for listeners to make inferences about the category of a novel referent using descriptive contrast. **fix this description to make it clear this is an intuitive gloss**

In this paper, we present a series of experiments to test whether and how listeners make inferences about novel referents using descriptive contrast. First, we examine whether listeners use descriptive contrast to resolve referential ambiguity. In a reference game, participants see groups of novel objects and are asked to pick one with a referring expression, e.g., "Find the blue toma." If participants interpret description contrastively, they should infer that the description was necessary to identify the referent--that the blue toma contrasts with some other-colored toma on the screen. Using this contrastive inference, they can resolve referential ambiguity, choosing a blue object with a similar non-blue counterpart rather than a blue object with no similar counterpart nearby. Second, we test whether listeners use descriptive contrast to make inferences about a novel object's category. Participants are presented with two interlocutors who exchange objects using referring expressions, such as "Pass me the blue toma." If participants interpret description as contrasting with an object's category, they should infer that in general, few tomas are blue. However, context should matter in these judgments: if the descriptor was necessary to identify the referent, an inference of contrast with the category is unwarranted. **fix this last sentence**

In order to determine whether people can use prenominal adjective contrast to disambiguate referents, and how those inferences are affected by adjective type, we use a reference game with novel objects. Novel objects provide both a useful experimental tool and an especially interesting testing ground for contrastive inferences. These objects avoid effects of typicality and familiarity that relate to level of description in production [@pechmann_incremental_1989; @rubio-fernandez_how_2016] on particular features [@mangold_informativeness_1988]. **check cites** They have unknown names and feature distributions, creating the ambiguity necessary for our test of referential disambiguation. But the ability to disambiguate novel referents, or to establish reference with incomplete information, is also the broader problem of learning about the world. We know that distributional information in the world affects people’s pragmatic use and interpreta-tion of description. Here, we ask: can people use pragmatic inferences from description to learn about unfamiliar things in the world?


```{r child = "sections/experiment1.Rmd"}
```

```{r child = "sections/experiment2.Rmd"}
```

```{r child = "sections/experiment3.Rmd"}
```

```{r joint-inference, eval = FALSE}
joint_utterances <- tibble(utterance = c("toma", "blue toma"),
                                utterance_num = as.character(1:2))

joint_inference <- map_dfr(joint_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/two_world_typicality.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(joint_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble()
```

```{r joint-analysis, eval = FALSE}
joint_data <- joint_inference %>%
  pivot_wider(names_from = "Parameter") %>%
  mutate(chosen = case_when(world == "two toma" ~ 
                           gsub("toma", "pair", obj),
                         world == "two dax" ~ 
                           gsub("toma", "single", obj))) %>%
  mutate(p = as.numeric(p),
         utterance = factor(utterance, 
                            levels = c("toma", "blue toma")))

joint_data_obj <- joint_data %>%
  group_by(utterance, chosen) %>%
  count() %>%
  group_by(utterance) %>%
  mutate(prob = n/sum(n))

ggplot(joint_data_obj, aes(x = chosen, y = prob, fill = chosen)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~ utterance) + 
  scale_fill_ptol(drop = FALSE) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "", y = "selection probability")

ggplot(joint_data, aes(x = p, y = chosen)) +
  facet_wrap(~ utterance) +
  stat_density_ridges(scale = 1, 
                      quantile_lines = TRUE, quantiles = 2) + 
  scale_fill_ptol() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(y = "object chosen", x = "proportion of tomas that are blue")


joint_data_subset <- joint_data %>%
  filter(!utterance %in% c("red dax"), chosen == "blue pair") %>%
  group_by(utterance, chosen) %>%
  summarise(p = mean(p)) %>%
  spread(utterance, p)

```

## Discussion

# Experiment 3

```{r e3-read-data}

e3_data <- read_csv(here("data/exp3_turk_data.csv"))

# participants who have unbalanced numbers of trials in each condition
e3_exclude <- e3_data %>% count(subid, utttype, searchtype) %>% filter(n == 2 | n == 3)

e3_keep_subjs <- e3_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, attncheckscore >= 6) %>%
  filter(!(subid %in% e3_exclude$subid)) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e3_model_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0)

e3_subj_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = if_else(searchtype == "differentshapes",
                                      "different", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # time before selections can be made
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(utttype == "adj", "adjective noun", utttype),
         adjective = if_else(adjective == "noutt", "alien utterance", adjective),
         adjective = if_else(adjective == "noadj", "noun", adjective),
         adjective = factor(adjective, levels = c("noun", "adjective noun", "alien utterance")),
         searchtype = factor(searchtype, levels = c("contrast", "different")))

e3_mean_data <- e3_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e3_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e3_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed")
```


In Experiments 1 and 2, we established that people can use contrastive inferences to resolve referential ambiguity and to make inferences about the feature distribution of a novel category. Additionally, in Experiment 2, we found that these two inferences do not seem to trade off substantially: even if an adjective is necessary to establish reference, people infer that it also marks atypicality. We also found that inferences of atypicality about color and size adjectives pattern very similarly, though their baseline typicality is shifted, while color and size are not equally contrastive with respect to referential disambiguation.


To strengthen our findings in a way that would allow us to better detect potential trade-offs between these two types of inference, here we replicate Experiment 2 in a larger sample of participants.  [ some explanation of why the new control condition is interesting as well ...]

## Method

### Participants.

Four hundred participants were recruited from Amazon Mechanical Turk. Half of the participants were assigned to a condition in which the critical feature was color (red, blue, purple, or green), and half of the participants were assigned to a condition in which the critical feature was size (small or big).

### Stimuli & Procedure.

The stimuli and procedure were identical to those of Experiment 2, with the following modifications. Two factors, utterance type and object context, were fully crossed within subjects. Object context had two levels: within-category contrast and between-category contrast. In the within-category context condition, Alien B possessed the target object and another object of the same shape, but with a different value of the critical feature (color or size). In the between-category contrast condition, Alien B possessed the target object and another object of a different shape, and with a different value of the critical feature. Thus, in the within-category contrast condition, the descriptor is necessary to distinguish the referent; in the between-category contrast condition it is unnecessary but potentially helpful. There were three utterance types: adjective, no adjective, and alien utterance. In the two alien utterance trials, the aliens spoke using completely unfamiliar utterances (e.g., "Zem, noba bi yix blicket"). Participants were told in the task instructions that sometimes the aliens would talk in a completely alien language, and sometimes their language will be partly translated into English. To keep participants from making inferences about the content of the alien utterances using the utterance content of other trials, both alien language trials were first; other than this constraint, trial order was random. We manipulated the critical feature type (color or size) between subjects.

After completing the study, participants were asked to select which of a set of alien words they had seen previously during the study. Four were words they had seen, and four were novel lure words. Participants were dropped from further analysis if they did not respond to at least 6 of these 8 correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level). Additionally, six participants were excluded because their trial conditions were not balanced due to an error in the run of the experiment. This resulted in excluding XX participants, leaving XX for further analysis.  

```{r mean-data}
means <- e2_subj_data %>%
  group_by(condition, adjective, searchtype, subid) %>%
  gather(measure, value, percentage, log_rt) %>%
  group_by(condition, adjective, searchtype, measure, subid) %>%
  summarise(value = mean(value)) %>%
  tidyboot_mean(value)
```

```{r meanplotinference, fig.env = "figure", fig.width=8, fig.cap="The proportion of the novel category participants judged to have the feature of the target object, by condition. The left panel shows judgments on trials in which no adjective was used in the referring expression (e.g., 'Pass me the blicket'), and the right panel shows judgments on trials in which an adjective was used (e.g., 'Pass me the [purple/small] blicket'). This is crossed by the type of object context (contrast, different, same) on the x-axis."}
ggplot(filter(means, measure == "percentage"),
       aes(x = adjective, color = condition)) + 
  facet_wrap(~searchtype) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  ylab("Prevalence judgment") +
  xlab("Context type") +
  theme(text = element_text(size=20)) +
  scale_color_ptol()
```







```{r e3-models}
# all prereg'd models below

e3_utt_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(e3_utt_model_no_alien, "e3_adj_no_alien", "utttypeadj")

e3_utt_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e3_utt_model_adj", "e3_utt_model_noadj"), 
      c("utttypeadj", "utttypenoadj"), 
      ~ make_text_vars(e3_utt_model, .x, .y))

# model did not converge with maximal slopes
e3_full_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

walk2(c("e3_full_noadj", "e3_full_adj", "e3_full_size", "e3_full_diffshapes", "e3_full_adj_size"), 
      c("utttypenoadj", "utttypeadj", "conditionsize", "searchtypedifferentshapes", "utttypeadj:conditionsize"), 
      ~ make_text_vars(e3_full_model, .x, .y))

e3_full_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

make_text_vars(e3_full_model_no_alien, "e3_noalien_adj", "utttypeadj")

```


```{r e3-rsa}
e3_null_inference <- webppl(program_file = 
                                         here("webppl/e3_null.wppl"))

e3_wppl_null_data <- e3_null_inference %>%
  pivot_wider(values_from = "value", names_from = "Parameter") %>%
  mutate(p = as.numeric(p)) %>%
  group_by(obj) %>%
  summarise(p = 100 * mean(p)) %>%
  filter(obj == "red toma") %>%
  rename(empirical_stat = p) %>%
  mutate(adjective = "alien utterance") %>%
  select(-obj) %>%
  expand_grid(condition = c("color", "size"),
              searchtype = c("different", "contrast"))
  
```

```{r extra-models}
no_baseline_model <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  lmer(percentage ~ utttype + searchtype  + condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

size_only_model <- e3_model_data %>%
  filter(condition == "size") %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ searchtype * utttype +  
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

```


```{r e3-wppl-plot, eval = FALSE}
e3_wppl_data <- e2_wppl_data %>%
  bind_rows(e3_wppl_null_data) %>%
  filter(searchtype != "same")

ggplot(e3_mean_data, aes(x = adjective, y = empirical_stat - 50, 
                         color = searchtype, fill = searchtype)) + 
  facet_wrap(~ condition) +
  geom_pointrange(aes(ymin = ci_lower - 50, ymax = ci_upper - 50), 
                  position = position_dodge(.5)) + 
  theme(legend.position = "top") + 
  geom_col(data = e3_wppl_data, width = .5, 
           position = position_dodge(.5), alpha = .5) + 
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  scale_y_continuous(labels = function(y) y + 50, )

```


## Results

We began by fitting a pre-registered maximum mixed-effects linear model: effects utterance type (alien utterance, adjective, or no adjective; alien utterance as reference level), context type (within category or between category), and critical feature (color or size) as well as all interactions and random slopes of utterance type and context type nested within subject. Random effects were removed until the model converged, which resulted in a model with all fixed effects, all interactions and a random slope of utterance type by subject. The final model revealed a significant effect of the no adjective utterance type compared to the alien utterance type ($\beta =$ `r e3_full_noadj_estimate`, $t =$ `r e3_full_noadj_statistic`, $p =$ `r e3_full_noadj_p.value`) and a marginal effect of the adjective utterance type compared to the alien utterance type ($\beta =$ `r e3_full_adj_estimate`, $t =$ `r e3_full_adj_statistic`, $p =$ `r e3_full_adj_p.value`). The effects of context type and adjective type were not significant ($\beta_{between} =$ `r e3_full_diffshapes_estimate`, $t_{between} =$ `r e3_full_diffshapes_statistic`, $p_{between} =$ `r e3_full_diffshapes_p.value`; $\beta_{size} =$ `r e3_full_size_estimate`, $t_{size} =$ `r e3_full_size_statistic`, $p_{size} =$ `r e3_full_size_p.value`), and there was a significant interaction between the adjective utterance type and the size condition ($\beta =$ `r e3_full_adj_size_estimate`, $t =$ `r e3_full_adj_size_statistic`, $p =$ `r e3_full_adj_size_p.value`). Thus, participants inferred that an object referred to in an intelligible utterance with no description was more typical of its category on the target feature than an object referred to with an alien utterance. They also inferred that an object referred to in an intelligible utterance with description was marginally less typical than an object referred to with an alien utterance, and this effect was slightly stronger in the size condition. They did not substantially adjust their inferences based on the object context.

Given that interpretation of these results with respect to the alien utterance condition can be difficult, we pre-registered a version of the same full model excluding alien utterance trials. This model revealed a significant effect of utterance type: participants' prevalence judgments were lower when an adjective was used than when it was not ($\beta =$ `r e3_noalien_adj_estimate`, $t =$ `r e3_noalien_adj_statistic`, $p =$ `r e3_noalien_adj_p.value`). No other effects were significant. This replicates the main effect of interest in Experiment 2: that when an adjective is used in referring to the object, participants infer that the described feature is less typical of that object's category than when the feature goes unmentioned. 



# Discussion

In Experiment 3, we replicated the main finding of interest in Experiment 2: when a novel object's feature is described, people infer that the feature is rarer of its category than when it goes unmentioned. Again, this effect was consistent across both size and color adjectives, and people did not substantially adjust this inference based on how necessary the description was to distinguish among potential referents. We also added an alien language condition, in which the entire referring expression was unintelligible to participants, to probe people's priors on feature typicality. We found that in the alien language condition, people judged features to be roughly between the adjective utterance and no adjective utterance conditions, and significantly different from the no adjective utterance condition. In the alien language condition, people's prevalence judgments were roughly around our model's prevalence judgments after observing the objects on each trial and before any inferences about the utterance.

The similarity of people's prevalence judgments on the alien language condition raises the question: is this effect driven by an atypicality inference in the adjective conditions, or a *typicality* effect when the feature is unmentioned? Our results suggest that it is a bit of both. When someone mentions an object without extra description, the listener can infer that its features are likely more typical than their prior; when they use description, they can infer that its features are likely less typical. Because using an extra word--an adjective--is generally not thought of as the default way to refer to something, this effect is still best described as a contrastive inference of atypicality when people use description. However, the fact that people infer high typicality when an object is referred to without description suggests that, in some sense, there is no neutral way to refer: people will make broader inferences about a category from even simple mentions of an object.


# General Discussion

Overall, we found that people are able to use descriptive contrast to infer the referent of a novel word and to make inferences about a novel referent’s category. In our first experiment, participants were able to resolve referential ambiguity using a contrastive interpretation of size adjectives, though not reliably with color adjectives. In our second and third experiments, participants inferred that a described referent was atypical of its category on that feature: hearing "big toma" led them to think that most tomas were not that size. In real life it is often unclear whether description is meant to contrast with present objects or imply atypicality. In Experiments 2 and 3, participants did not significantly adjust their prevalence judgments based on the interaction of adjective use and object context—-that is, they did not adjust their inferences about typicality based on how redundant description was in context. Further, contexts in which description was necessary to identify the referent did not preempt inferences of atypicality. 

In Experiment 1, participants notably failed to use color adjectives contrastively in choosing referents. What makes size different from color? One possibility is that color adjectives are often used redundantly, and therefore receive less contrastive weight than adjectives consistently used to differentiate between referents. Sedivy (2003) puts forth such an account, finding that color adjectives tend not to be interpreted contrastively in eye-tracking measures except in contexts that make their use unlikely. In comparison, adjectives describing material (e.g., plastic) and size are interpreted contrastively, which corresponds to less redundant use of material and size adjectives in production [@sedivy_pragmatic_2003-2]. Further work is necessary to determine whether contrastive inferences hew to production norms, and whether implicit indications of contrast usually extend to explicit referent choice.

In Experiment 2, we asked whether utterances like "Pass me the blue dax" lead people to infer that daxes are generally less likely to be blue. We found that people robustly infer that mentioned features are atypical of the object's category, across both color and size adjectives and in varying object contexts. 

In Experiment 3, we replicated Experiment 2 and asked what kinds of inferences people make about novel object typicality when they cannot understand the referring expression. We found that people tend to infer that the feature is as prevalent as their direct experience would suggest, around the same as our model's estimate after observation of the objects and before hearing an utterance. This is significantly less than their prevalence judgment when they hear the object referred to with a noun and no adjective (e.g., "Pass me the dax"). That is, people infer that an object is fairly typical when it is referred to in a sentence they understand, but think it is less typical—only as typical as their prior indicates—when it is referred to in a completely incomprehensible utterance. This suggests that even simple mentions, such as "Pass me the toma," prompt inferences about the typicality of the object in its category (namely, that this toma is typical). While the effects we show here are appropriately described as atypicality inferences from description, this result suggests that people's inferences about typicality are not simply inferring 'markedness' from the use of an adjective; any mention of an object can engender inferences about its category.

The relative robustness of contrastive inferences about typicality across contexts and adjective types compared to contrastive inferences among present referents raises questions about the relative importance of these two kinds of contrast in language understanding. Most prior work has focused on contrast with present referents as the main phenomenon of interest, with object typicality as a modulating factor; our results emphasize the role of contrast with an object’s category, particularly when ambiguity is at play. A reference-first view of utterance interpretation might predict that use of description would be largely explained away if the description was necessary for reference (e.g., the 'red' in 'red dax' is explained by a blue dax being present to distinguish from). Contrary to this possibility, we find that both our participants and a probabilistic model that integrates both referential utility and typicality make inferences of atypicality when the adjective was necessary to establish reference. The model slightly weakens its inference of atypicality in this case, and participants' inferences do not significantly differ based on object context. Future work will explore whether people make subtle trade-offs between contrast with present referents and with the referent’s category.

[add RSA stuff]

Though the participants in our experiments were adults, the ability to disambiguate novel referents using contrast most obviously serves budding language learners: children. Contrastive use of adjectives is a pragmatic regularity in language that children could potentially exploit to establish word--referent mappings. Further, use of adjectives has been shown to allow children to make contrastive inferences among familiar present objects (Huang & Snedeker, 2008) and, when paired with contrastive cues such as prosody, about novel object typicality (Horowitz & Frank, 2016); future work will explore whether adjective contrast alone is a viable learning tool in early childhood. Tasks using a mixture of novel adjectives and words suggest that children as young as 3 can make contrastive inferences about adjectives [@gelman_implicit_1985; @diesendruck_childrens_2006; @huangsnedeker2008]. Contrastive inferences allow people to learn the meanings of new words and the typical features of new categories, pointing to a broader potential role of pragmatic inference in learning about the world. 

# Conclusion

Taken together, these experiments show that people use contrastive inference to map novel words to novel referents and to make inferences about the typicality of novel referents' features. Hearing "small toma" allows people to narrow possible referents not only to small objects, but objects with larger counterparts nearby. Hearing "big toma" in a referential context leads them to think that most tomas are not that size. However, these two abilities do not appear to interact. A referential felicitous use of description does not block an inference of atypicality. These results do not yet provide an explanation of *why* these skills do not interact: the inference may be too complex, the stimuli too novel, or listeners may use contrast more heuristically than rational models of pragmatic inference assume [@frank2012]. Understanding the origins of these independent but non-interpendent inferential abilities, as well as asymmetries between comprehension and production and adjectives like color and size, will be an important next challenge in our development of theories of human pragmatic inference.

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to the last author.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
