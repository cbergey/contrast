---
title             : "Using contrastive inferences to learn about new words and categories"
shorttitle        : "Learning from contrastive inference"

author: 
  - name          : "Claire Bergey"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S. University Avenue, Chicago, IL 60637"
    email         : "cbergey@uchicago.edu"
  - name          : "Dan Yurovsky"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "The University of Chicago"
  - id            : "2"
    institution   : "Carnegie Mellon University"

 

abstract: |
  In the face of unfamiliar language or objects, description is one cue people can use to learn about both. Beyond narrowing potential referents to those that match a descriptor, listeners could infer that a described object is one that contrasts with other relevant objects of the same type (e.g., ``The tall cup'' contrasts with another, shorter cup). This contrast may be in relation to other present objects in the environment or to the referent’s category. In three experiments, we investigate whether listeners use descriptive contrast to resolve reference and make inferences about novel referents' categories. People use size adjectives contrastively to guide referent choice, though they do not do so using color adjectives (Experiment 1). People also use description to infer that a novel object is atypical of its category (Experiment 2). However, these two inferences do not trade off substantially: people infer a described referent is atypical even when the descriptor was necessary to establish reference. We model these experiments in the Rational Speech Act (RSA) framework and find it predicts both of these inferences. Overall, people are able to use descriptive contrast to resolve reference and make inferences about a novel object’s category, allowing them to learn more about new things than literal meaning alone allows."

authornote: |
  All data and code for these analyses are available at https://osf.io/3f8hy/?view_only=9a196db0444c4867bc899cc70a7a1e9c.


keywords          : "parent-child interaction; language development; communication"
wordcount         : "1385"
references        : "42"

bibliography      : ["contrast.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"    
output            : papaja::apa6_pdf
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = '!tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3)
set.seed(42)
options(digits=3, dplyr.summarise.inform = FALSE)
```

```{r libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(lme4)
library(broom)
library(broom.mixed)
library(here)
library(english)
library(ggthemes)
library(papaja)
library(gridExtra)
library(glue)
library(directlabels)
library(tidyboot)
library(lmerTest)
library(knitr)
library(rwebppl)
library(ggridges)

theme_set(theme_few(base_size = 10) + theme(legend.position = "none"))
logit <- function(x) {log(x/(1-x))}

```

```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```


# Introduction


When trying to communicate, human listeners are faced with uncertainty. Novice listeners---children---face a continuous speech stream filled with unknown words referring to unformed concepts. Even seasoned listeners---adults---contend with noise, variable pronunciation, ambiguous meanings, unfamiliar objects, and the occasional unknown word, too. Fortunately, listeners bring sensitive phonetic, syntactic, and semantic skills to the task, allowing them to reduce ambiguity during conversations and over developmental time. Most of these well-documented skills are concerned with the listener’s understanding of the speaker’s utterance alone. But communication occurs in context: in a rich world to which language refers. Listeners’ ability to combine utterance information with context---their pragmatic ability---may be a powerful tool in resolving referential ambiguity and learning about the concepts language describes.

One potential pragmatic tool for reducing referential uncertainty is contrastive inference. Contrastive inferences are those inferences that derive from the principle that description should discriminate. This principle falls out of the more general Gricean maxim that speakers should say as much as they need to say and no more [@grice1975logic]. To the extent that communicators strive to be minimal and informative, description should discriminate between the referent and some relevant contrasting set. This contrastive inference is fairly obvious from some types of description, such as some postnominal modifiers: "The door with the lock" clearly implies a contrasting door without one [@sedivy_invoking_2002; @sedivy_pragmatic_2003-2; @nietal]. The degree of contrast implied by more common descriptive forms, such as prenominal adjectives in English, is less clear. Speakers do not always use prenominal adjectives minimally, often describing more than is needed to establish reference [@pechmann_incremental_1989; @mangold_informativeness_1988; @engelhardt_speakers_2006]. How, then, do listeners interpret these descriptions? 

Sedivy and colleagues carried out a visual world task demonstrating that adults interpret at least some prenominal adjective use as contrastive [@sedivy_achieving_1999]. In their task, four objects appeared on a screen: a target (e.g., a tall cup), a contrastive pair (e.g., a short cup), a competitor that shares the target’s feature but not category (e.g., a tall pitcher), and an irrelevant distractor. Participants then heard a referential expression: “Pick up the tall cup.” Adults looked more quickly to the correct object when the utterance referred to an object with a same-category contrastive pair (tall cup vs. short cup) than when it referred to an object without a contrastive pair (e.g., the tall pitcher). Their results suggest that listeners expect speakers to use prenominal description when they are distinguishing between potential referents of the same type, and listeners use this inference to rapidly allocate their attention to the target as an utterance progresses. This kind of inference can be derived from a rational speaker framework in which listeners reason that speakers using an utterance with a description, rather than one without, chose to do so to make a useful contribution to listener understanding [@frank2012]. This effect was demonstrated for size and material adjectives; the results for color adjectives were mixed [@sedivy_achieving_1999; @sedivy_pragmatic_2003-2]. **more discussion of color/size and typicality effects** More recently, this contrastive processing effect was replicated with 5-year-old participants using size adjectives [@huangsnedeker2008]. These experiments demonstrate that listeners interpret at least some prenominal adjectives contrastively, and use this contrastive inference to guide their attention allocation. These results leave open, however, whether listeners use prenominal adjective contrast to resolve referential ambiguity and explicitly guide their referent choice.

Beyond contrasting a referent with other objects in the environment, description may draw a contrast between a referent and its category. In production studies, participants tend to describe atypical features more than they describe typical ones [@mitchell_2013; @westerbeek_2015; @rubio-fernandez_how_2016]. For instance, they almost always include a color descriptor when referring to a blue banana, but not when referring to a yellow one. This, too, can be derived from a rational model of speaker behavior [@degen_when_2019] **is this the cite we want?** . How do listeners interpret such adjective use? Suppose someone hears a referring expression to an unfamiliar object: "Look at that red sprocket." In order to determine whether 'red' was used in contrast to other objects in the environment or to the referent's category, a rational listener must integrate contextual information. If there are many sprockets of different colors around, 'red' was likely used to pick out an individual sprocket. If not, it may have been used to mark the abnormality of this sprocket—perhaps it is rare for sprockets to be red. In this way, it is possible for listeners to make inferences about the category of a novel referent using descriptive contrast. **fix this description to make it clear this is an intuitive gloss**

In this paper, we present a series of experiments to test whether and how listeners make inferences about novel referents using descriptive contrast. First, we examine whether listeners use descriptive contrast to resolve referential ambiguity. In a reference game, participants see groups of novel objects and are asked to pick one with a referring expression, e.g., "Find the blue toma." If participants interpret description contrastively, they should infer that the description was necessary to identify the referent--that the blue toma contrasts with some other-colored toma on the screen. Using this contrastive inference, they can resolve referential ambiguity, choosing a blue object with a similar non-blue counterpart rather than a blue object with no similar counterpart nearby. Second, we test whether listeners use descriptive contrast to make inferences about a novel object's category. Participants are presented with two interlocutors who exchange objects using referring expressions, such as "Pass me the blue toma." If participants interpret description as contrasting with an object's category, they should infer that in general, few tomas are blue. However, context should matter in these judgments: if the descriptor was necessary to identify the referent, an inference of contrast with the category is unwarranted. **fix this last sentence**

In order to determine whether adults can use prenominal adjective contrast to disambiguate referents, and how those inferences are affected by adjective type, we use a reference game with novel objects. Novel objects provide both a useful experimental tool and an especially interesting testing ground for contrastive inferences. These objects avoid effects of typicality and familiarity that relate to level of description in production [@pechmann_incremental_1989; @rubio-fernandez_how_2016] on particular features [@mangold_informativeness_1988]. **check cites** They have unknown names and feature distributions, creating the ambiguity necessary for our test of referential disambiguation. But the ability to disambiguate novel referents, or to establish reference with incomplete information, is also the broader problem of learning about the world. This skill would aid not only adult speakers dealing with ambiguous or degraded communicative signal, but also children who need to establish new word--referent mappings. Across the developmental span, contrastive inference could help listeners exploit regularities in language and their environment to learn about both.

# Experiment 1   

In Experiment 1, we test whether adult participants use prenominal adjective contrast to choose a novel referent. To examine whether contrast occurs across adjective types, we test participants in two conditions: color contrast and size contrast. In a task similar to that of Sedivy and colleagues (1999), we present participants with arrays of novel fruit objects. On critical trials, participants see a target object, a lure object that shares the target’s contrast feature but not its shape, and a contrastive pair that shares the target’s shape but not its contrast feature. Participants hear an utterance denoting the feature: "Find the [blue/big] dax." For the target object, use of the adjective is necessary to disambiguate from the same-shape distractor; for the lure, the adjective would be superfluous description. If participants use contrastive inference to choose novel referents, they should choose the target object more often than the lure. However, we do not expect listeners to treat color and size equally. Because color is often used redundantly in English while size is not [@pechmann_incremental_1989; @nadig_evidence_2002], we expect size to hold more contrastive weight, encouraging a more consistent contrastive inference. **cite rubio fernandez**

```{r colortrial, fig.env = "figure", fig.pos = "H", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "On the left: an example of a contrastive trial in which the critical feature is size. Here, the participant would hear the instruction ``Find the small dax.'' On the right: an example of a contrastive trial in which the critical feature is color. Here, the participant would hear the instruction ``Find the red dax.'' In both cases, the target is the top object."}
img <- png::readPNG(here("writing/figs/sizecolorcontrast.png"))
grid::grid.raster(img)
```


## Method

```{r load-data}
e1_data <- read_csv(here("data/exp1_turk_data.csv")) 

e1_keep_subjs <- e1_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e1_data_no_gather <- e1_data %>%
  filter(subid %in% e1_keep_subjs$subid,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_data %>%
  filter(subid %in% e1_keep_subjs$subid,
         trialtype != 0) %>%
  gather(item, chose, chosetarget, choselure, choseunique) %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose), n = n()) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 
```



### Participants.

300 participants were recruited from Amazon Mechanical Turk.  participants were assigned to a condition in which the critical feature was color (stimuli contrasted on color), and  participants were assigned to a condition in which the critical feature was size.


### Stimuli.

Stimulus displays were arrays of three novel fruit objects. Fruits were chosen randomly at each trial from 25 fruit kinds. Ten of the 25 fruit drawings were adapted and redrawn from @kanwisher; we designed the remaining 15 fruit kinds. Each fruit kind has an instance in each of four colors (red, blue, green, or purple) and two sizes (big or small). Particular colors and sizes were assigned randomly at each trial. **true of sizes?** There were two display types: unique target displays and contrastive displays. Unique target displays contain a target object that has a unique shape and is unique on the trial's critical feature (color or size), and two distractor objects that match each other's (but not the target's) shape and critical feature. Contrastive displays contain a target, its contrastive pair (matches the target's shape but not critical feature), and a lure (matches the target’s critical feature but not shape). The positions of the target and distractor items were randomized within a triad configuration.

```{r e1-fig, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Proportion of times that participants chose the target and lure items as a function of condition and whether an adjective was provided. Points indicate group means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping."}

condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )
e1_mean_data %>%
  mutate(empirical_stat = if_else(searchtype == "uniquetarget" & 
                                    item == "lure", as.double(NA), empirical_stat),
         item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(condition ~ searchtype, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("") + 
  geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")
```

### Design and Procedure.

Participants were told they would play a game in which they would search for strange alien fruits. Each participant saw eight trials. Half of the trials were unique target displays and half were contrastive displays. Crossed with display type, half of trials had audio instructions that described the critical feature of the target (“Find the [blue/big] dax”), and half of trials had audio instructions with no adjective description (“Find the dax”). A name was randomly chosen at each trial from a list of eight nonce names: blicket, wug, toma, gade, sprock, koba, zorp, and lomet. **check names**

## Results


```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)


glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest() %>%
  filter(effect == "fixed")

make_text_vars(glmer_chance_comparison, "condition")

glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```

We first confirmed that participants understood the task by analyzing performance on trials in which there was a target unique on both shape and the relevant adjective. We asked whether participants chose the target more often than expected by chance ($33\%$) by fitting a mixed effects logistic regression with an intercept term, a random effect of subject, and an offset of $logit(1/3)$ to set chance probability to the correct level. The intercept term was reliably different from zero for both color ($\beta =$ , $t =$ , $p$ ) and size ($\beta =$ , $t =$ , $p$ ). In addition, participants were more likely to select the target when an adjective was provided in the audio instruction in both conditions. We confirmed this effect statistically by fitting a mixed effects logistic regression predicting target selection from condition, adjective use, and their interaction with random effects of participants. Adjective type (color vs. size) was not statistically related to target choice ($\beta =$ , $p =$ ), and adjective description in the audio increased target choice ($\beta =$ , $t =$ , $p$ ). The two effects did not interact ($\beta =$ , $t =$ , $p =$ ). Participants had a general tendency to choose the target in unique target trials, which was strengthened if the audio instruction contained the relevant contrast adjective.

```{r e1-models-contrast}
# in contrast trials w/an adjective, do people choose the target over the lure?
# prereg'd
chance_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(group == "fixed")


# in only *color* contrast trials w/an adjective, do people choose the target over the lure?
# not prereg'd
color_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "color",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(group == "fixed")


# in contrast trials w/an adjective, 
# does the type of adjective matter in choosing target over lure?
# prereg'd
adj_type_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(group == "fixed")



# in contrast trials, do adj type and presence of an adj interact 
# in determining target over lure choice?
# not prereg'd
adj_by_adjtype_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition * adj + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(group == "fixed")


# throw everything in the model
# prereg'd
full_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast" | searchtype == "uniquetarget") %>%
  glmer(chosetarget ~ adj * condition * searchtype + (searchtype * adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(group == "fixed")
```

Our key test was whether participants would choose the target object on contrastive trials in which description was given, reflecting use of a contrastive inference to choose a novel referent. To do this, we compare participants' rate of choosing the target to their rate of choosing the lure, which shares the relevant contrast feature with the target, when the audio described the contrast feature. Participants chose the target more than the lure in the size condition ($\beta =$, $t =$, $p =$). However, participants in the color condition did not choose the target significantly more often than they chose the lure ($\beta =$ , $t =$ , $p =$ ). On contrastive trials in which a descriptor was not given, participants dispreferred the target, instead choosing the lure object, which matched the target on the descriptor but had a unique shape; this was true across color ($\beta =$ , $t =$ , $p =$ ) and size ($\beta =$ , $t =$ , $p =$ ) conditions. Adjective use therefore increased target choice ($\beta =$ , $t =$ , $p$ ) across contrastive trials. Participants' choice of the target in the size condition was therefore not due to a prior preference for the target in contrastive displays, but relied on contrastive interpretation of the adjective. 

## Discussion

When faced with unfamiliar objects referred to by unfamiliar names, people must resolve ambiguity to understand their conversational partner and learn more about the lexicon. In Experiment 1, we tested whether people could use contrastive inferences to resolve ambiguous reference to novel objects. We find that participants have a general tendency to choose objects that are unique in shape when reference is ambiguous. However, when people hear an utterance with description (e.g., "blue toma", "small toma"), they shift away from choosing unique objects and toward choosing objects that have a similar contrasting counterpart. Furthermore, use of size adjectives--but not color adjectives--prompts people to choose the target object with a contrasting counterpart more often than the unique lure object. We find that people are able to use contrastive inferences about size to successfully resolve which unfamiliar object an unfamiliar word refers to. 

## Model 1

To formalize the inference that participants were asked to make, we developed a model in the Rational Speech Act Framework  [RSA, @frank2012]. In this framework, pragmatic listeners ($L$) are modeled as drawing inferences about speakers' ($S$) communicative intentions in talking to a hypothetical literal listener ($L_{0}$. This literal listener makes no pragmatic inferences at all, evaluating the literal truth of statements, and chooses randomly among all referents consistent with a statement (e.g. it is true that a red toma can be called "toma" and "red toma" but not "blue toma"). In planning their referential expressions, speakers choose utterances that are successful at accomplishing two goals: (1) Making the listener as likely as possible to select the correct object, and (2) minimizing their communicative cost (i.e. producing as few words as possible). Pragmatic listeners use Bayes' rule to invert the speaker's utility function, essentially inferring what the speaker's intention was likely to be given the utterance they produced. 

$$Literal: P_{Lit} = \delta\left(u,r\right)P\left(r\right)$$
$$Speaker: P_S\left(u \vert r\right) \propto \alpha \left(P_{Lit}\left(r \vert u\right) - C\right)$$

$$Listener: P_{Learn}\left(r \vert u\right) \propto P_s\left(u \vert r\right)P\left(r\right)$$

This computation naturally predicts a number of phenomena in pragmatics. For example, RSA explains scalar implicature--listeners treat "I ate some of the cookies" as a poor description of a case where the speaker at all of the cookies. The speaker's statement is literally true--the speaker eating some of the cookies is consistent with a world in which they ate all of them. However, this statement is ambiguous--it is true of both the world in which some cookies remain and the world in which there are no cookies left. Thus, if the speaker intends to convey that they ate all of the cookies, saying "I ate some of the cookies" will cause the literal listener to guess the wrong world half of the time. In contrast, the statement "I ate all of the cookies" is consistent only with world in which all of the cookies were eaten. Thus, if the speaker ate all of the cookies, this statement would accomplish their goal of communicating the state of the world more effectively. Scalar implicature arises from exactly this inference: If the speaker actually ate all of the cookies, they should have said "I ate all of the cookies" because that would be a more effective utterance than "I ate some of the cookies." Since they produced "some," it is more likely that they wanted to communicate about the world in which cookies remain [@frank2012]. 
**do we need all of this RSA intro?**
Extensions of this framework have successfully accounted for a variety of other pragmatic inferences, including  inference that speech is hyperbolic (e.g. waiting "a million years" means waiting a long time), inferring when speakers are being polite rather than truthful, and learning new words in ambiguous contexts [@goodman2014; @kao2014; @yoon2016; @frank2014]. Further, a recent extension of the framework using continuous rather than discrete semantics has given an account of the kinds of differences between color and size modification that we observed in our experimental data [@degen_when_2020]. 

For this experiment, we build on a Rational Speech Act model developed by @frank2014 to jointly resolve reference and learn new words. The primary extension of RSA is that the pragmatic learner is a pragmatic listener who has has uncertainty about the meanings of words in their language, and thus cannot directly compute the speaker's utility as written. Instead, the speaker's utility is conditioned on the set of mappings, and the learners must also infer which set of mappings is correct:  

$$Learner: P_L\left(r \vert u\right) \propto P_s\left(u \vert r; m\right)P\left(r\right)P\left(m\right)$$

In these experiments, we assume that the prior probability to refer to each object $(P\left(r\right))$ is equal, and similarly that all mappings $(P\left(m\right))$ are equally likely, so they cancel out in computations. We further assume that the cost of producing any word is identical, and so the cost of an utterance is equal to its length. All that remains is to specify the possible mappings, and literal meanings, and alternative utterances possible on each trial of the experiment. We describe the size condition here, but the computation for the color condition is analogous. 

On the trial shown in the left panel of Figure \ref{fig:e1-fig} people see two objects that look something like a hair dryer and one that looks like a pear and they are asked to "find the dax." On the assumption that nouns generally refer to shapes, the two possible mappings are $\{m_1: hair dryer-``dax'', pear-``?''\}$, and $\{m_2: hair dryer-``?'', pear-``dax''\}$ The literal semantics of each object allow them to be referred to by their shape label (e.g. "dax"), or by a descriptor that is true of them (e.g. "small"), but not names for other shapes or untrue descriptors.

Having heard "Find the dax," the model must now choose a referent. If the true mapping for "dax" is the hair dryer ($m_1$), this utterance is ambiguous to the literal listener, as there are two referents consistent with the literal meaning dax. Consequently, whichever of the two referents the speaker intends to point out to the learner, the speaker's utility will be relatively low. In constrat, if the true mapping for "dax" is the pear ($m_1$), then the utterance will be unambiguous to the literal listener, and thus the speaker's utterance will have higher utility. As a result, the model can infer that the more likely mapping is $m_2$ and choose the pear, simultaneously resolving reference and learning the meaning of "dax."

If instead the speaker produced "find the small dax," the model will make a different inference. If the true mapping for "dax" is hair dryer ($m_2$), this utterance now uniquely identifies one referent for the literal listener and thus has high utility. It is also uniquely identifies the target if "dax" means pear ($m_1$). However, if "dax" means pear, the speaker's utterance was inefficient because the single word utterance "dax" would have identified the target to the literal listener and incurred less cost. Thus, the model can infer that "dax" is more likely to mean hair dryer and choose the small hair dryer appropriately.

While these descriptions use deterministic language for clarity, the model's computation is probabilistic and thus reflects tendencies to choose those objects rather than fixed rules. Figure \ref{fig:e1-webppl-plot} shows model predictions alongside people's behavior for the size and color contrast conditions in Experiment 1. In line with the intuition above, the model predicts that hearing a bare noun (e.g. "dax") should lead people to infer that the intended referent is the unique object (lure), whereas hearing a modified noun (e.g. "small dax") should lead people to infer that the speaker's intended referent is the target. 

```{r e1-webppl}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))

two_world_inference <- map_dfr(two_world_utterances %>% pull(utterance),
                               ~webppl(program_file =
                                         here("webppl/discrete_semantics.wppl"),
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 
  

e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj")) %>%
  mutate(prob_min = prob, prob_max = prob)
```

```{r e1-webppl-plot, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Proportion of times that people (and our model) chose the target and lure items as a function of adjective type and whether an adjective was provided. Points indicate empirical means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping. Solid lines show model predictions."}
ggplot(e1_webppl_data, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "item chosen")
```

Because the model we described has no way of distinguishing between color and size adjectives, making the same predictions for both. Based on our pilot studies, we pre-registered and observed an asymmetry in which contrastive inferences would be stronger for size than color. Why do we see an asymmetry in people? One possibility is that the semantics of color and size work differently. A recent model from @degen_when_2020 does predict a color--size asymmetry based on different semantic exactness. In this model, literal semantics are treated as continuous rather than discrete, so "blue" is neither 100% true or 100% false of a particular object, but can instead be 90% true. They successfully model a number of color--size asymmetries by treating color as having stronger literal semantics (i.e. "blue dax" is a better description of a small blue dax than "small dax" is). However, this model predicts the opposite asymmetry of what we found. Because color has stronger semantics than size, listeners show a stronger contrast effect for color than size. We show this effect in appendix A. Thus, though a continuous semantics can explain our asymmetry, this explanation is unlikely given the continuous semantics that predicts other empirical color--size asymmetries does not predict our findings.

Another possible explanation for this asymmetry is that people are aware of production asymmetries between color and size. As mentioned, speakers tend to over-describe color, providing more color adjectives than necessary to establish reference, while describing size more minimally [@pechmann_incremental_1989; @nadig_evidence_2002]. Listeners may be aware of this production asymmetry and discount the contrastive weight of color adjectives with respect to reference. This account explains our observed color--size asymmetry but is somewhat unsatisfying, raising the question: why there is a production asymmetry in the first place? For now, we bracket this question and note that listeners in our task appropriately discount color's contrastive weight given production norms.

# Experiment 2

```{r e2-read-data}
e2_color_data <- read_csv(here("data/exp2/color.csv")) %>%
  mutate(condition = "color", targetsize = "big") %>%
  rename(adj = colorasked, distractorfeature = distractorcolor)

e2_size_data <- read_csv(here("data/exp2/size.csv")) %>%
  mutate(condition = "size") %>%
  rename(adj = sizeasked, distractorfeature = distractorsize)

e2_data <- rbind(e2_color_data, e2_size_data)

e2_keep_subjs <- e2_data %>%
  filter(searchtype == "attncheck", attncheckscore >= 6) %>%
  group_by(subid) 

e2_model_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0)

e2_subj_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  rename(adjective = adj) %>%
  mutate(searchtype = if_else(searchtype == "polychrome" | 
                                searchtype == "differentsizes",
                                      "different", searchtype)) %>%
  mutate(searchtype = if_else(searchtype == "monochrome" |
                                searchtype == "samesize",
                                      "same", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(adjective == TRUE, "adjective noun", "noun"),
         adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("contrast", "different", "same")))

e2_mean_data <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)


ggplot(e2_subj_data, aes(x = adjective, y = percentage)) + 
  facet_grid(condition ~ searchtype) + 
  geom_boxplot()
```

In our first experiment, we examined whether adult listeners would interpret description as implying contrast with other present objects. However, as discussed earlier, description can imply contrast with sets other than the set of currently available referents. One of these alternative sets is the referent's category. Work by @mitchell_2013 and @westerbeek_2015 demonstrates that speakers use more description when referring to objects with atypical features (e.g., a yellow tomato) than typical ones (e.g., a red tomato). This marking of atypical objects potentially supplies useful information to listeners: they have the opportunity to not only learn about the object at hand, but also about its broader category. In the following experiment, we test whether listeners use this type of contrast to learn about unfamiliar objects' categories. 

In our first experiment, we found that participants treat color and size adjectives as having different contrastive weight with respect to reference. We posit that this is due to production asymmetries: speakers tend to produce more color adjectives that are superfluous to establishing reference, and describe size more minimally. However, color adjectives that are redundant with respect to reference are not necessarily redundant in general. @rubio-fernandez_how_2016 demonstrates that speakers often use 'redundant' color adjectives to describe colors when they are central to the category's meaning (e.g., colorful t-shirts) or when they are atypical (e.g., a purple banana). Therefore, color may be no less contrastive with respect to the category's feature distribution. Based on this work as well as pilot studies, we predicted that color and size adjectives would both be interpreted contrastively with respect to typicality.

If listeners do make contrastive inferences about typicality, it may not be as simple as judging that an over-described referent is atypical. Description can serve many purposes. In the prior experiment, we investigated its use in contrasting between present objects. If a descriptor was needed to distinguish between two present objects, it may not have been used to mark atypicality. For instance, in the context of a bin of heirloom tomatoes, a speaker who wanted a red one in particular might specify that they want a "red tomato" rather than just asking for a "tomato." In this case, the adjective "red" is being used contrastively with respect to reference (as in Experiment 1), and not to mark atypicality. Thus, a listener who does not know much about tomatoes may  attribute the production of "red" to referential disambiguation given the context and not infer that red is an unusual color for tomatoes.

In Experiment 2, we used an artificial language task to set up just this kind of learning situation. We manipulated the contexts in which listeners hear adjectives modifying novel referents. We asked whether listeners rationally infer that these adjectives identify atypical features of the named objects, and whether the strength of this inference depends on the referential ambiguity of the context in which adjectives are used.

## Method

### Participants.
Two hundred and forty participants were recruited from Amazon Mechanical Turk. Half of the participants were assigned to a condition in which the critical feature was color (red, blue, purple, or green), and the other half of participants were assigned to a condition in which the critical feature was size (small or big).

### Stimuli & Procedure.
Stimulus displays showed two alien interlocutors, one on the left (Alien A) and one on the right (Alien B) side of the screen, each with two novel fruit objects beneath them. Alien A, in a speech bubble, asked Alien B for one of its fruits (e.g., "Hey, pass me the red gade.") Alien B replied, "Here you go!" and the referent disappeared from Alien B's side and reappeared on Alien A's side. 

Two factors, presence of the critical adjective in the referring expression and object context, were fully crossed within subjects. Object context had three levels: within-category contrast, between-category contrast, and same feature. In the within-category contrast condition (hereafter abbreviated as "contrast"), Alien B possessed the target object and another object of the same shape, but with a different value of the critical feature (color or size). In the between-category contrast condition (abbreviated as "different"), Alien B possessed the target object and another object of a different shape, and with a different value of the critical feature. In the same feature condition (abbreviated as "same"), Alien B possessed the target object and another object of a different shape but with the same value of the critical feature as the target. Thus, in the within-category contrast condition, the descriptor is necessary to distinguish the referent; in the between-category contrast condition it is unnecessary but potentially helpful; and in the same feature condition it is unnecessary and unhelpful. We manipulated the critical feature type (color or size) between subjects.

Participants performed six trials. After each exchange between the alien interlocutors, they made a judgment about the prevalence of the target's critical feature in the target object's category. For instance, after seeing a red blicket being exchanged, participants would be asked, "On this planet, what percentage of blickets do you think are red?" and answer on a sliding scale between zero and 100. In the size condition, participants were asked, "On this planet, what percentage of blickets do you think are the size shown below?" with an image of the target object they just saw available on the screen. 

After completing the study, participants were asked to select which of a set of alien words they had seen previously during the study. Four were words they had seen, and four were novel lure words. Participants were dropped from further analysis if they did not respond to at least 6 of these 8 correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level). This resulted in excluding XX participants, leaving XX for further analysis.  

## Results

We first analyzed participants' judgments of the prevalence of the target object's critical feature in its category. We began by fitting a maximum mixed-effects linear model: effects utterance type (adjective or no adjective), context type (contrast, different, or same), and critical feature (color or size) as well as all interactions and random slopes of utterance type and context type nested within subject. Random effects were removed until the model converged, and fixed effects were removed if they did not improve model fit. The final model revealed significant effects of utterance type ($\beta_{adjective} =$ , $t =$ , $p$ ), critical feature ($\beta_{size} =$ , $t =$ , $p$ ) and a marginally lower prevalence for same search type relative to contrast search type ($\beta_{same} =$ , $t =$ , $p =$ ). Prevalence judgments for different trials was not reliably different from contrast trials ($\beta_{different} =$ , $t =$ , $p =$ ). Participants robustly inferred that described features were less prevalent in the target's category than unmentioned features. This atypicality inference was marginally stronger for trials on which the distractor had the same feature as the target, making the descriptor particularly unhelpful, than on trials in which the descriptor was necessary to distinguish between two objects of the same type. Overall, however, participants failed to substantially adjust their inferences according to the context of the referring expression.

Thus, participants treated all adjectives as marked, and inferred lower typicality, regardless of whether they could felicitiously be interpreted as contrasting between potential target referents. But were participants nonetheless sensitive to this information in their response times? We investigated this question by analyzing participants' time to advance after seeing the aliens' referential exchange. Though this task was not speeded, we hypothesized that participants would advance more quickly after seeing referential exchanges that were easier to process. After dropping all response times less than 1 second and longer than 10 seconds, and log transforming them because of the right skew in response time data, we predicted participants' time to advance on each trial of the experiment from utterance type, context type, critical adjective type, and the interaction between utterance type and context type (\texttt{log(rt) $\sim$ adjective * search + type + (1 |subj)}). This model showed a reliable effect of utterance type ($\beta_{adjective} =$ , $t =$ , $p$ )--participants were faster when an a descriptor was provided despite having to process an additional word. There was no main effect of critical adjective type ($\beta_{size} =$ , $t =$ , $p =$ ), nor context type ($\beta_{different} =$ , $t =$ , $p =$ ; $\beta_{same} =$ , $t =$ , $p =$ ), but the interactions between utterance type and context type trended towards significance for both non-contrast searches ($\beta_{adjective*different} =$ , $t =$ , $p =$ ; $\beta_{adjecgive*same} =$ , $t =$ , $p =$ ). Directionally, these results indicate that participants took longer to process utterances which were under-described (contrast trials with no adjective) than those with appropriately no description, and processed trials with an appropriate level of description (contrast trials with an adjective) more quickly than those with superfluous description.

## Discussion

Description is often used not to distinguish among present objects, but to pick out an object's feature as atypical of its category. In Experiment 2, we asked whether people would infer that a described feature is atypical of a novel category after hearing it mentioned in an exchange. We find that 

## Model 2

To allow the Rational Speech Act Framework to capture inferences about typicality, we modified the Speaker's utility function to have an additional term: the listener's expected processing difficulty. Speakers may be motivated to help listeners to select the correct referent not just eventually but as quickly as possible. People are both slower and less accurate at identifying atypical members of a category as members of that category [@rosch_structural_1976, @dale_graded_2007]. If speakers account for listeners' processing difficulties, they should be unlikely to produce bare nouns to refer to low typicality exemplars (e.g. unlikely to call a yellow tomato "tomato"). This is roughly the kind of inference encoded in @degen2020's continuous semantics Rational Speech Act model. 

We model the speaker as reasoning about the listener's label verification process. Because the speed of verification scales with the typicality of a referent, a natural way of modeling it is as process of searching for that particular referent in the set of all exemplars of the named category, or alternatively of sampling that particular referent from the set of all exemplars in that category $P\left(r \vert Cat\right)$. On this account, speakers want to provide a modifying adjective for atypical referents because the probability of sampling them from their category is low, but the probability of sampling of them from the modified category is much higher (e.g. $P\left(\text{yellow tomato_i} \vert \text{tomatoes}\right) < P\left(\text{yellow tomato_i} \vert \text{yellow tomatoes}\right)$^{This is a generaliztaion of @xu2007's size principle to categories where exemplars are not equally likely}.
 
If speakers use this utility function, learners who do not know the feature distribution for a category can use speakers' production to infer it. Intuitively, speakers should prefer not to modify nouns with adjectives because they incur a cost for producing that adjective. If they did, it must be because they thought the learner would have a difficult time finding the referent from a bare noun alone because of typicality. To infer the true prevalence of the target feature in the category, learners combine the speaker's utterance with their prior beliefs about the prevalence distribution. We model the listener's prior about the prevalance of features in any category as a $\text{Beta}$ distribution with two parameters $\alpha$ and $\beta$ that encode the number of hypothesized prior psuedo-exemplars with the feature and without feature that the learner has previously observed (e.g. one red dax and one blue dax). We assume that the learner believes they have previously observed 1 hypothetical psuedo-examplar of each type, which is a weak symmetric prior indicating that the learner expects features to occur in half of all members of a category on average, but would find many levels of prevalence reasonably unsurprising. To model the learner's direct experience with the category, we add the observed instances in the experiment to these hypothesized prior instance. After observing one memember of the target category with the relevant feature and one without, the listeners prior is thus updated to be $\text{Beta}\left(2,\,2\right)$.

```{r e2-webppl-markedness}
markedness_utterances <- tibble(utterance = c("toma", "red toma"),
                                utterance_num = as.character(1:2))

markedness_inference <- map_dfr(markedness_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/markedness.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(markedness_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble() %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")))


markedness_means <- markedness_inference %>%
  group_by(utterance) %>%
  summarise(value = mean(value))
```

```{r e2-markedness, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Model estimates of typicality judgments for one object seen alone and labeled either [noun] or [adjective noun]."}
ggplot(markedness_inference, aes(x = utterance, y = value)) + 
  geom_violin() +
  scale_y_continuous(limits = c(0, 1)) +
  geom_crossbar(aes(ymin = value, ymax = value, y = value), 
                data = markedness_means, size = .5) + 
  labs(x = "utterance", y = "proportion of [nouns] that are [adjective]")
```



```{r e2-contrast-webbpl}
e2_contrast_utterances <- tibble(utterance = c("toma", "red toma"),
                                utterance_num = as.character(1:2))

e2_contrast_inference <- map_dfr(e2_contrast_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/e2_contrast.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(e2_contrast_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble() %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")))


e2_contrast_means <- e2_contrast_inference %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma") %>%
  group_by(utterance) %>%
  summarise(p = mean(p)) %>%
  mutate(searchtype = "contrast", condition = "color")
```


```{r e2-same-webppl}
e2_same_inference <- map_dfr(e2_contrast_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/e2_same.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(e2_contrast_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble() %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")))


e2_same_means <- e2_same_inference %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma", world == "target world") %>%
  group_by(utterance) %>%
  summarise(p = mean(p)) %>%
  mutate(searchtype = "same", condition = "color")
```


```{r e2-different-webppl}
e2_different_inference <- map_dfr(e2_contrast_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/e2_different.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(e2_contrast_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble() %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")))


e2_different_means <- e2_different_inference %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma", world == "target world") %>%
  group_by(utterance) %>%
  summarise(p = mean(p)) %>%
  mutate(searchtype = "different", condition = "color")
```

```{r}

e2_wppl_data <- bind_rows(e2_contrast_means, e2_same_means, 
                          e2_different_means) %>%
  mutate(p = 100 * p) %>%
  rename(empirical_stat = p, adjective = utterance)

ggplot(e2_mean_data, aes(x = adjective, y = empirical_stat, 
                         color = searchtype, fill = searchtype)) + 
  facet_wrap(~ condition) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(.5)) + 
  theme(legend.position = "top") + 
  geom_col(data = e2_wppl_data, width = .5, 
           position = position_dodge(.5), alpha = .5) + 
  geom_hline(aes(yintercept = 50), linetype = "dashed")
```

```{r joint-inference, eval = FALSE}
joint_utterances <- tibble(utterance = c("toma", "blue toma"),
                                utterance_num = as.character(1:2))

joint_inference <- map_dfr(joint_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/two_world_typicality.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(joint_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble()
```

```{r joint-analysis, eval = FALSE}
joint_data <- joint_inference %>%
  pivot_wider(names_from = "Parameter") %>%
  mutate(chosen = case_when(world == "two toma" ~ 
                           gsub("toma", "pair", obj),
                         world == "two dax" ~ 
                           gsub("toma", "single", obj))) %>%
  mutate(p = as.numeric(p),
         utterance = factor(utterance, 
                            levels = c("toma", "blue toma")))

joint_data_obj <- joint_data %>%
  group_by(utterance, chosen) %>%
  count() %>%
  group_by(utterance) %>%
  mutate(prob = n/sum(n))

ggplot(joint_data_obj, aes(x = chosen, y = prob, fill = chosen)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~ utterance) + 
  scale_fill_ptol(drop = FALSE) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "", y = "selection probability")

ggplot(joint_data, aes(x = p, y = chosen)) +
  facet_wrap(~ utterance) +
  stat_density_ridges(scale = 1, 
                      quantile_lines = TRUE, quantiles = 2) + 
  scale_fill_ptol() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(y = "object chosen", x = "proportion of tomas that are blue")


joint_data_subset <- joint_data %>%
  filter(!utterance %in% c("red dax"), chosen == "blue pair") %>%
  group_by(utterance, chosen) %>%
  summarise(p = mean(p)) %>%
  spread(utterance, p)

```

## Discussion


```{r e3-read-data}
e3_data <- read_csv(here("data/exp3_turk_data.csv"))

e3_keep_subjs <- e3_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, attncheckscore >= 6) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e3_model_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0)

e3_subj_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = if_else(searchtype == "differentshapes",
                                      "different", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # time before selections can be made
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(utttype == "adj", "adjective noun", utttype),
         adjective = if_else(adjective == "noutt", "alien utterance", adjective),
         adjective = if_else(adjective == "noadj", "noun", adjective),
         adjective = factor(adjective, levels = c("noun", "adjective noun", "alien utterance")),
         searchtype = factor(searchtype, levels = c("contrast", "different")))

e3_mean_data <- e3_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)
```

# Experiment 3

In Experiments 1 and 2, we established that people can use contrastive inferences to resolve referential ambiguity and to make inferences about the feature distribution of a novel category. Additionally, in Experiment 2, we found that these two inferences do not seem to trade off substantially: even if an adjective is necessary to establish reference, people infer that it also marks atypicality. We also found that inferences of atypicality about color and size adjectives pattern very similarly, though their baseline is shifted, while color and size are not equally contrastive with respect to referential disambiguation.

To strengthen our findings in a way that would allow us to better detect potential differences between color and size or trade-offs between these two types of inference, here we replicate Experiment 2 in a larger sample of participants. ... [ some explanation of why the new control condition is interesting as well ...]

## Method

### Participants.

Four hundred participants were recruited from Amazon Mechanical Turk. Two hundred were assigned to a condition in which the critical feature was color (red, blue, purple, or green), and 200 participants were assigned to a condition in which the critical feature was size (small or big).

### Stimuli & Procedure.

Stimulus displays showed two alien interlocutors, one on the left (Alien A) and one on the right (Alien B) side of the screen, each with two novel fruit objects beneath them. Alien A, in a speech bubble, asked Alien B for one of its fruits (e.g., "Hey, pass me the red gade.") Alien B replied, "Here you go!" and the referent disappeared from Alien B's side and reappeared on Alien A's side. 

Two factors, presence of the critical adjective in the referring expression and object context, were fully crossed within subjects. Object context had two levels: within-category contrast and between-category contrast. In the within-category contrast condition (hereafter abbreviated as "contrast"), Alien B possessed the target object and another object of the same shape, but with a different value of the critical feature (color or size). In the between-category contrast condition (abbreviated as "different"), Alien B possessed the target object and another object of a different shape, and with a different value of the critical feature. Thus, in the within-category contrast condition, the descriptor is necessary to distinguish the referent; in the between-category contrast condition it is unnecessary but potentially helpful. We manipulated the critical feature type (color or size) between subjects.

```{r e2-stimuli, fig.env = "figure", fig.pos = "H", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Experiment 2 stimuli"}
img <- png::readPNG(here("writing/figs/sizecolorcontrast.png"))
grid::grid.raster(img)
```

Participants performed six trials. After each exchange between the alien interlocutors, they made a judgment about the prevalence of the target's critical feature in the target object's category. For instance, after seeing a red blicket being exchanged, participants would be asked, "On this planet, what percentage of blickets do you think are the color shown below?" with an image of the target object they just saw available on the screen. They answered on a slider scale from 0 to 100.

After completing the study, participants were asked to select which of a set of alien words they had seen previously during the study. Four were words they had seen, and four were novel lure words. Participants were dropped from further analysis if they did not respond to at least 6 of these 8 correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level). This resulted in excluding XX participants, leaving XX for further analysis.  

```{r mean-data}
means <- e2_subj_data %>%
  group_by(condition, adjective, searchtype, subid) %>%
  gather(measure, value, percentage, log_rt) %>%
  group_by(condition, adjective, searchtype, measure, subid) %>%
  summarise(value = mean(value)) %>%
  tidyboot_mean(value)
```

```{r meanplotinference, fig.env = "figure", fig.width=8, fig.cap="The proportion of the novel category participants judged to have the feature of the target object, by condition. The left panel shows judgments on trials in which no adjective was used in the referring expression (e.g., 'Pass me the blicket'), and the right panel shows judgments on trials in which an adjective was used (e.g., 'Pass me the [purple/small] blicket'). This is crossed by the type of object context (contrast, different, same) on the x-axis."}
ggplot(filter(means, measure == "percentage"),
       aes(x = adjective, color = condition)) + 
  facet_wrap(~searchtype) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  ylab("Prevalence judgment") +
  xlab("Context type") +
  theme(text = element_text(size=20)) +
  scale_color_ptol()
```


```{r rtplotinference, fig.env = "figure", fig.width=8, fig.cap="The log reaction time participants took to advance after seeing the referential exchange, by condition."}

ggplot(filter(means, measure == "log_rt"),
       aes(x = adjective, color = condition)) + 
  facet_wrap(~searchtype) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  ylab("log reaction time") +
  xlab("Utterance type") +
  theme(text = element_text(size=20)) +
  scale_color_ptol()
```


```{r first-trial}
first_means <- e2_subj_data %>%
  group_by(condition, subid) %>%
  slice(1) %>%
  group_by(condition, adjective, searchtype, subid) %>%
  summarise(percentage = mean(percentage)) %>%
  tidyboot_mean(percentage)
# ggplot(first_means, aes(x = searchtype, color = type)) + 
#   facet_wrap(~adjective) + 
#   geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
#                       position = position_dodge(.5)) + 
#   scale_color_ptol()
```

```{r old-models}
model <- lmer(percentage ~ condition + adjective + searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data)

tidy_model <- tidy(model) %>%
  filter(effect == "fixed")

# old factors : 
rtmodel <- lmer(log_rt ~ adjective * searchtype + condition +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data)

tidy_rtmodel <- tidy(rtmodel) %>%
  filter(effect == "fixed")
```

```{r e3-models}
# all prereg'd models below

utt_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  lmer(percentage ~ utttype + (utttype|subid), data = .) %>%
  tidy(effects = "fixed")


utt_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")


# model did not converge with maximal slopes
full_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

```

```{r extra-models}
no_baseline_model <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  lmer(percentage ~ utttype + searchtype  + condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

size_only_model <- e3_model_data %>%
  filter(condition == "size") %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ searchtype * utttype +  
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

```


## Results

We first analyzed participants' judgments of the prevalence of the target object's critical feature in its category. We began by fitting a maximum mixed-effects linear model: effects utterance type (adjective or no adjective), context type (contrast, different, or same), and critical feature (color or size) as well as all interactions and random slopes of utterance type and context type nested within subject. Random effects were removed until the model converged, and fixed effects were removed if they did not improve model fit. The final model revealed significant effects of utterance type ($\beta_{adjective} =$ , $t =$ , $p$ ), critical feature ($\beta_{size} =$ , $t =$ , $p$ ) and a marginally lower prevalence for same search type relative to contrast search type ($\beta_{same} =$ , $t =$ , $p =$ ). Prevalence judgments for different trials was not reliably different from contrast trials ($\beta_{different} =$ , $t =$ , $p =$ ). Participants robustly inferred that described features were less prevalent in the target's category than unmentioned features. This atypicality inference was marginally stronger for trials on which the distractor had the same feature as the target, making the descriptor particularly unhelpful, than on trials in which the descriptor was necessary to distinguish between two objects of the same type. Overall, however, participants failed to substantially adjust their inferences according to the context of the referring expression.

Thus, participants treated all adjectives as marked, and inferred lower typicality, regardless of whether they could felicitously be interpreted as contrasting between potential target referents. But were participants nonetheless sensitive to this information in their response times? We investigated this question by analyzing participants' time to advance after seeing the aliens' referential exchange. Though this task was not speeded, we hypothesized that participants would advance more quickly after seeing referential exchanges that were easier to process. After dropping all response times less than 1 second and longer than 10 seconds, and log transforming them because of the right skew in response time data, we predicted participants' time to advance on each trial of the experiment from utterance type, context type, critical adjective type, and the interaction between utterance type and context type (\texttt{log(rt) $\sim$ adjective * search + type + (1 |subj)}). This model showed a reliable effect of utterance type ($\beta_{adjective} =$ , $t =$ , $p$ )--participants were faster when an a descriptor was provided despite having to process an additional word. There was no main effect of critical adjective type ($\beta_{size} =$ , $t =$ , $p =$ ), nor context type ($\beta_{different} =$ , $t =$ , $p =$ ; $\beta_{same} =$ , $t =$ , $p =$ ), but the interactions between utterance type and context type trended towards significance for both non-contrast searches ($\beta_{adjective*different} =$ , $t =$ , $p =$ ; $\beta_{adjecgive*same} =$ , $t =$ , $p =$ ). Directionally, these results indicate that participants took longer to process utterances which were under-described (contrast trials with no adjective) than those with appropriately no description, and processed trials with an appropriate level of description (contrast trials with an adjective) more quickly than those with superfluous description.


# Discussion

In this series of experiments, we asked whether listeners could use pragmatic contrast to resolve referential ambiguity and make inferences about a referent's category. In our first experiment, participants were able to use size adjectives contrastively to establish a novel word–referent mapping. Their contrastive inference goes beyond the implicit attention allocation shown in prior eye-tracking paradigms [@sedivy_achieving_1999; @huangsnedeker2008], determining explicit referent choice. This finding bolsters contrastive inference as a viable tool for referential disambiguation. In our second experiment, participants interpreted size and color adjectives contrastively to make inferences about a novel referent's category.

Participants failed, however, to use color adjectives contrastively in choosing referents. What makes size different from color? One possibility is that the scalar nature of size supports a contrastive interpretation. We tested whether using relative color adjectives (e.g., bluer, greyer) or adjectives describing value (bright, dark) on saturated and desaturated stimuli would encourage the contrastive inference. We also tested whether adding a prosodic cue to contrast (e.g., “Find the *blue* dax”) would encourage contrastive inference. Participants persisted in interpreting color non-contrastively, never consistently choosing the intended target over the lure. Though we do not claim that contrastive color inferences cannot be used to explicitly choose referents, it seems that a contrastive interpretation is difficult to elicit using color, while it emerges under similar conditions using size.

Another possibility is that color adjectives are often used redundantly, and therefore receive less contrastive weight than adjectives consistently used to differentiate between referents. Sedivy (2003) puts forth such an account, finding that color adjectives tend not to be interpreted contrastively in eye-tracking measures except in contexts that make their use unlikely. In comparison, adjectives describing material (e.g., plastic) and size are interpreted contrastively, which corresponds to less redundant use of material and size adjectives in production [@sedivy_pragmatic_2003-2; see Chapter 10 of @gibson_processing_2011]. This account explains well why color is not interpreted contrastively here, but fails to explain why presumably rare adjectives (bluer, bright) do not receive contrastive treatment in our task. Further work is necessary to determine whether contrastive inferences hew to production norms, and whether implicit indications of contrast usually extend to explicit referent choice.

Description is not limited to conveying contrast between present objects: it can also convey contrast with an object's category. In Experiment 3, we tested whether listeners inferred that a described feature of a novel object was atypical of its category, and how this inference was affected by the distractor objects present. We find that listeners infer atypicality from use of descriptors. However, they do not reserve this inference for cases of over-description alone: listeners inferred atypicality of a described feature even when the descriptor was necessary to establish reference. Listeners, then, seem not to rationally weigh the potential contrasts intended by the listener and trade off between them. Rather, participants' behavior in this task is better described by a coarse heuristic: use of description implies atypicality in relation to the category. Despite not being very sensitive to the referential context in their overt judgments, participants in our third experiment did show facilitation from contrast in processing. Directionally, participants advanced more quickly on trials in which a descriptor was used and was necessary to establish reference than on trials when a supplied descriptor was unnecessary. Overall, our results suggest that the atypicality inference is robust to the point of being difficult to suppress: it is not discounted, even when a descriptor is needed to distinguish between present objects. Participants do trend toward showing effects of the object context in their reaction times, but this processing effect does not consistently extend to overt judgments about the target's category.

Though the participants in our experiments were adults, the ability to disambiguate novel referents using contrast most obviously serves budding language learners: children. Contrastive use of adjectives is a pragmatic regularity in language that children could potentially exploit to establish word--referent mappings. Tasks using a mixture of novel adjectives and words suggest that children as young as 3 can make contrastive inferences about adjectives [@gelman_implicit_1985; @diesendruck_childrens_2006; @huangsnedeker2008]. We plan to research further the development of these contrastive skills, as well as their potential as tools for extracting information from language and context.

# Conclusion

Taken together, these experiments show that people use contrastive inference to map novel words to novel referents and to make inferences about the typicality of novel referents' features. Hearing "small toma" allows people to narrow possible referents not only to small objects, but objects with larger counterparts nearby. Hearing "big toma" in a referential context leads them to think that most tomas are not that size. However, these two abilities do not appear to interact. A referential felicitous use of description does not block an inference of atypicality. These results do not yet provide an explanation of *why* these skills do not interact: the inference may be too complex, the stimuli too novel, or listeners may use contrast more heuristically than rational models of pragmatic inference assume [@frank2012]. Understanding the origins of these independent but non-interpendent inferential abilities, as well as asymmetries between comprehension and production and adjectives like color and size, will be an important next challenge in our development of theories of human pragmatic inference.

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to the last author.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
