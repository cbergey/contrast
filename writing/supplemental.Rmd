---
title: "\\LARGE Using contrastive inferences to learn about new words and categories"
author: "\\large \\emph{Claire Augusta Bergey and Daniel Yurovsky}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: contrast.bib
fontsize: 11pt
geometry: margin=1in
csl: apa6.csl
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(readxl)
library(janitor)
library(here)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(feather)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(effectsize)
library(glue)
library(directlabels)
library(ggthemes)
library(rwebppl)
library(grid)
library(gridExtra)

opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE, fig.width = 3, fig.height = 3)

knitr::opts_chunk$set(fig.pos = '!tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 6,
                      fig.height = 3)


theme_set(theme_few(base_size = 10) + theme(legend.position = "none"))
options(digits=2)
logit <- function(x) {log(x/(1-x))}
```

```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```
\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\section{Experiment 1}

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv"),
                        show_col_types = FALSE) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 

# adj is presence of adjective in utterance (noun vs adjective noun)
# condition is adjective type (size vs color)
# searchtype is the display type (contrast vs unique target)

# full model to report in supplemental, pre-reg'd
# full adj * searchtype random effect model didn't converge. 
# searchtype + 1 also didn't converge. 1+ adj converges.
full_model <- e1_data_no_gather %>%
  glmer(chosetarget ~ adj * condition * searchtype + (1 + adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")  %>%
  mutate(p.value = printp(p.value))

walk2(c("e1full_adj", "e1full_size","e1full_uniquetarget","e1full_adj_size", 
        "e1full_adj_uniquetarget","e1full_size_uniquetarget", 
        "e1full_adj_size_uniquetarget"),
      c("adjTRUE", "conditionsize", "searchtypeuniquetarget", 
        "adjTRUE:conditionsize", "adjTRUE:searchtypeuniquetarget", 
        "conditionsize:searchtypeuniquetarget", 
        "adjTRUE:conditionsize:searchtypeuniquetarget"),  
      ~make_text_vars(full_model, .x, .y))
```

```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)

glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-term) %>%
  rename(term = condition) %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_color_chance", "e1_size_chance"), c("color", "size"), 
      ~ make_text_vars(glmer_chance_comparison, .x, .y))


glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_target_size", "e1_target_adj", "e1_target_size_adj"), 
      c("conditionsize", "adjTRUE", "conditionsize:adjTRUE"), 
      ~ make_text_vars(glmer_unique, .x, .y))
```


## Unique target display trials

In Experiment 1, half of the trials were filler trials in which the target had both a unique shape and unique value of the critical feature. We expected participants to choose the unique object when directed to "Find the toma" or "Find the blue toma" in these trials. These were included as filler trials to keep participants from cluing into the intended inference in contrastive display trials, and they provide a sanity check that people are making sensible selections in the task. 

To analyze these trials, we asked whether participants chose the target more often than expected by chance ($33\%$) by fitting a mixed effects logistic regression with an intercept term, a random effect of subject, and an offset of $logit(1/3)$ to set chance probability to the correct level. The intercept term was reliably different from zero for both color ($\beta =$ `r e1_color_chance_estimate`, $t =$ `r e1_color_chance_statistic`, $p$ `r e1_color_chance_p.value`) and size ($\beta =$ `r e1_size_chance_estimate` , $t =$ `r e1_size_chance_statistic`, $p$ `r e1_size_chance_p.value`), indicating that participants consistently chose the unique object on the screen when given an instruction like "Find the (blue) toma" or "Find the (big) toma," across utterances with and without an adjective. 

To test whether the utterance type (with or without an adjective) and feature type (size or color) affected people's referent choices, we fit a mixed effects logistic regression predicting target selection from feature type, utterance type, and their interaction with random effects of participants. Use of an adjective in the utterance increased target choice ($\beta =$ `r e1_target_adj_estimate`, $t =$ `r e1_target_adj_statistic`, $p$ `r e1_target_adj_p.value`), and feature type (color vs. size) was not statistically related to target choice ($\beta =$ `r e1_target_size_estimate`, $t =$ `r e1_target_size_statistic`, $p =$ `r e1_target_size_p.value`). The two effects had a marginal interaction ($\beta =$ `r e1_target_size_adj_estimate`, $t =$ `r e1_target_size_adj_statistic`, $p =$ `r e1_target_size_adj_p.value`). Participants had a general tendency to choose the target in unique target display trials, which was strengthened if the audio instruction contained the relevant adjective. These effects did not significantly differ between color and size adjectives, which suggests that participants did not treat color and size differently in these baseline trials, though the marginal interaction suggests that use of an adjective may strengthen their tendency to choose the unique object more powerfully in the size condition.

## Additional analyses

In addition to the analyses reported in the main text, we ran a pre-registered linear mixed effects model predicting target choice from the presence of an adjective in the utterance, the adjective type (size or color), and the display type (unique target display or contrastive display) (Table \ref{tab:e1-table}). People were more likely to choose the target if there was an adjective in the utterance ($\beta_{adjective} =$ `r e1full_adj_estimate`, $t =$ `r e1full_adj_statistic`, $p =$ `r e1full_adj_p.value`), and were more overall likely to choose the target on unique target trials ($\beta_{unique} =$ `r e1full_uniquetarget_estimate`, $t =$ `r e1full_uniquetarget_statistic`, $p =$ `r e1full_uniquetarget_p.value`). There was an interaction between the presence of an adjective and the type of adjective, such that people were especially likely to choose the target when there was a size adjective in the utterance ($\beta_{adjective*size} =$ `r e1full_adj_size_estimate`, $t =$ `r e1full_adj_size_statistic`, $p =$ `r e1full_adj_size_p.value`). There was a three-way interaction between the presence of an adjective, the type of adjective, and the search type such that the contrastive strength of size over color was stronger in the contrastive trials than the unique target trials ($\beta_{adjective*size*unique} =$ `r e1full_adj_size_uniquetarget_estimate`, $t =$ `r e1full_adj_size_uniquetarget_statistic`, $p =$ `r e1full_adj_size_uniquetarget_p.value`).

```{r e1-table, results = "asis"}

full_model %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "adjTRUE" ~ "utterance type: adjective (vs. no adjective)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "searchtypeuniquetarget" ~ "display type: unique target (vs. contrastive)",
    term == "adjTRUE:conditionsize" ~ "adjective * size",
    term == "adjTRUE:searchtypeuniquetarget" ~ "adjective * unique target",
    term == "conditionsize:searchtypeuniquetarget" ~ "size * unique target",
    term == "adjTRUE:conditionsize:searchtypeuniquetarget" ~ "adjective * size * unique target"
  )) %>%
  select(-std.error, -effect, -group) %>% 
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(font_size = "footnotesize", caption = "Full model of target choice from Experiment 1. Model specification is chose\\_target $\\sim$ utterance\\_type * adjective\\_type * display\\_type + (1 + utterance\\_type | subject).", escape = FALSE, placement = "h")

```


```{r e1-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:e1-plot}Referent choice in both the contrastive display trials and the unique target display trials. "}

condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )

e1_mean_data %>%
  mutate(empirical_stat = if_else(searchtype == "uniquetarget" & 
                                    item == "lure", as.double(NA), empirical_stat),
         item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(condition ~ searchtype, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("") + 
  geom_dl(method = list(dl.trans(x = x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")
```

Figure \ref{fig:e1-plot} shows referent choice in both the unique target display trials and the contrastive display trials. Unique target displays had one unique referent (the target) and two identical distractors that differed from it both in shape and the critical feature. Contrastive displays had a target, a contrastive pair which matched the target in shape but had a different critical feature, and a lure which matched the target on the critical shape but differed from it on the critical feature.



```{r webppl-continuous, eval = FALSE}
cont_semantics_utterances <- tibble(utterance = c("dax", "blue dax"),
                                sem_values = c("0.8,0.99", "0.99,0.8")) %>%
  tidyr::expand(utterance, sem_values) %>%
  mutate(all_vals = paste(utterance, sem_values, sep = ","),
         label = if_else(sem_values == "0.8,0.99", "low_size_high_color", "high_size_low_color"),
         ids = as.character(1:4))


cont_semantics_inference <- map_dfr(cont_semantics_utterances %>% pull(all_vals), 
                               ~webppl(program_file = 
                                         here("webppl/continuous_semantics.wppl"), 
                                       data = .x),
                               .id = "ids") %>%
  left_join(cont_semantics_utterances, by = "ids") %>%
  as_tibble()  %>%
  select(-all_vals)



write_csv(cont_semantics_inference, here("webppl/model_estimates/continuous_semantics.csv"))
```

```{r load-webppl-continuous}
cont_semantics_inference <- read_csv(here("webppl/model_estimates/continuous_semantics.csv"),
                                 show_col_types = FALSE)
```

\subsection{Modeling Experiment 1 with continuous semantics}

@degen_when_2020 capture asymmetries in description of size and color by positing that different features have different semantic strength. They posit that color has stronger semantics than size, such that "red table" is a better literal description of a small red table than "small table" is. Under these assumptions, RSA using these continuous semantics explains people's tendency to mention color more often than size in a variety of tasks. Can their model explain the asymmetry we find between color and size in Experiment 1? 

In Experiment 1, we found that people more consistently choose the target using contrastive inferences about size than color. We incorporated their continuous semantics into our RSA model of referent choice, which reasons over possible lexicons. In Figure \ref{fig:continuous-sem-plot} we show the difference in referent choice when a feature has low semantic strength (0.8) compared to high semantic strength (0.99). A feature with low semantic strength results in a weaker contrastive inference (reduced choice of the target in the *adjective noun* trials) compared to a feature with high semantic strength. @degen_when_2020 find that color has stronger semantics than size, which would result in a stronger contrastive inference about referent choice when color adjectives are used. This is not what we find: people make stronger contrastive inferences about referent choice when *size* adjectives are used. Thus, while a model with continuous semantics could in principle explain the asymmetry we find, it would need to have stronger semantic values for size than color. We note that while the same continuous semantics do not explain both our data and the production data from @degen_when_2020, neither does the model we propose explain the production data. We leave it to future work to form a more complete account of color-size asymmetries in both production and comprehension.

```{r continuous-sem-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:continuous-sem-plot}Results of modeling target choice in Experiment 1 using continuous semantics. Stronger continuous semantics predict higher choice of the target, while weaker continuous semantics predict lower choice of the target. "}

cont_semantics_inference %>%
  mutate(utterance_cond = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance_cond = factor(utterance_cond, levels = c("noun", "adjective noun")),
         semantic_val = if_else(label == "high_size_low_color", 
                                "Low semantic strength", "High semantic strength"),
         semantic_val = factor(semantic_val, levels = c("Low semantic strength", "High semantic strength")),
         choice = case_when(obj == "blue dax" & world_string == "two dax" ~ "target",
                            obj == "blue dax" & world_string == "two toma" ~ "lure",
                            obj != "blue dax" ~ "other")) %>%
  group_by(utterance_cond, semantic_val, choice) %>%
  summarise(prob = sum(prob)) %>%
  filter(choice != "other") %>%
  ggplot(aes(x = utterance_cond, color = choice, fill = choice)) + 
  geom_crossbar(aes(ymin = prob, ymax = prob, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ semantic_val) +
  labs(x = "Utterance", y = "Proportion each item is chosen") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = choice, y = prob), 
          position = position_dodge(.5),
          method = list(dl.trans(y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")

# Our webppl code is set up to do both color and size with different semantic values, but we're only doing
# inference over "color" values here as a demonstration, so we can relabel to just "low semantic strength" and 
# "high semantic strength".
```

\section{Experiment 2}

```{r e2-read-data}
e2_color_data <- read_csv(here("data/exp2/color.csv"), 
                          show_col_types = FALSE) %>%
  mutate(condition = "color", targetsize = "big") %>%
  rename(adj = colorasked, distractorfeature = distractorcolor)

e2_size_data <- read_csv(here("data/exp2/size.csv"),
                         show_col_types = FALSE) %>%
  mutate(condition = "size") %>%
  rename(adj = sizeasked, distractorfeature = distractorsize)

e2_data <- rbind(e2_color_data, e2_size_data) %>%
  mutate(subid = paste0(subid, condition))

e2_keep_subjs <- e2_data %>%
  filter(searchtype == "attncheck", attncheckscore >= 6) %>%
  group_by(subid) 

e2_model_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  filter(trialtype != 0)

e2_subj_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  rename(adjective = adj) %>%
  mutate(searchtype = if_else(searchtype == "polychrome" | 
                                searchtype == "differentsizes",
                                      "different", searchtype)) %>%
  mutate(searchtype = if_else(searchtype == "monochrome" |
                                searchtype == "samesize",
                                      "same", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # delay before people could select
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(adjective == TRUE, "adjective noun", "noun"),
         adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("different", "contrast", "same")))

e2_mean_data <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e2_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e2_adj", "e2_search_same", "e2_search_contrast", "e2_adj_contrast", "e2_adj_same"), 
      c("adjectiveadjective noun", "searchtypesame", "searchtypecontrast",
        "adjectiveadjective noun:searchtypecontrast", "adjectiveadjective noun:searchtypesame"), 
      ~ make_text_vars(e2_model, .x, .y))
```

\subsection{Experiment 2 Prevalence Judgments}
The full regression of prevalence judgments, also reported in the main text, is in Table \ref{tab:e2-table}.

```{r e2-table}
e2_model %>%
  select(term, estimate, statistic, p.value) %>%
  mutate(estimate = as.numeric(estimate)) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "adjectiveadjective noun" ~ "utterance type: adjective (vs. no adjective)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "searchtypecontrast" ~ "context: within-category contrast display (vs. between-category contrast)",
    term == "searchtypesame" ~ "context: same feature display (vs. between-category contrast)",
    term == "conditionsize:adjectiveadjective noun" ~ "size * adjective",
    term == "conditionsize:searchtypecontrast" ~ "size * within-category contrast display",
    term == "conditionsize:searchtypesame" ~ "size * same feature display",
    term == "adjectiveadjective noun:searchtypecontrast" ~ "adjective * within-category contrast display",
    term == "adjectiveadjective noun:searchtypesame" ~ "adjective * same feature display",
    term == "conditionsize:adjectiveadjective noun:searchtypecontrast" ~ "size * adjective * within-category contrast display",
    term == "conditionsize:adjectiveadjective noun:searchtypesame" ~ "size * adjective * same feature display"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(font_size = "footnotesize", caption = "Full model of prevalence judgments from Experiment 2. Model specification is percentage $\\sim$ adjective\\_type * utterance\\_type * context\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")


```

\section{Experiment 3}

```{r e3-read-data}
e3_data <- read_csv(here("data/exp3_turk_data.csv"), show_col_types = FALSE)

# participants who have unbalanced numbers of trials in each condition
e3_exclude <- e3_data %>% count(subid, utttype, searchtype) %>% filter(n == 2 | n == 3)

e3_keep_subjs <- e3_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, attncheckscore >= 6) %>%
  filter(!(subid %in% e3_exclude$subid)) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e3_model_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = factor(searchtype, levels = c("differentshapes", "contrast")))

e3_subj_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = if_else(searchtype == "differentshapes",
                                      "different", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # time before selections can be made
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(utttype == "adj", "adjective noun", utttype),
         adjective = if_else(adjective == "noutt", "alien utterance", 
                             adjective),
         adjective = if_else(adjective == "noadj", "noun", adjective),
         adjective = factor(adjective, 
                            levels = c("noun", "adjective noun", 
                                       "alien utterance")),
         searchtype = factor(searchtype, levels = c("different", "contrast")))

e3_mean_data <- e3_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e3_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e3_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed")
```

```{r e3-models}
# all prereg'd models below

e3_utt_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(e3_utt_model_no_alien, "e3_adj_no_alien", "utttypeadj")

e3_utt_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e3_utt_model_adj", "e3_utt_model_noadj"), 
      c("utttypeadj", "utttypenoadj"), 
      ~ make_text_vars(e3_utt_model, .x, .y))

# model did not converge with maximal slopes
e3_full_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

walk2(c("e3_full_noadj", "e3_full_adj", "e3_full_size", "e3_full_contrast", "e3_full_adj_size"), 
      c("utttypenoadj", "utttypeadj", "conditionsize", "searchtypecontrast", "utttypeadj:conditionsize"), 
      ~ make_text_vars(e3_full_model, .x, .y))

e3_full_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

make_text_vars(e3_full_model_no_alien, "e3_noalien_adj", "utttypeadj")

```

The full regression predicting Experiment 3 prevalence judgments, also reported in the main text, is shown in Table \ref{tab:e3-full}. The regression predicting Experiment 3 prevalence judgments among only adjective utterances and no adjective utterances (excluding alien utterance trials), also reported in the main text, is shown in Table \ref{tab:e3-full-no-alien}.

In addition to the regressions reported in the manuscript, we report two pre-registered, targeted regressions to test the effect of utterance type to more specifically in case these effects were unclear in the maximal models. First, we filtered to adjective and no adjective trials and fit a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject (Table \ref{tab:e3-adj-no-alien}). Participants' prevalence judgments were significantly lower when an adjective was used in the utterance ($\beta =$ `r e3_adj_no_alien_estimate`, $t =$ `r e3_adj_no_alien_statistic`, $p =$ `r e3_adj_no_alien_p.value`). Second, we included all trials in a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject (Table \ref{tab:e3-adj}). Utterances without an adjective resulted in significantly higher prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_noadj_estimate`, $t =$ `r e3_utt_model_noadj_statistic`, $p =$ `r e3_utt_model_noadj_p.value`), and utterances with an adjective did not result in significantly different prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_adj_estimate`, $t =$ `r e3_utt_model_adj_statistic`, $p =$ `r e3_utt_model_adj_p.value`).

```{r e3-full, results = "asis"}
e3_full_model %>%
  select(-std.error, -df) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "utterance type: adjective (vs. alien utterance)",
    term == "utttypenoadj" ~ "utterance type: no adjective utterance (vs. alien utterance)",
    term == "searchtypecontrast" ~ "context type: within-category contrast (vs. between-category)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "utttypenoadj:searchtypecontrast" ~ "no adjective utterance * within-category contrast display",
    term == "utttypeadj:searchtypecontrast" ~ "adjective utterance * within-category contrast display",
    term == "utttypenoadj:conditionsize" ~ "no adjective utterance * size",
    term == "utttypeadj:conditionsize" ~ "adjective utterance * size",
    term == "searchtypecontrast:conditionsize" ~ "within-category contrast display",
    term == "utttypenoadj:searchtypecontrast:conditionsize" ~ "no adjective utterance * within-category contrast display * size"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type, context type, and adjective type in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type * context\\_type * adjective\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")

```

```{r e3-full-no-alien, results = "asis"}
e3_full_model_no_alien %>%
  select(-std.error, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "utterance type: adjective (vs. no adjective)",
    term == "searchtypecontrast" ~ "context tyep: within-category contrast (vs. between-category)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "utttypeadj:searchtypecontrast" ~ "adjective utterance * within-category contrast display",
    term == "utttypeadj:conditionsize" ~ "adjective utterance * size",
    term == "searchtypecontrast:conditionsize" ~ "within-category contrast display",
    term == "utttypeadj:searchtypecontrast:conditionsize" ~ "no adjective utterance * within-category contrast display * size"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type, context type, and adjective type only among adjective and no adjective utterances (excluding alien utterances) in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type * context\\_type * adjective\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")

```

```{r e3-adj-no-alien, results = "asis"}
e3_utt_model_no_alien %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "adjective utterance (vs. no adjective utterance)"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from presence of an adjective in the utterance (excluding alien language utterances) in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type + (utterance\\_type | subject). ", escape = FALSE, placement = "h")


```

```{r e3-adj, results = "asis"}
e3_utt_model %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "adjective utterance (vs. alien utterance)",
    term == "utttypenoadj" ~ "no adjective utterance (vs. alien utterance)"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type + (utterance\\_type | subject). ", escape = FALSE, placement = "h")


```

```{r combined-analysis}
e2_to_combine <- e2_subj_data %>%
  mutate(experiment = 2, 
         subid = paste0(subid, "2")) %>%
  filter(searchtype != "same") %>%
  select(subid, experiment, counter, condition, adjective, searchtype, percentage)

e3_to_combine <- e3_subj_data %>%
  mutate(experiment = 3,
         subid = paste0(subid, "3")) %>%
  filter(adjective != "alien utterance") %>%
  select(subid, experiment, counter, condition, adjective, searchtype, percentage)

combined_data <- rbind(e2_to_combine, e3_to_combine)

n_subjs_2 <- n_distinct(e2_to_combine$subid)
n_subjs_3 <- n_distinct(e3_to_combine$subid)
n_subjs_combined <- n_distinct(combined_data$subid)

combined_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_adj", "combined_size", "combined_search_contrast", "combined_adj_contrast"), 
      c("adjectiveadjective noun", "conditionsize", "searchtypecontrast", 
        "adjectiveadjective noun:searchtypecontrast"), 
      ~ make_text_vars(combined_model, .x, .y))

combined_mean_data <- combined_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

```

```{r combined-separate-adj}
# just an exploratory check to see if these context differences only show up in adj trials or non-adj trials,
# considered separately.
# there's a small effect among only adj trials and a marginal effect among only non-adj trials.
# it doesn't seem likely that the overall context difference is due to, e.g., 
# a *typicality* inference when a necessary adjective was omitted. 
combined_model_all_adj <- lmer(percentage ~ condition * searchtype +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data %>% filter(adjective == "adjective noun")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_all_adj_search_contrast"), 
      c("searchtypecontrast"), 
      ~ make_text_vars(combined_model_all_adj, .x, .y))

combined_model_no_adj <- lmer(percentage ~ condition * searchtype +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data %>% filter(adjective == "noun")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_no_adj_search_contrast"), 
      c("searchtypecontrast"), 
      ~ make_text_vars(combined_model_no_adj, .x, .y))

```

\section{Combined Analyses of Experiment 2 and 3}

Given that many participants failed the memory check in Experiments 2 and 3 (which was our pre-registered exclusion criterion), we yielded data from fewer participants than expected in our pre-registrations. This means that the analyses of Experiments 2 and 3 may be underpowered to detect subtle effects of context or feature type. Since Experiments 2 and 3 had very similar procedures, we can conduct an exploratory combined analysis comparing the conditions they shared. 

Combining data from Experiment 2 (N = `r n_subjs_2`) and Experiment 3 (N = `r n_subjs_3`) yields a total sample of `r n_subjs_combined` participants in this combined analysis. Only trials from the conditions that Experiments 2 and 3 share are included in this analysis: two types of referential context (within-category context and between-category context), two types of feature (size and color), and two types of utterance (adjective noun and noun only).

We fit a mixed effects linear regression on the combined data with the same specification as the full models we used for Experiments 2 and 3 in the main text: effects of utterance type, context type, and critical feature and their interactions, and a random slope of utterance type by subject. This model revealed a significant effect of utterance type ($\beta_{adjective} =$ `r combined_adj_estimate`, $t =$ `r combined_adj_statistic`, $p$ `r combined_adj_p.value`): people inferred that a feature was rarer when it was mentioned, consistent with our findings in each experiment separately. Participants' inferences did not significantly differ between color and size adjective conditions ($\beta_{size} =$ `r combined_size_estimate`, $t =$ `r combined_size_statistic`, $p =$ `r combined_size_p.value`). This is also consistent with findings from each experiment. There was a significant effect of context, with people making overall slightly higher prevalence judgments in the within-category context ($\beta_{within} =$ `r combined_search_contrast_estimate`, $t =$ `r combined_search_contrast_statistic`, $p =$ `r combined_search_contrast_p.value`), which was not found in either experiment separately. However, there was not a significant interaction between context and utterance type ($\beta_{within*adjective} =$ `r combined_adj_contrast_estimate`, $t =$ `r combined_adj_contrast_statistic`, $p =$ `r combined_adj_contrast_p.value`), consistent with findings from each experiment. That is, people did not modulate the size of their typicality inferences (the difference between the adjective noun and noun utterance types) based on context type, though there was a baseline difference in prevalence judgments between context types. 

Overall, these results are consistent with what we found in Experiments 2 and 3, with the exception of finding an overall difference in prevalence judgments between context types. However, we did not find an interaction between context type and utterance type, which would demonstrate that participants are trading off between referential utility and typicality when making these inferences. This exploratory combined analysis suggests further research is necessary, as even when combined our data are not definitive about whether potential trade-offs are small or nonexistent.

```{r combined-plot, fig.cap = "Participants' prevalence judgments in combined data from Experiments 2 and 3."}

labels <- rasterGrob(png::readPNG(here("writing/figs/e3_plot_labels.png")), interpolate = TRUE)

plot <- ggplot(combined_mean_data %>% mutate(searchtype = factor(searchtype, levels = c("contrast", "different"))), 
               aes(x = adjective, y = empirical_stat, 
                         color = condition, fill = condition)) + 
  facet_wrap(~ searchtype) +
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  labs(color = "Adjective type") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(.5)) + 
  geom_hline(aes(yintercept = 50), linetype = "dashed") +
  scale_color_ptol() +
  theme(strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = "none") 

grid.arrange(labels, plot, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))
```

\{Inferring Atypicality with Non-Fruit Novel Stimuli}

The use of alien fruit stimuli in our experiments raises the question of whether people would infer that *non-fruits* are atypical when their color is remarked upon. As noted in the introduction, people remark on the colors of some kinds of objects more than others, perhaps because they more often vary in color [@rubio-fernandez_how_2016]. Because fruits tend to have stereotypical colors, it is possible that people would not expect the colors of fruits to be remarked upon very often, and thus have a stronger inference of atypicality for fruit than other types of objects. Here, we provide some additional data showing that people still make these inferences for categories about which they likely do not have such an expectation: block shapes.

### Participants.
346 participants were recruited on Amazon Mechanical Turk to perform this task. They were paid 10 cents to complete the task. Participants on average took 20 seconds to complete the single trial (not including reading the consent form).

Two participants were excluded because they repeated the task, leaving 344 participants for analysis. There were no other exclusion criteria.

### Procedure
Before the main task began, participants were shown an array of colorful block shapes and told they were going to learn about shapes like these, and were given three examples of novel names these kinds of shapes might have. The object shapes, colors, and names in this introduction phase were randomly chosen and different from the ones used in the task.

In this one-trial task, participants saw a stimulus display with six identical block shapes (distractors) surrounding one unique block shape (the target) (see label in Figure \ref{fig:tetris-plot}). They were asked to "Find the [blue] toma," with an utterance that either included a color adjective or did not. We expected that participants would click on the unique, central object, and they only moved forward in the task once they selected that object. 

Two conditions of interest, utterance type (with or without an adjective) and display type (whether the distractors' color was the same as or different from the target) were varied between subjects. Additionally, several factors were randomly assigned between subjects: the name of the target (among *dax, blicket, wug, toma, gade,* or *sprock*), the target color (among red, blue, green, orange, purple, yellow, pink, tan, teal, or grey), the distractor color if different from the target (among the same set of colors), and the target and distractor shapes (among ten possible block shapes with five square components).

After selecting the target object, participants made a judgment about the prevalence of the target's color among the target's category. They were asked, e.g., "What percentage of tomas do you think are blue?" and responded on a slider scale between 0 and 100%.

```{r tetris}
tetris_data <- read_csv(here("data/supplemental/tetris_data_one_shot.csv"))

tetris_subj_data <- tetris_data %>%
  mutate(utterance = if_else(colorasked == FALSE, "toma", "blue toma"),
         context = if_else(diffcolor == TRUE, "different color", "same color"),
         counter = as.numeric(counter)) %>%
  mutate(utterance = factor(utterance, levels = c("toma", "blue toma")),
         context = factor(context, levels = c("different color", "same color")))

repeated_subjs <- tetris_subj_data %>%
  count(subid) %>%
  filter(n > 1)

tetris_subj_data <- tetris_subj_data %>%
  filter(!(subid %in% repeated_subjs$subid))

tetris_model <- lm(percentage ~ utterance * context,
              data = tetris_subj_data) %>%
  tidy() %>%
  mutate(p.value = printp(p.value))

walk2(c("tetris_adj", "tetris_samecolor", "tetris_adj_samecolor"),
      c("utteranceblue toma", "contextsame color", "utteranceblue toma:contextsame color"), 
      ~ make_text_vars(tetris_model, .x, .y))

tetris_mean_data <- tetris_subj_data %>%
  group_by(context, utterance) %>%
  tidyboot_mean(percentage)

tetris_avg_time <- tetris_subj_data %>%
  mutate(trial_time = as.numeric(rttrain) + as.numeric(rttest)) %>% 
  group_by(subid) %>%
  summarise(total_time = sum(trial_time)/1000) %>% # convert milliseconds to seconds
  ungroup() %>%
  summarise(mean_time = mean(total_time))
```

### Results
The main question of interest is whether people infer that a color is rarer when it is mentioned than when it is not; when a participant hears "blue toma," do they infer that tomas are less likely to be blue? We fit a linear model predicting participants' prevalence judgments from the utterance type (noun or adjective noun) and context type (different-color or same-color distractors) and their interaction. There was a significant effect of utterance type, such that people's prevalence judgments were lower when there was an adjective in the utterance than when there was not ($\beta_{adjective} =$ `r tetris_adj_estimate`, $t =$ `r tetris_adj_statistic`, $p =$ `r tetris_adj_p.value`). There was not a significant effect of context type ($\beta_{same-color-context} =$ `r tetris_samecolor_estimate`, $t =$ `r tetris_samecolor_statistic`, $p =$ `r tetris_samecolor_p.value`) or an utterance by context interaction ($\beta_{adjective-context} =$ `r tetris_adj_samecolor_estimate`, $t =$ `r tetris_adj_samecolor_statistic`, $p =$ `r tetris_adj_samecolor_p.value`). 

## Discussion
We found that when a block object is referred to as "the blue toma" rather than "the toma," people think tomas are less likely to be blue in general. People's judgments also did not significantly differ depending on whether surrounding objects were the same color as the target. This stripped-down demonstration of the effect provides additional evidence that people infer described objects are atypical even when they likely do not have strong expectations that the object categories have a stereotypical color (as they may with fruit). It is an open and interesting question whether people's prior expectations about a category's feature distribution would modulate or extinguish this effect. This demonstration is evidence that the effect generalizes beyond fruit; we leave a systematic investigation of that question to future work.

```{r tetris-plot}
tetris_labels <- rasterGrob(png::readPNG(here("writing/figs/tetris_plot_labels.png")), interpolate = TRUE)


plot <- ggplot(tetris_mean_data, aes(x = utterance, color = context, group = context)) +
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  facet_wrap(~context) +
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  scale_color_ptol() +
  geom_dl(aes(label = context, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 1, y = y - .7), "first.points", cex=.7)) +
theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = "none")


grid.arrange(tetris_labels, plot, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))
```


\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\noindent
<div id = "refs"></div>
\endgroup