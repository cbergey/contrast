---
title: "\\LARGE Using contrastive inferences to learn about new words and categories"
author: "\\large \\emph{Claire Bergey and Daniel Yurovsky}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: contrast.bib
fontsize: 11pt
geometry: margin=1in
csl: apa6.csl
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(readxl)
library(janitor)
library(here)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(feather)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(effectsize)
library(glue)
library(directlabels)
library(ggthemes)
library(rwebppl)

opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE, fig.width = 3, fig.height = 3)

theme_set(theme_classic(base_size = 12))
options(digits=2)
```

```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```
\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\section{Experiment 1}

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv"),
                        show_col_types = FALSE) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 


# full model to report in supplemental, pre-reg'd
# full adj * searchtype random effect model didn't converge. 
# searchtype + 1 also didn't converge. 1+ adj converges.
full_model <- e1_data_no_gather %>%
  glmer(chosetarget ~ adj * condition * searchtype + (1 + adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")  %>%
  mutate(p.value = printp(p.value))

walk2(c("e1full_adj", "e1full_size","e1full_uniquetarget","e1full_adj_size", 
        "e1full_adj_uniquetarget","e1full_size_uniquetarget", 
        "e1full_adj_size_uniquetarget"),
      c("adjTRUE", "conditionsize", "searchtypeuniquetarget", 
        "adjTRUE:conditionsize", "adjTRUE:searchtypeuniquetarget", 
        "conditionsize:searchtypeuniquetarget", 
        "adjTRUE:conditionsize:searchtypeuniquetarget"),  
      ~make_text_vars(full_model, .x, .y))
```

In addition to the analyses reported in the main text, we ran a pre-registered linear mixed effects model predicting target choice from the presence of an adjective in the utterance, the adjective type (size or color), and the display type (unique target display or contrastive display). People were more likely to choose the target if there was an adjective in the utterance ($\beta_{adjective} =$ `r e1full_adj_estimate`, $t =$ `r e1full_adj_statistic`, $p =$ `r e1full_adj_p.value`), and were more overall likely to choose the target on unique target trials ($\beta_{unique} =$ `r e1full_uniquetarget_estimate`, $t =$ `r e1full_uniquetarget_statistic`, $p =$ `r e1full_uniquetarget_p.value`). 

** Add more text here with effects of adj*size and three-way interaction

```{r e1-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:e1-plot}Referent choice in both the contrastive display trials and the unique target display trials. "}

condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )

e1_mean_data %>%
  mutate(empirical_stat = if_else(searchtype == "uniquetarget" & 
                                    item == "lure", as.double(NA), empirical_stat),
         item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(condition ~ searchtype, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("") + 
  geom_dl(method = list(dl.trans(x = x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")
```

Figure \ref{fig:e1-plot} shows referent choice in both the unique target display trials and the contrastive display trials. Unique target displays had one unique referent (the target) and two identical distractors that differed from it both in shape and the critical feature. Contrastive displays had a target, a contrastive pair which matched the target in shape but had a different critical feature, and a lure which matched the target on the critical shape but differed from it on the critical feature.

\section{Experiment 2}

```{r webppl-continuous, eval = FALSE}
cont_semantics_utterances <- tibble(utterance = c("dax", "blue dax"),
                                sem_values = c("0.8,0.99", "0.99,0.8")) %>%
  tidyr::expand(utterance, sem_values) %>%
  mutate(all_vals = paste(utterance, sem_values, sep = ","),
         label = if_else(sem_values == "0.8,0.99", "low_size_high_color", "high_size_low_color"),
         ids = as.character(1:4))


cont_semantics_inference <- map_dfr(cont_semantics_utterances %>% pull(all_vals), 
                               ~webppl(program_file = 
                                         here("webppl/continuous_semantics.wppl"), 
                                       data = .x),
                               .id = "ids") %>%
  left_join(cont_semantics_utterances, by = "ids") %>%
  as_tibble()  %>%
  select(-all_vals)



write_csv(cont_semantics_inference, here("webppl/model_estimates/continuous_semantics.csv"))
```

```{r load-webppl-continuous}
cont_semantics_inference <- read_csv(here("webppl/model_estimates/continuous_semantics.csv"),
                                 show_col_types = FALSE)
```

```{r continuous-sem-plot, fig.width = 5, fig.height = 3, fig.cap = "\\label{fig:continuous-sem-plot}"}
# Our webppl code is set up to do both color and size with different semantic values, but we're only doing
# inference over "color" values here as a demonstration, so we can relabel to just "low semantic strength" and 
# "high semantic strength".
cont_semantics_inference %>%
  mutate(utterance_cond = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance_cond = factor(utterance_cond, levels = c("noun", "adjective noun")),
         semantic_val = if_else(label == "high_size_low_color", 
                                "Low semantic strength", "High semantic strength"),
         semantic_val = factor(semantic_val, levels = c("Low semantic strength", "High semantic strength")),
         choice = case_when(obj == "blue dax" & world_string == "two dax" ~ "target",
                            obj == "blue dax" & world_string == "two toma" ~ "lure",
                            obj != "blue dax" ~ "other")) %>%
  group_by(utterance_cond, semantic_val, choice) %>%
  summarise(prob = sum(prob)) %>%
  filter(choice != "other") %>%
  ggplot(aes(x = utterance_cond, color = choice, fill = choice)) + 
  geom_crossbar(aes(ymin = prob, ymax = prob, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ semantic_val) +
  labs(x = "Utterance", y = "Proportion each item is chosen") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = choice, y = prob), 
          position = position_dodge(.5),
          method = list(dl.trans(y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")
```

@degen_when_2020 capture asymmetries in description of size and color by positing that different features have different semantic strength. They posit that color has stronger semantics than size, such that "red table" is a better literal description of a small red table than "small table" is. Under these assumptions, RSA using these continuous semantics explains people's tendency to mention color more often than size in a variety of tasks. Can their model explain the asymmetry we find between color and size in Experiment 1? 

In Experiment 1, we found that people more consistently choose the target using contrastive inferences about size than color. We incorporated their continuous semantics into our RSA model of referent choice, which reasons over possible lexicons. In Figure \ref{fig:continuous-sem-plot} we show the difference in referent choice when a feature has low semantic strength (0.8) compared to high semantic strength (0.99). A feature with low semantic strength results in a weaker contrastive inference (reduced choice of the target in the *adjective noun* trials) compared to a feature with high semantic strength. @degen_when_2020 find that color has stronger semantics than size, which would result in a stronger contrastive inference about referent choice when color adjectives are used. This is not what we find: people make stronger contrastive inferences about referent choice when *size* adjectives are used. Thus, while a model with continuous semantics could in principle explain the asymmetry we find, it would need to have stronger semantic values for size than color. We note that while the same continuous semantics do not explain both our data and the production data from @degen_when_2020, neither does the model we propose explain the production data. We leave it to future work to form a more complete account of color-size asymmetries in both production and comprehension.

\subsection{Experiment 2 Reaction Times}

```{r e2-read-data}
e2_color_data <- read_csv(here("data/exp2/color.csv"), 
                          show_col_types = FALSE) %>%
  mutate(condition = "color", targetsize = "big") %>%
  rename(adj = colorasked, distractorfeature = distractorcolor)

e2_size_data <- read_csv(here("data/exp2/size.csv"),
                         show_col_types = FALSE) %>%
  mutate(condition = "size") %>%
  rename(adj = sizeasked, distractorfeature = distractorsize)

e2_data <- rbind(e2_color_data, e2_size_data) %>%
  mutate(subid = paste0(subid, condition))

e2_keep_subjs <- e2_data %>%
  filter(searchtype == "attncheck", attncheckscore >= 6) %>%
  group_by(subid) 

e2_model_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  filter(trialtype != 0)

e2_subj_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  rename(adjective = adj) %>%
  mutate(searchtype = if_else(searchtype == "polychrome" | 
                                searchtype == "differentsizes",
                                      "different", searchtype)) %>%
  mutate(searchtype = if_else(searchtype == "monochrome" |
                                searchtype == "samesize",
                                      "same", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(adjective == TRUE, "adjective noun", "noun"),
         adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("different", "contrast", "same")))

e2_mean_data <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e2_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e2_adj", "e2_search_same", "e2_search_contrast", "e2_adj_contrast", "e2_adj_same"), 
      c("adjectiveadjective noun", "searchtypesame", "searchtypecontrast",
        "adjectiveadjective noun:searchtypecontrast", "adjectiveadjective noun:searchtypesame"), 
      ~ make_text_vars(e2_model, .x, .y))
```

```{r rt-model}
rtmodel <- lmer(log_rt ~ adjective * searchtype + condition +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

e2_rt_means <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(log_rt)
```

Thus, participants treated all adjectives as marked, and inferred lower typicality, regardless of whether they could felicitiously be interpreted as contrasting between potential target referents. But were participants nonetheless sensitive to this information in their response times? We investigated this question by analyzing participants' time to advance after seeing the aliens' referential exchange. Though this task was not speeded, we hypothesized that participants would advance more quickly after seeing referential exchanges that were easier to process. After dropping all response times less than 1 second and longer than 10 seconds, and log transforming them because of the right skew in response time data, we predicted participants' time to advance on each trial of the experiment from utterance type, context type, critical adjective type, and the interaction between utterance type and context type (\texttt{log(rt) $\sim$ adjective * search + type + (1 |subj)}). This model showed a reliable effect of utterance type ($\beta_{adjective} =$ , $t =$ , $p$ )--participants were faster when an a descriptor was provided despite having to process an additional word. There was no main effect of critical adjective type ($\beta_{size} =$ , $t =$ , $p =$ ), nor context type ($\beta_{contrast} =$ , $t =$ , $p =$ ; $\beta_{same} =$ , $t =$ , $p =$ ), but the interactions between utterance type and context type trended towards significance for both non-contrast searches ($\beta_{adjective*contrast} =$ , $t =$ , $p =$ ; $\beta_{adjective*same} =$ , $t =$ , $p =$ ). Directionally, these results indicate that participants took longer to process utterances which were under-described (within-category contrast trials with no adjective) than those with appropriately no description, and processed trials with an appropriate level of description (contrast trials with an adjective) more quickly than those with superfluous description.

```{r rt-plot-inference, fig.width = 6, fig.height = 4.5}
fig.cap="The log reaction time participants took to advance after seeing the referential exchange, by condition."
ggplot(e2_mean_data, aes(x = adjective, color = condition)) +
  facet_wrap(~searchtype) +
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5)) +
  ylab("log reaction time") +
  xlab("Utterance type") +
  scale_color_ptol()
```

\section{Experiment 3}

```{r e3-read-data}
e3_data <- read_csv(here("data/exp3_turk_data.csv"), show_col_types = FALSE)

# participants who have unbalanced numbers of trials in each condition
e3_exclude <- e3_data %>% count(subid, utttype, searchtype) %>% filter(n == 2 | n == 3)

e3_keep_subjs <- e3_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, attncheckscore >= 6) %>%
  filter(!(subid %in% e3_exclude$subid)) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e3_model_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = factor(searchtype, levels = c("differentshapes", "contrast")))

e3_subj_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = if_else(searchtype == "differentshapes",
                                      "different", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # time before selections can be made
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(utttype == "adj", "adjective noun", utttype),
         adjective = if_else(adjective == "noutt", "alien utterance", 
                             adjective),
         adjective = if_else(adjective == "noadj", "noun", adjective),
         adjective = factor(adjective, 
                            levels = c("noun", "adjective noun", 
                                       "alien utterance")),
         searchtype = factor(searchtype, levels = c("different", "contrast")))

e3_mean_data <- e3_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e3_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e3_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed")
```

```{r e3-models}
# all prereg'd models below

e3_utt_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(e3_utt_model_no_alien, "e3_adj_no_alien", "utttypeadj")

e3_utt_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e3_utt_model_adj", "e3_utt_model_noadj"), 
      c("utttypeadj", "utttypenoadj"), 
      ~ make_text_vars(e3_utt_model, .x, .y))

# model did not converge with maximal slopes
e3_full_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

walk2(c("e3_full_noadj", "e3_full_adj", "e3_full_size", "e3_full_contrast", "e3_full_adj_size"), 
      c("utttypenoadj", "utttypeadj", "conditionsize", "searchtypecontrast", "utttypeadj:conditionsize"), 
      ~ make_text_vars(e3_full_model, .x, .y))

e3_full_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

make_text_vars(e3_full_model_no_alien, "e3_noalien_adj", "utttypeadj")

```

In addition to the regressions reported in the manuscript, we two pre-registered, targeted regressions to test the effect of utterance type to more specifically in case these effects were unclear in the maximal models. First, we filtered to adjective and no adjective trials and fit a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject. Participants' prevalence judgments were significantly lower when an adjective was used in the utterance ($\beta =$ `r e3_adj_no_alien_estimate`, $t =$ `r e3_adj_no_alien_statistic`, $p =$ `r e3_adj_no_alien_p.value`). Second, we included all trials in a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject. Utterances without an adjective resulted in significantly higher prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_noadj_estimate`, $t =$ `r e3_utt_model_noadj_statistic`, $p =$ `r e3_utt_model_noadj_p.value`), and utterances with an adjective did not result in significantly different prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_adj_estimate`, $t =$ `r e3_utt_model_adj_statistic`, $p =$ `r e3_utt_model_adj_p.value`).

\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\noindent
<div id = "refs"></div>
\endgroup