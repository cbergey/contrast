---
title: "\\LARGE Supplemental Materials: Using contrastive inferences to learn about new words and categories"
author: "\\large \\emph{Claire Augusta Bergey and Daniel Yurovsky}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: contrast.bib
fontsize: 11pt
geometry: margin=1in
csl: apa6.csl
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(readxl)
library(janitor)
library(here)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(feather)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(effectsize)
library(glue)
library(directlabels)
library(ggthemes)
library(rwebppl)
library(grid)
library(gridExtra)

opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE, fig.width = 3, fig.height = 3)

knitr::opts_chunk$set(fig.pos = '!tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 6,
                      fig.height = 5)


theme_set(theme_few(base_size = 10) + theme(legend.position = "none"))
options(digits=2)
logit <- function(x) {log(x/(1-x))}
```

```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```
\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\section{Experiment 1}

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv"),
                        show_col_types = FALSE) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 

# adj is presence of adjective in utterance (noun vs adjective noun)
# condition is adjective type (size vs color)
# searchtype is the display type (contrast vs unique target)

# full model to report in supplemental, pre-reg'd
# full adj * searchtype random effect model didn't converge. 
# searchtype + 1 also didn't converge. 1+ adj converges.
full_model <- e1_data_no_gather %>%
  glmer(chosetarget ~ adj * condition * searchtype + (1 + adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")  %>%
  mutate(p.value = printp(p.value))

walk2(c("e1full_adj", "e1full_size","e1full_uniquetarget","e1full_adj_size", 
        "e1full_adj_uniquetarget","e1full_size_uniquetarget", 
        "e1full_adj_size_uniquetarget"),
      c("adjTRUE", "conditionsize", "searchtypeuniquetarget", 
        "adjTRUE:conditionsize", "adjTRUE:searchtypeuniquetarget", 
        "conditionsize:searchtypeuniquetarget", 
        "adjTRUE:conditionsize:searchtypeuniquetarget"),  
      ~make_text_vars(full_model, .x, .y))
```

```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)

glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-term) %>%
  rename(term = condition) %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_color_chance", "e1_size_chance"), c("color", "size"), 
      ~ make_text_vars(glmer_chance_comparison, .x, .y))


glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_target_size", "e1_target_adj", "e1_target_size_adj"), 
      c("conditionsize", "adjTRUE", "conditionsize:adjTRUE"), 
      ~ make_text_vars(glmer_unique, .x, .y))
```

## Unique target display trials

In Experiment 1, half of the trials were filler trials in which the target had both a unique shape and unique value of the critical feature. We expected participants to mostly choose the unique object when directed to "Find the toma" in these trials, and to even more strongly prefer the target when directed to "Find the blue toma" (as it is the only object with the correct feature). These were included as filler trials to keep participants from cluing into the intended inference in contrastive display trials, and they provide a sanity check that people are making sensible selections in the task. 

To analyze these trials, we asked whether participants chose the target more often than expected by chance ($33\%$) by fitting a mixed effects logistic regression with an intercept term, a random effect of subject, and an offset of $logit(1/3)$ to set chance probability to the correct level. The intercept term was reliably different from zero for both color ($\beta =$ `r e1_color_chance_estimate`, $t =$ `r e1_color_chance_statistic`, $p$ `r e1_color_chance_p.value`) and size ($\beta =$ `r e1_size_chance_estimate` , $t =$ `r e1_size_chance_statistic`, $p$ `r e1_size_chance_p.value`), indicating that participants consistently chose the unique object on the screen when given an instruction like "Find the (blue) toma" or "Find the (big) toma," across utterances with and without an adjective. 

To test whether the utterance type (with or without an adjective) and feature type (size or color) affected people's referent choices, we fit a mixed effects logistic regression predicting target selection from feature type, utterance type, and their interaction with random effects of participants. Use of an adjective in the utterance increased target choice ($\beta =$ `r e1_target_adj_estimate`, $t =$ `r e1_target_adj_statistic`, $p$ `r e1_target_adj_p.value`), and feature type (color vs. size) was not statistically related to target choice ($\beta =$ `r e1_target_size_estimate`, $t =$ `r e1_target_size_statistic`, $p =$ `r e1_target_size_p.value`). The two effects had a marginal interaction ($\beta =$ `r e1_target_size_adj_estimate`, $t =$ `r e1_target_size_adj_statistic`, $p =$ `r e1_target_size_adj_p.value`). Participants had a general tendency to choose the target in unique target display trials, which was strengthened if the audio instruction contained the relevant adjective. These effects did not significantly differ between color and size adjectives, which suggests that participants did not treat color and size differently in these baseline trials, though the marginal interaction suggests that use of an adjective may strengthen their tendency to choose the unique object more powerfully in the size condition.

```{r uniquetrial, fig.env = "figure", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "On the left: an example of a unique target display trial in which the critical feature is size. Here, the participant would hear the instruction, e.g., ``Find the toma'' or ``Find the big toma.'' On the right: an example of a unique target display trial in which the critical feature is color. Here, the participant would hear the instruction ``Find the toma'' or ``Find the red toma.'' In each case, the target has both a unique shape and critical feature (size or color). Target labels are provided for clarity and were not shown to participants.", cache = FALSE}
img <- png::readPNG(here("writing/figs/unique_target.png"))
grid::grid.raster(img)
```

## Additional analyses

In addition to the analyses reported in the main text, we ran a pre-registered linear mixed effects model predicting target choice from the presence of an adjective in the utterance, the adjective type (size or color), and the display type (unique target display or contrastive display) (Table \ref{tab:e1-table}). People were more likely to choose the target if there was an adjective in the utterance ($\beta_{adjective} =$ `r e1full_adj_estimate`, $t =$ `r e1full_adj_statistic`, $p =$ `r e1full_adj_p.value`), and were more overall likely to choose the target on unique target trials ($\beta_{unique} =$ `r e1full_uniquetarget_estimate`, $t =$ `r e1full_uniquetarget_statistic`, $p =$ `r e1full_uniquetarget_p.value`). There was an interaction between the presence of an adjective and the type of adjective, such that people were especially likely to choose the target when there was a size adjective in the utterance ($\beta_{adjective*size} =$ `r e1full_adj_size_estimate`, $t =$ `r e1full_adj_size_statistic`, $p =$ `r e1full_adj_size_p.value`). There was a three-way interaction between the presence of an adjective, the type of adjective, and the search type such that the contrastive strength of size over color was stronger in the contrastive trials than the unique target trials ($\beta_{adjective*size*unique} =$ `r e1full_adj_size_uniquetarget_estimate`, $t =$ `r e1full_adj_size_uniquetarget_statistic`, $p =$ `r e1full_adj_size_uniquetarget_p.value`).

```{r e1-table, results = "asis"}

full_model %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "adjTRUE" ~ "utterance type: adjective (vs. no adjective)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "searchtypeuniquetarget" ~ "display type: unique target (vs. contrastive)",
    term == "adjTRUE:conditionsize" ~ "adjective * size",
    term == "adjTRUE:searchtypeuniquetarget" ~ "adjective * unique target",
    term == "conditionsize:searchtypeuniquetarget" ~ "size * unique target",
    term == "adjTRUE:conditionsize:searchtypeuniquetarget" ~ "adjective * size * unique target"
  )) %>%
  select(-std.error, -effect, -group) %>% 
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(font_size = "footnotesize", caption = "Full model of target choice from Experiment 1. Model specification is chose\\_target $\\sim$ utterance\\_type * adjective\\_type * display\\_type + (1 + utterance\\_type | subject).", escape = FALSE, placement = "h")

```


```{r e1-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:e1-plot}Referent choice in both the contrastive display trials and the unique target display trials. "}

condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )

e1_mean_data %>%
  mutate(empirical_stat = if_else(searchtype == "uniquetarget" & 
                                    item == "lure", as.double(NA), empirical_stat),
         item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(condition ~ searchtype, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("") + 
  geom_dl(method = list(dl.trans(x = x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")
```

Figure \ref{fig:e1-plot} shows referent choice in both the unique target display trials and the contrastive display trials. Unique target displays had one unique referent (the target) and two identical distractors that differed from it both in shape and the critical feature. Contrastive displays had a target, a contrastive pair which matched the target in shape but had a different critical feature, and a lure which matched the target on the critical shape but differed from it on the critical feature.



```{r webppl-continuous-toy-model, eval = FALSE}
cont_semantics_utterances <- tibble(utterance = c("dax", "blue dax"),
                                sem_values = c("0.8,0.99", "0.99,0.8")) %>%
  tidyr::expand(utterance, sem_values) %>%
  mutate(all_vals = paste(utterance, sem_values, sep = ","),
         label = if_else(sem_values == "0.8,0.99", "low_size_high_color", "high_size_low_color"),
         ids = as.character(1:4))


cont_semantics_inference <- map_dfr(cont_semantics_utterances %>% pull(all_vals), 
                               ~webppl(program_file = 
                                         here("webppl/continuous_semantics.wppl"), 
                                       data = .x),
                               .id = "ids") %>%
  left_join(cont_semantics_utterances, by = "ids") %>%
  as_tibble()  %>%
  select(-all_vals)

#write_csv(cont_semantics_inference, here("webppl/model_estimates/continuous_semantics.csv"))
```

```{r e1-estimation-data, eval = FALSE}
e1_estimation_data <- e1_data_no_gather %>%
  filter(searchtype == "contrast") %>%
  mutate(choseother = !chosetarget & !choselure) %>%
  select(condition, adj, chosetarget, choselure, choseother) %>%
  mutate(utt = if_else(adj, "blue dax", "dax")) %>%
  mutate(world = case_when(chosetarget ~ "two dax",
                         choselure ~ "two toma",
                         choseother ~ "two dax",
                         TRUE ~ NA_character_),
         obj = case_when(choseother ~ "red dax",
                         chosetarget | choselure ~ "blue dax",
                         TRUE ~ NA_character_)) %>%
  filter(!is.na(world), !is.na(obj))

size_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "size")

color_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "color")

size_estimation_data_adj <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj),
         adj == TRUE) %>%
  filter(condition == "size")

color_estimation_data_adj <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj),
         adj == TRUE) %>%
  filter(condition == "color")
```

```{r e1-params-continuous, eval = FALSE}
size_parameter <- webppl(program_file =
                           here("webppl/continuous_semantics.wppl"), 
                          data = size_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

color_parameter <- webppl(program_file =
                           here("webppl/continuous_semantics.wppl"), 
                          data = color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e1_continuous_parameters <- size_parameter %>%
  bind_rows(color_parameter)

size_parameter_adj <- webppl(program_file =
                           here("webppl/continuous_semantics.wppl"), 
                          data = size_estimation_data_adj, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

color_parameter_adj <- webppl(program_file =
                           here("webppl/continuous_semantics.wppl"), 
                          data = color_estimation_data_adj, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e1_continuous_parameters <- size_parameter %>%
  bind_rows(color_parameter)

e1_continuous_parameters_adj <- size_parameter_adj %>%
  bind_rows(color_parameter_adj)

#write_csv(e1_continuous_parameters, here("webppl/model_parameters/e1_continuous_parameters.csv"))
#write_csv(e1_continuous_parameters_adj, here("webppl/model_parameters/e1_continuous_parameters_adj_only.csv"))
```

```{r summarise-e1-parameters-cont}
e1_continuous_parameters <- read_csv(here("webppl/model_parameters/e1_continuous_parameters.csv"))

e1_cont_parameter_means <- e1_continuous_parameters %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e1_cont_color_parameter <- e1_cont_parameter_means %>% 
  filter(parameter == "color")

e1_cont_size_parameter <- e1_cont_parameter_means %>% 
  filter(parameter == "size")

e1_continuous_parameters_adj <- read_csv(here("webppl/model_parameters/e1_continuous_parameters.csv"))

e1_cont_parameter_adj_means <- e1_continuous_parameters_adj %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e1_cont_color_parameter_adj <- e1_cont_parameter_adj_means %>% 
  filter(parameter == "color")

e1_cont_size_parameter_adj <- e1_cont_parameter_adj_means %>% 
  filter(parameter == "size")
```

```{r two_world_continuous, eval = FALSE}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))

two_world_utterances_adj <- tibble(utterance = c("blue dax"),
                               utterance_num = as.character(1)) 

e1_cont_webppl_input <- two_world_utterances %>%
  mutate(parameter = "color") %>%
  bind_rows(mutate(two_world_utterances, parameter = "size")) %>%
  left_join(e1_cont_parameter_means, by = "parameter") %>%
  group_by(utterance_num, parameter) %>%
  nest()

e1_cont_webppl_input_adj <- two_world_utterances_adj %>%
  mutate(parameter = "color") %>%
  bind_rows(mutate(two_world_utterances_adj, parameter = "size")) %>%
  left_join(e1_cont_parameter_adj_means, by = "parameter") %>%
  group_by(utterance_num, parameter) %>%
  nest()
  
cont_two_world_inference <- e1_cont_webppl_input  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/continuous_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

cont_two_world_inference_adj <- e1_cont_webppl_input_adj  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/continuous_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances_adj, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

#write_csv(cont_two_world_inference, here("webppl/model_estimates/e1_estimates_continuous.csv"))
#write_csv(cont_two_world_inference_adj, here("webppl/model_estimates/e1_estimates_continuous_adj_only.csv"))
```

```{r load-e1-webppl-empirical-continuous}
cont_two_world_inference <- read_csv(here("webppl/model_estimates/e1_estimates_continuous.csv"),
                                show_col_types = FALSE)

cont_two_world_inference_adj <- read_csv(here("webppl/model_estimates/e1_estimates_continuous_adj_only.csv"),
                                show_col_types = FALSE)
```

```{r e1-join-data-continuous}
e1_webppl_continuous_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(cont_two_world_inference, by = c("utterance", "item" = "obj", 
                                        "condition" = "parameter")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)

e1_webppl_continuous_data_adj <- e1_mean_data %>%
  filter(searchtype == "contrast",
         adj == TRUE) %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(cont_two_world_inference_adj, by = c("utterance", "item" = "obj", 
                                        "condition" = "parameter")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)
```

\subsection{Modeling Experiment 1 with continuous semantics}

@degen_when_2020 capture asymmetries in description of size and color by positing that different features have different semantic strength. They posit that color has stronger semantics than size, such that "red table" is a better literal description of a small red table than "small table" is. Under these assumptions, RSA using these continuous semantics explains people's tendency to mention color more often than size in a variety of tasks. Can continuous semantics explain the asymmetry we find between color and size in Experiment 1? 

```{r e1-webppl-cont-plot, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "People's choice patterns in Experiment 1, with model predictions from a model with continuous literal semantics."}
ggplot(e1_webppl_continuous_data, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "proportion item choice") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = item, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 0.5, y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")
```

```{r e1-webppl-cont-plot-adj, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "People's choice patterns in Experiment 1 on only adjective noun trials, with model predictions from a model with continuous literal semantics.", eval = FALSE}
ggplot(e1_webppl_continuous_data_adj, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "proportion item choice") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = item, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 0.5, y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")
```

In Experiment 1, we found that people more consistently choose the target using contrastive inferences about size than color. We incorporated their continuous semantics into our RSA model of referent choice, which reasons over possible lexicons, and allowed the semantic values of color and size to vary (rather than fitting feature-specific alpha values). In Figure \ref{fig:webppl-cont-plot} we show the resulting model predictions. This model predicts the overall inference that people choose the target more often when there is an adjective in the utterance. However, the model's fit of the color--size asymmetry, when fitting all the data, is off: it overpredicts people's contrastive inferences about color. Further, the estimated continuous semantics parameters are not in the expected direction: the semantic strength of size is `r e1_cont_size_parameter$mean`, and the semantic strength of color is `r e1_cont_color_parameter$mean`. 

However, when we examine only the *adjective noun* condition, a continuous semantics model can explain the general pattern. We confirmed this by fitting the model to only the *adjective noun* trials, and found the expected pattern of semantic values: for size, `r e1_cont_size_parameter_adj$mean`, and for color, `r e1_cont_color_parameter_adj$mean`. The model seems to be fitting the data in the *noun* condition at the expense of estimating people's inferences in the *adjective noun* condition, which causes the fitted semantic strength parameters to be misaligned with the findings in @degen_when_2020. Thus, although the continuous semantics model in its current form does not fit our data well, we think it is possible this is due to the pattern of responses in the *noun* condition, and potentially also the overall noisy pattern of guessing in this highly ambiguous task. Adapting a continuous semantics model to account for the response pattern in this task is a potentially promising way to integrate the findings of @degen_when_2020 with ours.

```{r continuous-sem-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:continuous-sem-plot}Results of modeling target choice in Experiment 1 using continuous semantics. Stronger continuous semantics predict higher choice of the target, while weaker continuous semantics predict lower choice of the target. ", eval = FALSE}

cont_semantics_inference %>%
  mutate(utterance_cond = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance_cond = factor(utterance_cond, levels = c("noun", "adjective noun")),
         semantic_val = if_else(label == "high_size_low_color", 
                                "Low semantic strength", "High semantic strength"),
         semantic_val = factor(semantic_val, levels = c("Low semantic strength", "High semantic strength")),
         choice = case_when(obj == "blue dax" & world_string == "two dax" ~ "target",
                            obj == "blue dax" & world_string == "two toma" ~ "lure",
                            obj != "blue dax" ~ "other")) %>%
  group_by(utterance_cond, semantic_val, choice) %>%
  summarise(prob = sum(prob)) %>%
  filter(choice != "other") %>%
  ggplot(aes(x = utterance_cond, color = choice, fill = choice)) + 
  geom_crossbar(aes(ymin = prob, ymax = prob, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ semantic_val) +
  labs(x = "Utterance", y = "Proportion each item is chosen") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = choice, y = prob), 
          position = position_dodge(.5),
          method = list(dl.trans(y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")

# Our webppl code is set up to do both color and size with different semantic values, but we're only doing
# inference over "color" values here as a demonstration, so we can relabel to just "low semantic strength" and 
# "high semantic strength".
```

```{r production-probs}
prod_utterances <- tibble(utterance = c("dax", "blue dax"),
                          utt_priors = c("0.9,0.1", "0.5,0.5")) %>%
  tidyr::expand(utterance, utt_priors) %>%
  mutate(all_vals = paste(utterance, utt_priors, sep = ","),
         label = if_else(utt_priors == "0.9,0.1", "high_adj", "low_adj"),
         ids = as.character(1:4))


prod_inference <- map_dfr(prod_utterances %>% pull(all_vals), 
                               ~webppl(program_file = 
                                         here("webppl/e1_production_probabilities.wppl"), 
                                       data = .x),
                               .id = "ids") %>%
  left_join(prod_utterances, by = "ids") %>%
  as_tibble()  %>%
  select(-all_vals) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 
```

```{r production-plot, fig.width = 5, fig.height = 4.5, fig.cap = "\\label{fig:production-plot}Results of modeling Experiment 1 using differing production probabilities. When an adjective is likely to be produced (left), the model predicts a weaker preference for the target when an adjective is produced (*adjective noun* utterance) than it does when an adjective is unlikely to be produced.", eval = FALSE}
prod_plot_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(prod_inference %>% mutate(condition = if_else(label == "low_adj", "size", "color")), 
            by = c("utterance", "item" = "obj", "condition" = "condition")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)

ggplot(prod_plot_data, 
       aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob, ymax = prob, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "proportion item choice") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = item, y = prob), 
          position = position_dodge(0.5),
          method = list(dl.trans(x = x - 0.5, y = y - 0.5), "last.points", cex=.7)) +
  theme(legend.position = "none")

```
<!--
\section{Modeling Experiment 1 with Production Expectations}

One possible explanation of the asymmetry we see between people's inferences about color and size is that listeners expect speakers to mention objects' colors more than their sizes. Speakers do remark on color more than size [@pechmann_incremental_1989], and listeners may keep track of these expectations when interpreting utterances, downweighting the communicative weight of color adjectives. In our models in the main text, we implement this downweighting in a feature-specific rationality parameter: we posit that listeners expect speakers to be less strictly informative when talking about color than size. Another way to implement this difference is to put it in a different part of the model: the utterance prior. That is, to build into the model the listener's expectation that speakers will mention color adjectives more often than size adjectives. 

Here, we provide a demonstration that this implementation would explain the asymmetry we observe in people's data. Figure \ref{fig:production-plot} shows the model predictions when using an adjective is likely (on the left) and using an adjective is unlikely (on the right). When the listener thinks a speaker is fairly likely to use a type of adjective (as they would with color adjectives) and the adjective is used, they prefer the target, but only weakly. When a listener thinks a speaker is unlikely to use a type of adjective (as they would with size adjectives) and the adjective is used, they prefer the target very strongly. We also plot the data from people's judgments here for comparison, but note that this demonstration is only meant to show qualitative patterns—we did not estimate specific parameters (the rationality parameter or specific rates of adjective use) to fit these data more closely.

Where might these production differences come from? The models we present in the main text implement this asymmetry in a feature rationality parameter for simplicity, but this alternative modeling choice is a promising avenue for explaining differences between inferences about different features in a more principled way.
-->

\section{Experiment 2 Additional Analyses}

```{r e2-read-data}
e2_color_data <- read_csv(here("data/exp2/color.csv"), 
                          show_col_types = FALSE) %>%
  mutate(condition = "color", targetsize = "big") %>%
  rename(adj = colorasked, distractorfeature = distractorcolor)

e2_size_data <- read_csv(here("data/exp2/size.csv"),
                         show_col_types = FALSE) %>%
  mutate(condition = "size") %>%
  rename(adj = sizeasked, distractorfeature = distractorsize)

e2_data <- rbind(e2_color_data, e2_size_data) %>%
  mutate(subid = paste0(subid, condition))

e2_keep_subjs <- e2_data %>%
  filter(searchtype == "attncheck", attncheckscore >= 6) %>%
  group_by(subid) 

e2_model_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  filter(trialtype != 0)

e2_subj_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  rename(adjective = adj) %>%
  mutate(searchtype = if_else(searchtype == "polychrome" | 
                                searchtype == "differentsizes",
                                      "different", searchtype)) %>%
  mutate(searchtype = if_else(searchtype == "monochrome" |
                                searchtype == "samesize",
                                      "same", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # delay before people could select
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(adjective == TRUE, "adjective noun", "noun"),
         adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("different", "contrast", "same")))

e2_mean_data <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e2_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e2_adj", "e2_search_same", "e2_search_contrast", "e2_adj_contrast", "e2_adj_same"), 
      c("adjectiveadjective noun", "searchtypesame", "searchtypecontrast",
        "adjectiveadjective noun:searchtypecontrast", "adjectiveadjective noun:searchtypesame"), 
      ~ make_text_vars(e2_model, .x, .y))
```

\subsection{Experiment 2 Prevalence Judgments}
The full regression of prevalence judgments, also reported in the main text, is in Table \ref{tab:e2-table}.

```{r e2-table}
e2_model %>%
  select(term, estimate, statistic, p.value) %>%
  mutate(estimate = as.numeric(estimate)) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "adjectiveadjective noun" ~ "utterance type: adjective (vs. no adjective)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "searchtypecontrast" ~ "context: within-category contrast display (vs. between-category contrast)",
    term == "searchtypesame" ~ "context: same feature display (vs. between-category contrast)",
    term == "conditionsize:adjectiveadjective noun" ~ "size * adjective",
    term == "conditionsize:searchtypecontrast" ~ "size * within-category contrast display",
    term == "conditionsize:searchtypesame" ~ "size * same feature display",
    term == "adjectiveadjective noun:searchtypecontrast" ~ "adjective * within-category contrast display",
    term == "adjectiveadjective noun:searchtypesame" ~ "adjective * same feature display",
    term == "conditionsize:adjectiveadjective noun:searchtypecontrast" ~ "size * adjective * within-category contrast display",
    term == "conditionsize:adjectiveadjective noun:searchtypesame" ~ "size * adjective * same feature display"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(font_size = "footnotesize", caption = "Full model of prevalence judgments from Experiment 2. Model specification is percentage $\\sim$ adjective\\_type * utterance\\_type * context\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")


```

\section{Experiment 3 Additional Analyses}

```{r e3-read-data}
e3_data <- read_csv(here("data/exp3_turk_data.csv"), show_col_types = FALSE)

# participants who have unbalanced numbers of trials in each condition
e3_exclude <- e3_data %>% count(subid, utttype, searchtype) %>% filter(n == 2 | n == 3)

e3_keep_subjs <- e3_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, attncheckscore >= 6) %>%
  filter(!(subid %in% e3_exclude$subid)) %>%
  group_by(subid) %>%
  count() %>%
  filter(n == 4)

e3_model_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = factor(searchtype, levels = c("differentshapes", "contrast")))

e3_subj_data <- e3_data %>%
  filter(subid %in% e3_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  mutate(searchtype = if_else(searchtype == "differentshapes",
                                      "different", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>% # time before selections can be made
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(utttype == "adj", "adjective noun", utttype),
         adjective = if_else(adjective == "noutt", "alien utterance", 
                             adjective),
         adjective = if_else(adjective == "noadj", "noun", adjective),
         adjective = factor(adjective, 
                            levels = c("noun", "adjective noun", 
                                       "alien utterance")),
         searchtype = factor(searchtype, levels = c("different", "contrast")))

e3_mean_data <- e3_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

e3_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e3_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed")
```

```{r e3-models}
# all prereg'd models below

e3_utt_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(e3_utt_model_no_alien, "e3_adj_no_alien", "utttypeadj")

e3_utt_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype + (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e3_utt_model_adj", "e3_utt_model_noadj"), 
      c("utttypeadj", "utttypenoadj"), 
      ~ make_text_vars(e3_utt_model, .x, .y))

# model did not converge with maximal slopes
e3_full_model <- e3_model_data %>%
  mutate(utttype = factor(utttype, levels = c("noutt", "noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

walk2(c("e3_full_noadj", "e3_full_adj", "e3_full_size", "e3_full_contrast", "e3_full_adj_size"), 
      c("utttypenoadj", "utttypeadj", "conditionsize", "searchtypecontrast", "utttypeadj:conditionsize"), 
      ~ make_text_vars(e3_full_model, .x, .y))

e3_full_model_no_alien <- e3_model_data %>%
  filter(utttype != "noutt") %>%
  mutate(utttype = factor(utttype, levels = c("noadj", "adj"))) %>%
  lmer(percentage ~ utttype * searchtype * condition + 
         (utttype | subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = papaja::printp(p.value))

make_text_vars(e3_full_model_no_alien, "e3_noalien_adj", "utttypeadj")

```

The full regression predicting Experiment 3 prevalence judgments, also reported in the main text, is shown in Table \ref{tab:e3-full}. The regression predicting Experiment 3 prevalence judgments among only adjective utterances and no adjective utterances (excluding alien utterance trials), also reported in the main text, is shown in Table \ref{tab:e3-full-no-alien}.

In addition to the regressions reported in the manuscript, we report two pre-registered, targeted regressions to test the effect of utterance type to more specifically in case these effects were unclear in the maximal models. First, we filtered to adjective and no adjective trials and fit a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject (Table \ref{tab:e3-adj-no-alien}). Participants' prevalence judgments were significantly lower when an adjective was used in the utterance ($\beta =$ `r e3_adj_no_alien_estimate`, $t =$ `r e3_adj_no_alien_statistic`, $p =$ `r e3_adj_no_alien_p.value`). Second, we included all trials in a linear mixed effects model predicting prevalence judgment by utterance type with a random slope of utterance type by subject (Table \ref{tab:e3-adj}). Utterances without an adjective resulted in significantly higher prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_noadj_estimate`, $t =$ `r e3_utt_model_noadj_statistic`, $p =$ `r e3_utt_model_noadj_p.value`), and utterances with an adjective did not result in significantly different prevalence judgments than alien utterances ($\beta =$ `r e3_utt_model_adj_estimate`, $t =$ `r e3_utt_model_adj_statistic`, $p =$ `r e3_utt_model_adj_p.value`).

```{r e3-full, results = "asis"}
e3_full_model %>%
  select(-std.error, -df) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "utterance type: adjective (vs. alien utterance)",
    term == "utttypenoadj" ~ "utterance type: no adjective utterance (vs. alien utterance)",
    term == "searchtypecontrast" ~ "context type: within-category contrast (vs. between-category)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "utttypenoadj:searchtypecontrast" ~ "no adjective utterance * within-category contrast display",
    term == "utttypeadj:searchtypecontrast" ~ "adjective utterance * within-category contrast display",
    term == "utttypenoadj:conditionsize" ~ "no adjective utterance * size",
    term == "utttypeadj:conditionsize" ~ "adjective utterance * size",
    term == "searchtypecontrast:conditionsize" ~ "within-category contrast display",
    term == "utttypenoadj:searchtypecontrast:conditionsize" ~ "no adjective utterance * within-category contrast display * size"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type, context type, and adjective type in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type * context\\_type * adjective\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")

```

```{r e3-full-no-alien, results = "asis"}
e3_full_model_no_alien %>%
  select(-std.error, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "utterance type: adjective (vs. no adjective)",
    term == "searchtypecontrast" ~ "context tyep: within-category contrast (vs. between-category)",
    term == "conditionsize" ~ "adjective type: size (vs. color)",
    term == "utttypeadj:searchtypecontrast" ~ "adjective utterance * within-category contrast display",
    term == "utttypeadj:conditionsize" ~ "adjective utterance * size",
    term == "searchtypecontrast:conditionsize" ~ "within-category contrast display",
    term == "utttypeadj:searchtypecontrast:conditionsize" ~ "no adjective utterance * within-category contrast display * size"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type, context type, and adjective type only among adjective and no adjective utterances (excluding alien utterances) in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type * context\\_type * adjective\\_type + (utterance\\_type | subject).", escape = FALSE, placement = "h")

```

```{r e3-adj-no-alien, results = "asis"}
e3_utt_model_no_alien %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "adjective utterance (vs. no adjective utterance)"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from presence of an adjective in the utterance (excluding alien language utterances) in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type + (utterance\\_type | subject). ", escape = FALSE, placement = "h")


```

```{r e3-adj, results = "asis"}
e3_utt_model %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "intercept",
    term == "utttypeadj" ~ "adjective utterance (vs. alien utterance)",
    term == "utttypenoadj" ~ "no adjective utterance (vs. alien utterance)"
  )) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Regression predicting prevalence judgments from utterance type in Experiment 3. Model specification is percentage $\\sim$ utterance\\_type + (utterance\\_type | subject). ", escape = FALSE, placement = "h")


```

```{r combined-analysis}
e2_to_combine <- e2_subj_data %>%
  mutate(experiment = 2, 
         subid = paste0(subid, "2")) %>%
  filter(searchtype != "same") %>%
  select(subid, experiment, counter, condition, adjective, searchtype, percentage)

e3_to_combine <- e3_subj_data %>%
  mutate(experiment = 3,
         subid = paste0(subid, "3")) %>%
  filter(adjective != "alien utterance") %>%
  select(subid, experiment, counter, condition, adjective, searchtype, percentage)

combined_data <- rbind(e2_to_combine, e3_to_combine)

n_subjs_2 <- n_distinct(e2_to_combine$subid)
n_subjs_3 <- n_distinct(e3_to_combine$subid)
n_subjs_combined <- n_distinct(combined_data$subid)

combined_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_adj", "combined_size", "combined_search_contrast", "combined_adj_contrast"), 
      c("adjectiveadjective noun", "conditionsize", "searchtypecontrast", 
        "adjectiveadjective noun:searchtypecontrast"), 
      ~ make_text_vars(combined_model, .x, .y))

combined_mean_data <- combined_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)

```

```{r combined-separate-adj}
# just an exploratory check to see if these context differences only show up in adj trials or non-adj trials,
# considered separately.
# there's a small effect among only adj trials and a marginal effect among only non-adj trials.
# it doesn't seem likely that the overall context difference is due to, e.g., 
# a *typicality* inference when a necessary adjective was omitted. 
combined_model_all_adj <- lmer(percentage ~ condition * searchtype +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data %>% filter(adjective == "adjective noun")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_all_adj_search_contrast"), 
      c("searchtypecontrast"), 
      ~ make_text_vars(combined_model_all_adj, .x, .y))

combined_model_no_adj <- lmer(percentage ~ condition * searchtype +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = combined_data %>% filter(adjective == "noun")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("combined_no_adj_search_contrast"), 
      c("searchtypecontrast"), 
      ~ make_text_vars(combined_model_no_adj, .x, .y))

```

\section{Combined Analyses of Experiment 2 and 3}

Given that many participants failed the memory check in Experiments 2 and 3 (which was our pre-registered exclusion criterion), we yielded data from fewer participants than expected in our pre-registrations. This means that the analyses of Experiments 2 and 3 may be underpowered to detect subtle effects of context or feature type. Since Experiments 2 and 3 had very similar procedures, we can conduct an exploratory combined analysis comparing the conditions they shared. 

Combining data from Experiment 2 (N = `r n_subjs_2`) and Experiment 3 (N = `r n_subjs_3`) yields a total sample of `r n_subjs_combined` participants in this combined analysis. Only trials from the conditions that Experiments 2 and 3 share are included in this analysis: two types of referential context (within-category context and between-category context), two types of feature (size and color), and two types of utterance (adjective noun and noun only).

We fit a mixed effects linear regression on the combined data with the same specification as the full models we used for Experiments 2 and 3 in the main text: effects of utterance type, context type, and critical feature and their interactions, and a random slope of utterance type by subject. This model revealed a significant effect of utterance type ($\beta_{adjective} =$ `r combined_adj_estimate`, $t =$ `r combined_adj_statistic`, $p$ `r combined_adj_p.value`): people inferred that a feature was rarer when it was mentioned, consistent with our findings in each experiment separately. Participants' inferences did not significantly differ between color and size adjective conditions ($\beta_{size} =$ `r combined_size_estimate`, $t =$ `r combined_size_statistic`, $p =$ `r combined_size_p.value`). This is also consistent with findings from each experiment. There was a significant effect of context, with people making overall slightly higher prevalence judgments in the within-category context ($\beta_{within} =$ `r combined_search_contrast_estimate`, $t =$ `r combined_search_contrast_statistic`, $p =$ `r combined_search_contrast_p.value`), which was not found in either experiment separately. However, there was not a significant interaction between context and utterance type ($\beta_{within*adjective} =$ `r combined_adj_contrast_estimate`, $t =$ `r combined_adj_contrast_statistic`, $p =$ `r combined_adj_contrast_p.value`), consistent with findings from each experiment. That is, people did not modulate the size of their typicality inferences (the difference between the adjective noun and noun utterance types) based on context type, though there was a baseline difference in prevalence judgments between context types. 

Overall, these results are consistent with what we found in Experiments 2 and 3, with the exception of finding an overall difference in prevalence judgments between context types. However, we did not find an interaction between context type and utterance type, which would demonstrate that participants are trading off between referential utility and typicality when making these inferences. This exploratory combined analysis suggests further research is necessary, as even when combined our data are not definitive about whether potential trade-offs are small or nonexistent.

```{r combined-plot, fig.cap = "Participants' prevalence judgments in combined data from Experiments 2 and 3."}

labels <- rasterGrob(png::readPNG(here("writing/figs/e3_plot_labels.png")), interpolate = TRUE)

plot <- ggplot(combined_mean_data %>% mutate(searchtype = factor(searchtype, levels = c("contrast", "different"))), 
               aes(x = adjective, y = empirical_stat, 
                         color = condition, fill = condition)) + 
  facet_wrap(~ searchtype) +
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  labs(color = "Adjective type") + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(.5)) + 
  geom_hline(aes(yintercept = 50), linetype = "dashed") +
  scale_color_ptol() +
  theme(strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = "none") 

# grid.arrange(labels, plot, nrow = 2,
#              widths = c(1,19),
#              heights = c(2,5),
#              layout_matrix = rbind(c(NA, 1),
#                                    c(2)))
```


```{r estimate-combined-params, eval = FALSE}
combined_estimation_data <- combined_data %>%
  select(searchtype, adjective, condition, percentage) %>%
  mutate(utt = if_else(adjective == "adjective noun", "red toma", "toma")) %>%
  rowwise() %>%
  mutate(p = min(max(percentage, 1),99)) %>%
  ungroup() 

combined_size_estimation_data <- combined_estimation_data %>%
  filter(condition == "size") 

combined_color_estimation_data <- combined_estimation_data %>%
  filter(condition == "color") 

combined_size_parameter_samples <- webppl(program_file =
                           here("webppl/infer_combined_params.wppl"), 
                          data = combined_size_estimation_data,
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

combined_color_parameter_samples <- webppl(program_file =
                           here("webppl/infer_combined_params.wppl"), 
                          data = combined_color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

combined_parameters <- combined_size_parameter_samples %>%
  bind_rows(combined_color_parameter_samples)

#write_csv(combined_parameters, here("webppl/model_parameters/combined_parameters.csv"))
```

```{r estimate-combined-params-no-context, eval = FALSE}
combined_size_parameter_samples_no_context <- webppl(program_file =
                           here("webppl/infer_combined_params_no_ref.wppl"), 
                          data = combined_size_estimation_data,
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

combined_color_parameter_samples_no_context <- webppl(program_file =
                           here("webppl/infer_combined_params_no_ref.wppl"), 
                          data = combined_color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

combined_parameters_no_context <- combined_size_parameter_samples_no_context %>%
  bind_rows(combined_color_parameter_samples_no_context)

#write_csv(combined_parameters_no_context, here("webppl/model_parameters/combined_parameters_no_context.csv"))
```

```{r read-combined-parameters}
combined_params <- read_csv(here("webppl/model_parameters/combined_parameters.csv"),
                          show_col_types = FALSE)

combined_params_no_context <- read_csv(here("webppl/model_parameters/combined_parameters_no_context.csv"),
                          show_col_types = FALSE)

combined_parameter_means <- combined_params %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

combined_color_parameter <- combined_parameter_means %>%
  filter(parameter == "color")

combined_size_parameter <- combined_parameter_means %>%
  filter(parameter == "size")

combined_parameter_means_no_context <- combined_params_no_context %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

combined_color_parameter_no_context <- combined_parameter_means_no_context %>%
  filter(parameter == "color")

combined_size_parameter_no_context <- combined_parameter_means_no_context %>%
  filter(parameter == "size")
```

```{r combined-inference, eval = FALSE}
combined_utterances <- expand_grid(utterance = c("toma", "red toma"),
                             type = c("contrast", "different"),
                             parameter = c("size", "color")) %>%
  mutate(utterance_num = if_else(utterance == "toma", 1, 2)) %>%
  left_join(combined_parameter_means, by = "parameter")
 
combined_inference <- combined_utterances %>%
  group_by(utterance_num, parameter, type) %>%
  nest() %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here(glue("webppl/e2_{type}_empirical.wppl")),
                                       data = .x))) %>%
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(combined_utterances, by = c("utterance_num", "type", "parameter")) %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, 
                            levels = c("noun", "adjective noun"))) 

combined_inference_no_context <- combined_utterances %>%
  group_by(utterance_num, parameter, type) %>%
  nest() %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/combined_no_ref_empirical.wppl"),
                                       data = .x))) %>%
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(combined_utterances, by = c("utterance_num", "type", "parameter")) %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, 
                            levels = c("noun", "adjective noun"))) 
  
#write_csv(combined_inference, here("webppl/model_estimates/combined_estimates.csv"))
#write_csv(combined_inference_no_context, here("webppl/model_estimates/combined_estimates_no_context.csv"))
```

```{r load-combined-estimates}
combined_inference <- read_csv(here("webppl/model_estimates/combined_estimates.csv"),
                         show_col_types = FALSE)
combined_inference_no_context <- read_csv(here("webppl/model_estimates/combined_estimates_no_context.csv"),
                         show_col_types = FALSE)

combined_inference_means <- combined_inference %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma", world == "target world") %>%
  group_by(utterance, type, parameter) %>%
  summarise(p = mean(p))

combined_inference_no_context_means <- combined_inference_no_context %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma", world == "target world") %>%
  group_by(utterance, type, parameter) %>%
  summarise(p = mean(p))
```

```{r combined-wppl-plot, fig.height = 4, fig.width = 6.46, fig.cap = "Prevalence judgments of combined data from Experiments 2 and 3, along with our model's predictions (marked by horizontal lines) and the predictions of a model that does not take into account referential context (marked by X shapes)."}
combined_wppl_data <- combined_inference_means %>%
  mutate(p = 100 * p) %>%
  rename(empirical_stat = p, adjective = utterance,
         searchtype = type, condition = parameter) %>%
  mutate(adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("contrast", "different")))

combined_no_context_wppl_data <- combined_inference_no_context_means %>%
  mutate(p = 100 * p) %>%
  rename(empirical_stat = p, adjective = utterance,
         searchtype = type, condition = parameter) %>%
  mutate(adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("contrast", "different")))

labels <- rasterGrob(png::readPNG(here("writing/figs/e3_plot_labels.png")), interpolate = TRUE)

plot_model_combined <- ggplot(combined_mean_data %>% mutate(searchtype = factor(searchtype, 
                                                           levels = c("contrast", "different"))),
                      aes(x = adjective, y = empirical_stat, 
                         color = condition)) + 
  facet_wrap(~ searchtype) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(.5)) + 
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  labs(color = "Adjective type") + 
  geom_crossbar(aes(ymin = empirical_stat, ymax = empirical_stat, y = empirical_stat),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5, data = combined_wppl_data) + 
  geom_point(mapping = aes(ymin = empirical_stat, ymax = empirical_stat, y = empirical_stat),
                position = position_dodge(.5), 
           alpha = .5, size = 5, data = combined_no_context_wppl_data, shape = 4) + 
  geom_hline(aes(yintercept = 50), linetype = "dashed") +
  scale_color_ptol() +
  geom_dl(aes(label = condition), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 1, y = y - .7), "first.points", cex=.7)) +
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

grid.arrange(labels, plot_model_combined, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))
```

```{r model-likelihoods}
combined_likelihoods <- combined_params %>%
  group_by(parameter) %>%
  summarise(likelihood = max(score))

combined_likelihoods_no_context <- combined_params_no_context %>%
  group_by(parameter) %>%
  summarise(likelihood = max(score))

```

\section{Experiments 2 and 3: Comparison to a Model with No Referential Context}

Our experiments set out to compare two possibilities: a *reference-first view*, on which there would be a strong tradeoff between the goals of reference and conveying typicality, and an adjective used for reference would not prompt any further inferences about typicality; and a *probabilistic weighing view*, on which conveying contrast with respect to reference and with respect to typicality would trade off in a graded way. Our data rule out the reference-first view, as there is not a strong trade-off: people still make inferences about typicality when an adjective was useful or necessary for reference. Our findings leave open the possibility that either (1) there is a weak trade-off that we do not have the power to detect in our data or (2) there is no trade-off whatsoever. Here, we compare our model with a model that does not incorporate reference to evaluate this latter possibility.

We constructed a model much like that used in Experiment 2, except that the model only considered the target object as a potential referent. This is analogous to only presenting the model with the target object and an utterance, e.g., "the [red/small] toma." The model has the same prior on the object's feature distribution and direct observation as the model with context (and as the participants in our task)—it does observe another toma with a different feature from the target, but that observation is separate from its reasoning about reference.

We fit this model to the combined set of people's judgments from Experiment 2 and Experiment 3, allowing for separate feature rationality parameters. We also re-fit the model from the text in the same way. In Figure \ref{fig:combined-wppl-plot}, we show the combined data from Experiment 2 and Experiment 3, along with our original model's estimates (horizontal lines) and the estimates of the model that does not take into account reference (X shapes). The model without referential context makes very similar predictions to the model with referential context. Combined with the null effects of context we find in the regressions on people's judgments, this suggests that context has either an undetectably small effect or no effect on people's typicality judgments in this task.  

\section{Inferring Atypicality with Non-Fruit Novel Stimuli}

The use of alien fruit stimuli in our experiments raises the question of whether people would infer that *non-fruits* are atypical when their color is remarked upon. As noted in the introduction, people remark on the colors of some kinds of objects more than others, perhaps because they more often vary in color [@rubio-fernandez_how_2016]. Because fruits tend to have stereotypical colors, it is possible that people would not expect the colors of fruits to be remarked upon very often, and thus have a stronger inference of atypicality for fruit than other types of objects. Here, we provide some additional data showing that people still make these inferences for categories about which they likely do not have such an expectation: block shapes.

### Participants.
346 participants were recruited on Amazon Mechanical Turk to perform this task. They were paid 10 cents to complete the task. Participants on average took 20 seconds to complete the single trial (not including reading the consent form).

Two participants were excluded because they repeated the task, leaving 344 participants for analysis. There were no other exclusion criteria.

### Procedure
Before the main task began, participants were shown an array of colorful block shapes and told they were going to learn about shapes like these, and were given three examples of novel names these kinds of shapes might have. The object shapes, colors, and names in this introduction phase were randomly chosen and different from the ones used in the task.

In this one-trial task, participants saw a stimulus display with six identical block shapes (distractors) surrounding one unique block shape (the target) (see label in Figure \ref{fig:tetris-plot}). They were asked to "Find the [blue] toma," with an utterance that either included a color adjective or did not. We expected that participants would click on the unique, central object, and they only moved forward in the task once they selected that object. 

Two conditions of interest, utterance type (with or without an adjective) and display type (whether the distractors' color was the same as or different from the target) were varied between subjects. Additionally, several factors were randomly assigned between subjects: the name of the target (among *dax, blicket, wug, toma, gade,* or *sprock*), the target color (among red, blue, green, orange, purple, yellow, pink, tan, teal, or grey), the distractor color if different from the target (among the same set of colors), and the target and distractor shapes (among ten possible block shapes with five square components).

After selecting the target object, participants made a judgment about the prevalence of the target's color among the target's category. They were asked, e.g., "What percentage of tomas do you think are blue?" and responded on a slider scale between 0 and 100%.

```{r tetris}
tetris_data <- read_csv(here("data/supplemental/tetris_data_one_shot.csv"))

tetris_subj_data <- tetris_data %>%
  mutate(utterance = if_else(colorasked == FALSE, "toma", "blue toma"),
         context = if_else(diffcolor == TRUE, "different color", "same color"),
         counter = as.numeric(counter)) %>%
  mutate(utterance = factor(utterance, levels = c("toma", "blue toma")),
         context = factor(context, levels = c("different color", "same color")))

repeated_subjs <- tetris_subj_data %>%
  count(subid) %>%
  filter(n > 1)

tetris_subj_data <- tetris_subj_data %>%
  filter(!(subid %in% repeated_subjs$subid))

tetris_model <- lm(percentage ~ utterance * context,
              data = tetris_subj_data) %>%
  tidy() %>%
  mutate(p.value = printp(p.value))

walk2(c("tetris_adj", "tetris_samecolor", "tetris_adj_samecolor"),
      c("utteranceblue toma", "contextsame color", "utteranceblue toma:contextsame color"), 
      ~ make_text_vars(tetris_model, .x, .y))

tetris_mean_data <- tetris_subj_data %>%
  group_by(context, utterance) %>%
  tidyboot_mean(percentage)

tetris_avg_time <- tetris_subj_data %>%
  mutate(trial_time = as.numeric(rttrain) + as.numeric(rttest)) %>% 
  group_by(subid) %>%
  summarise(total_time = sum(trial_time)/1000) %>% # convert milliseconds to seconds
  ungroup() %>%
  summarise(mean_time = mean(total_time))
```

```{r tetris-plot, cache = FALSE}
tetris_labels <- rasterGrob(png::readPNG(here("writing/figs/tetris_plot_labels.png")), interpolate = TRUE)


plot <- ggplot(tetris_mean_data, aes(x = utterance, color = context, group = context)) +
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  facet_wrap(~context) +
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  scale_color_ptol() +
theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = "none")


grid.arrange(tetris_labels, plot, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))
```

### Results
The main question of interest is whether people infer that a color is rarer when it is mentioned than when it is not; when a participant hears "blue toma," do they infer that tomas are less likely to be blue? We fit a linear model predicting participants' prevalence judgments from the utterance type (noun or adjective noun) and context type (different-color or same-color distractors) and their interaction. There was a significant effect of utterance type, such that people's prevalence judgments were lower when there was an adjective in the utterance than when there was not ($\beta_{adjective} =$ `r tetris_adj_estimate`, $t =$ `r tetris_adj_statistic`, $p =$ `r tetris_adj_p.value`). There was not a significant effect of context type ($\beta_{same-color-context} =$ `r tetris_samecolor_estimate`, $t =$ `r tetris_samecolor_statistic`, $p =$ `r tetris_samecolor_p.value`) or an utterance by context interaction ($\beta_{adjective-context} =$ `r tetris_adj_samecolor_estimate`, $t =$ `r tetris_adj_samecolor_statistic`, $p =$ `r tetris_adj_samecolor_p.value`). 

## Discussion
We found that when a block object is referred to as "the blue toma" rather than "the toma," people think tomas are less likely to be blue in general. People's judgments also did not significantly differ depending on whether surrounding objects were the same color as the target. This stripped-down demonstration of the effect provides additional evidence that people infer described objects are atypical even when they likely do not have strong expectations that the object categories have a stereotypical color (as they may with fruit). It is an open and interesting question whether people's prior expectations about a category's feature distribution would modulate or extinguish this effect. This demonstration is evidence that the effect generalizes beyond fruit; we leave a systematic investigation of that question to future work.



\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\noindent
<div id = "refs"></div>
\endgroup